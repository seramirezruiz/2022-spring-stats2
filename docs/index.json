[{"authors":null,"categories":null,"content":"Adri√°n Santonja is a Ph.D. student at the DIW Berlin Graduate Center since October 2020. He obtained both his M.Sc. and B.Sc degree in economics at the University of Mannheim. Adri√°n¬¥s research interests lie in the fields of climate and environmental economics, applied microeconometrics and policy evaluation. Before joining DIW, Adri√°n worked as a research assistant for the Chair of Quantitative Economics and the Chair of Econometrics at the University of Mannheim. In addition, he gained professional experience at the Directorate-General for Climate Action of the European Commission in Brussels and the economic consultancy Frontier Economics in Madrid.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b8cfebb0d61cc10c64f61b7b56223eb0","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/adrian-santonja-di-fonzo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/adrian-santonja-di-fonzo/","section":"authors","summary":"Adri√°n Santonja is a Ph.D. student at the DIW Berlin Graduate Center since October 2020. He obtained both his M.Sc. and B.Sc degree in economics at the University of Mannheim. Adri√°n¬¥s research interests lie in the fields of climate and environmental economics, applied microeconometrics and policy evaluation.","tags":null,"title":"Adri√°n Santonja di Fonzo","type":"authors"},{"authors":null,"categories":null,"content":"Kindye Adugna graduated from the Hertie School in July 2021. He also holds a BSc degree in Economics from Mekelle University. Before moving to Berlin, Kindye was an analyst at the Tony Blair Institute, advising central governments across Africa. Between September 2020 and December 2021, he worked as a research and teaching assistant to Dr. Bechar√° at the Hertie School Data Science Lab.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"45433e734a380c25d99bd7773b1ef854","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/kindye-adugna/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/kindye-adugna/","section":"authors","summary":"Kindye Adugna graduated from the Hertie School in July 2021. He also holds a BSc degree in Economics from Mekelle University. Before moving to Berlin, Kindye was an analyst at the Tony Blair Institute, advising central governments across Africa.","tags":null,"title":"Kindye Adugna","type":"authors"},{"authors":null,"categories":null,"content":"Lisa Oswald is pursuing a PhD in Governance at the Hertie School in Berlin. She graduated from the University of Oxford with a MSc degree in Social Data Science, and from the University of Kassel with a BSc and MSc degree in Psychology. She is interested in online communication and deliberation, the public perception of climate change, political opinion formation and the emergence of collective behaviour.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6efcaa8911b4c0459710d83df8d3e3e8","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/lisa-oswald/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/lisa-oswald/","section":"authors","summary":"Lisa Oswald is pursuing a PhD in Governance at the Hertie School in Berlin. She graduated from the University of Oxford with a MSc degree in Social Data Science, and from the University of Kassel with a BSc and MSc degree in Psychology.","tags":null,"title":"Lisa Oswald","type":"authors"},{"authors":null,"categories":null,"content":"Sebastian Ramirez Ruiz is a Phd Researcher and a Research Associate to Prof. Simon Munzert. He holds a Master\u0026rsquo;s of Public Policy from the Hertie School and B.A.s in Sociology and Political Science from Stony Brook University. He is particularly interested in causal inference, the use of evidence in decision-making, and most importantly, bicycles.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d68d186b5437c0054f698afb12d818b3","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/sebastian-ramirez-ruiz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/sebastian-ramirez-ruiz/","section":"authors","summary":"Sebastian Ramirez Ruiz is a Phd Researcher and a Research Associate to Prof. Simon Munzert. He holds a Master\u0026rsquo;s of Public Policy from the Hertie School and B.A.s in Sociology and Political Science from Stony Brook University.","tags":null,"title":"Sebastian Ramirez Ruiz","type":"authors"},{"authors":null,"categories":null,"content":"Simon Munzert is Assistant Professor of Data Science and Public Policy at the Hertie School and member of the Hertie School Data Science Lab. His research interests include attitude formation in the digital age, public opinion, and the use of online data in social research. He received his Doctoral Degree in Political Science from the University of Konstanz.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"39676266dc03f7a9ebc998657bf4cd2d","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/simon-munzert/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/simon-munzert/","section":"authors","summary":"Simon Munzert is Assistant Professor of Data Science and Public Policy at the Hertie School and member of the Hertie School Data Science Lab. His research interests include attitude formation in the digital age, public opinion, and the use of online data in social research.","tags":null,"title":"Simon Munzert","type":"authors"},{"authors":null,"categories":null,"content":"Till Koeveker is a PhD candidate at DIW Graduate Center and in the Climate Policy department of DIW. He holds a Master in Economics (Economic Theory and Econometrics) from Toulouse School of Economics and a Bachelor in Sociology, Politics \u0026amp; Economics from Zeppelin University in Friedrichshafen. Furthermore, he has experience from internships in different sectors (e.g. at KfW Development Bank, at the √ñko-Insitut ‚Äì Institute for Applied Ecology and at the economic consultancy Frontier Economics). In his current research, Till studies policy instruments for decarbonizing the industry sector, in particular carbon border adjustment mechanisms.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5292b55c488db0476cec94b4b16c4a92","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/till-koeveker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/till-koeveker/","section":"authors","summary":"Till Koeveker is a PhD candidate at DIW Graduate Center and in the Climate Policy department of DIW. He holds a Master in Economics (Economic Theory and Econometrics) from Toulouse School of Economics and a Bachelor in Sociology, Politics \u0026amp; Economics from Zeppelin University in Friedrichshafen.","tags":null,"title":"Till Koeveker","type":"authors"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references      Today\u0026rsquo;s session  Take a step back to review how to compare the means of two groups in R Learn how to perform matching with the MatchIt package Illustrate the mechanics of propensity score matching with gml()  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nTo explore more about MatchIt: MatchIt: Getting Started: https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html\nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html Helpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \nLisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about matching in R\n  Matching in R Welcome Introduction! Welcome to our fifth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. During this week‚Äôs lecture you reviewed randomization in experimental setups.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1615161600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1615161600,"objectID":"2211b25d6069d4c59ebe1dc415ed9cb6","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-5/","publishdate":"2021-03-08T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-5/","section":"materials","summary":"Welcome to our tutorial about matching in R","tags":null,"title":"üìä 05 - Matching","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references      Today\u0026rsquo;s session  use the ggdag and dagitty packages to assess your modeling strategy review how to run regression models using R illustrate omitted variable and collider bias  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html Helpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \n Lisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about performing analyses on DAGs in R and regression\n  The Backdoor Criterion and Basics of Regression in R Welcome Introduction! Welcome to our fourth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. During this week's lecture you reviewed bivariate and multiple linear regressions.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1614470400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1614470400,"objectID":"3ead50f0ec2b4ec49a2fc5cdbd1e4d9e","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-4/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-4/","section":"materials","summary":"Welcome to our tutorial about performing analyses on DAGs in R and regression","tags":null,"title":"üìä 04 - The Backdoor Criterion and Regression","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references      Today\u0026rsquo;s session  identify the rationale of the grammar of graphics in ggplot2 create visualizations with ggplot2 identify basic functionalities of ggdag for DAG creation in R  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html More info about ggplot2 The complete ggplot tutorial: http://r-statistics.co/Complete-Ggplot2-Tutorial-Part1-With-R-Code.html \nHelpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \n Lisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about data visualization with ggplot2 and DAGs\n  Basics of Data Visualization and DAGs in R # Load packages. Install them first, in case you don\u0026#39;t have them yet. library(palmerpenguins) # To get our example\u0026#39;s dataset library(tidyverse) # To use dplyr functions and the pipe operator when needed library(ggplot2) # To visualize data (this package is also loaded by library(tidyverse)) library(ggdag) # To create our DAGs Welcome This week's tutorial will be divided in two broader camps.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1614038400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1614038400,"objectID":"402a6112202baa8a57ee5b66cff17ea6","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-3/","publishdate":"2021-02-23T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-3/","section":"materials","summary":"Welcome to our tutorial about data visualization with `ggplot2` and DAGs","tags":null,"title":"üìä 03 - Visualization and Directed Acyclic Graphs (DAGs)","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session Further references    Today\u0026rsquo;s session  identify the purpose of a set of dplyr verbs write statements in tidy syntax apply dplyr verbs to solve your data manipulation challenges  Download slides - PDF\n Further references For dplyr Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nHelpful cheatsheets Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx\nLisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about data manipulation with dplyr and the Potential Outcomes Framework\n  Introduction to Data Manipulation 1. Introduction Welcome! Welcome to our second tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. The labs are designed to reinforce the material covered during the lectures by introducing you to hands-on applications.\n  The Potential Outcomes Framework The POF in practice Let's revisit the example from our slides once again. Say we are interested in assessing the premise of Allport's hypothesis about interpersonal contact being conducive to reducing intergroup prejudice.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1612742400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1612742400,"objectID":"dbc7d94f5edd7bc1d5484cb1286fe5cd","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-2/","publishdate":"2021-02-08T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-2/","section":"materials","summary":"Welcome to our tutorial about data manipulation with `dplyr` and the Potential Outcomes Framework","tags":null,"title":"üìä 02 - Foundations","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session Program overview Further references    Today\u0026rsquo;s session  Discuss the logistics of the drop-in tutorials Explain the assignment submission process Introduce you to RMarkdown  Download slides - PDF\n Program overview Welcome to our introductory week. During this session we will go over the basic set-up for this semester.\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg RMarkdown cheatsheet: https://tinyurl.com/uqoelrx  Slides Welcome to our introduction to the tutorials\n  R and RStudio basics Welcome! The practical component of the Statistics II: Statistical Modeling and Causal Inference course relies largely in R programming. Today we will center on some of the necessary skills to perform the assignments for the course.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1612310400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1612310400,"objectID":"c5c2b8f3885bac92985371787ae0c913","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-1/","publishdate":"2021-02-03T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-1/","section":"materials","summary":"Welcome to our introduction to the tutorials","tags":null,"title":"üìä 01 - Introduction","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1615161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615161600,"objectID":"b82f18c3fac1daa6da6651055b484027","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-5/slides/","publishdate":"2021-03-08T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-5/slides/","section":"materials","summary":"Welcome to our tutorial about matching in R","tags":null,"title":"05 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1614470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614470400,"objectID":"c0ba5cb612cf233f7b2c4a49603ec587","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-4/slides/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-4/slides/","section":"materials","summary":"Welcome to our tutorial about performing analyses on DAGs in R and regression","tags":null,"title":"04 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1614038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614038400,"objectID":"ac69de0c16ff1ab65a31c6637942a5f7","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-3/slides/","publishdate":"2021-02-23T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-3/slides/","section":"materials","summary":"Welcome to our tutorial about data visualization with `ggplot2` and DAGs","tags":null,"title":"03 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1612742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612742400,"objectID":"9213c49561d9f1b07797ec92baaaa4a1","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-2/slides/","publishdate":"2021-02-08T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-2/slides/","section":"materials","summary":"Welcome to our tutorial about data manipulation with `dplyr` and the Potential Outcomes Framework","tags":null,"title":"02 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1612310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612310400,"objectID":"54210879398f2cecb50f62ab00bbb544","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-1/slides/","publishdate":"2021-02-03T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-1/slides/","section":"materials","summary":"Welcome to our introduction to the tutorials","tags":null,"title":"01 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Welcome! The practical component of the Statistics II: Statistical Modeling and Causal Inference course relies largely in R programming. Today we will center on some of the necessary skills to perform the assignments for the course.\nIn this tutorial, you will learn to:\n identify the some of the basic functionalities of R and RStudio import data sets into your R environment  R and RStudio: Basics The RStudio layout RStudio is an integrated development environment (IDE) for R. Think of RStudio as a front that allows us to interact, compile, and render R code in a more instinctive way. The following image shows what the standard RStudio interface looks like:\n    Console: The console provides a means to interact directly with R. You can type some code at the console and when you press ENTER, R will run that code. Depending on what you type, you may see some output in the console or if you make a mistake, you may get a warning or an error message.\n  Script editor: You will utilize the script editor to complete your assignments. The script editor will be the space where files will be displayed. For example, once you download and open the bi-weekly assignment .Rmd template, it will appear here. The editor is a where you should place code you care about, since the code from the console cannot be saved into a script.\n  Environment: This area holds the abstractions you have created with your code. If you run myresult \u0026lt;- 5+3+2, the myresult object will appear there.\n  Plots and files: This area will be where graphic output will be generated. Additionally, if you write a question mark before any function, (i.e. ?mean) the online documentation will be displayed here.\n   R Packages For the most part, R Packages are collections of code and functions that leverage R programming to expand on the basic functionalities. Last week we met dplyr that aids R programmers in the process of data cleaning and manipulation. There are a plethora of packages in R designed to facilite the completion of tasks. In fact, this website is built with the blogdown package that lets you create websites using RMarkdown and Hugo\nUnlike other programming languages, in R you only need to install a package once. The following times you will only need to \u0026ldquo;require\u0026rdquo; the package. As a good practice I recommend running the code to install packages only in your R console, not in the code editor. You can install a package with the following syntax\ninstall.packages(\u0026quot;name_of_your_package\u0026quot;) #note that the quotation marks are mandatory at this stage  Once the package has been installed, you just need to \u0026ldquo;call it\u0026rdquo; every time you want to use it in a file by running:\nlibrary(\u0026quot;name_of_your_package\u0026quot;) #either of this lines will require the package library(name_of_your_package) #library understands the code with, or without, quotation marks   It is extremely important that you do not have any lines installing packages for your assignments because the file will fail to knit    Working directory The working directory is just a file path on your computer that sets the default location of any files you read into R, or save out of R. Normally, when you open RStudio it will have a default directory (a folder in your computer). You can check you directory by running getwd() in your console:\n#this is the default in my case getwd() #[1] \u0026quot;/Users/sebastianramirezruiz\u0026quot;  When your RStudio is closed and you open a file from your finder in MacOS or file explorer in Windows, the default working directory will be the folder where the file is hosted\n Setting your working directory You can set you directory manually from RStudio: use the menu to change your working directory under Session \u0026gt; Set Working Directory \u0026gt; Choose Directory.\n  You can also use the setwd() function:\nsetwd(\u0026quot;/path/to/your/directory\u0026quot;) #in macOS setwd(\u0026quot;c:/path/to/your/directory\u0026quot;) #in windows   Recommended folder structure for the class   We recommend you pay close attention to your folder structure. You will receive a new folder for each assignment. Make the folder your working directory when working on the assignment. This folder will be populated with the template .Rmd and the data for the week. When you knit the file, the .html will be created in this folder.\n We will learn more about the assignment submission workflow next week. Still, avoid changing the name of the files you receive in Github since it will create issues.    Dealing with errors in R Errors in R occur when code is used in a way that it is not intended. For example when you try to add two character strings, you will get the following error:\n\u0026quot;hello\u0026quot; + \u0026quot;world\u0026quot; Error in \u0026quot;hello\u0026quot; + \u0026quot;world\u0026quot;: non-numeric argument to binary operator  Normally, when something has gone wrong with your program, R will notify you of an error in the console. There are errors that will prevent the code from running, while others will only produce warning messages. In the following case, the code will run, but you will notice that the string \u0026ldquo;three\u0026rdquo; is turned into a NA.\nas.numeric(c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;three\u0026quot;)) Warning: NAs introduced by coercion [1] 1 2 NA  Since we will be utilizing widely used packages and functions in the course of the semester, the errors that you may come across in the process of completing your assignments will be common for other R users. Most errors occur because of typos. A Google search of the error message can take you a long way as well. Most of the times the first entry on stackoverflow.com will solve the problem.\n Importing data Next we will work with data provided by the palmerpenguins package; however, most practical applications will require you to work with your own data. In fact, for most assignments, you will be given a data set to work with. As we will see in the coming weeks, data are messy. You know what else is messy? Data formats. You may be acquainted with a couple of them (.csv, .tsv, .xlsx).\nFortunately for us, the tidyverse has two packages that make the process of loading data sets from different formats very easy.\n readr: The goal of readr is to provide a fast and friendly way to read rectangular data (like csv, tsv, rds, and fwf) haven: The goal of haven is to enable R to read and write various data formats used by other statistical packages (like dta, sas, and sav)  You can read more about how to load different types of data in the respective documentations of the packages ‚Äî readr and haven\n For the assignments you will be required to load datasets. You can do that by installing the readr package and utilizing:\n#with readr your_data_frame \u0026lt;- readr::read_rds(\u0026quot;path_for_the_file\u0026quot;) #you can alternatively use a base R option your_data_frame \u0026lt;- base::readRDS(\u0026quot;path_for_the_file\u0026quot;)  This week\u0026rsquo;s mock assignment will feature a .tsv file. When in doubt just Google \u0026ldquo;How to load x_format data in R?\u0026quot; That will do the trick!\n The double colon operator :: You may have noted in the previous section that the functions were preceded by their package name and two colons, for example: readr::read_rds(). The double colon operator :: helps us ensure that we select functions from a particular package. We utilize the operator to explicitly state where the function is coming. This may become even more important when you are doing data analysis as part of a team further in your careers. Though it is likely that this will not be a problem during the course, we can try to employ the following convention package_name::function() to ensure that we will not encounter errors in our knitting process:\ndplyr::select()   Let\u0026rsquo;s look at what happens when we load tidyverse.\nlibrary(tidyverse) #‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.0 #‚îÄ‚îÄ #‚úì ggplot2 3.3.2 ‚úì purrr 0.3.4 #‚úì tibble 3.0.3 ‚úì dplyr 1.0.2 #‚úì tidyr 1.1.2 ‚úì stringr 1.4.0 #‚úì readr 1.3.1 ‚úì forcats 0.5.0 #‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() #‚îÄ‚îÄ #x dplyr::filter() masks stats::filter() #x dplyr::lag() masks stats::lag()  You may notice that R points out some conflicts where some functions are being masked. The default in this machine will become the filter() from the dplyr package during this session. If you were to run some code that is based on the filter() from the stats package, your code will probably result in errors.\n ","date":1612310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612310400,"objectID":"027efd9630c3bd9e00a3f8413c01c502","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-1/foundations/","publishdate":"2021-02-03T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-1/foundations/","section":"materials","summary":"Welcome! The practical component of the Statistics II: Statistical Modeling and Causal Inference course relies largely in R programming. Today we will center on some of the necessary skills to perform the assignments for the course.","tags":null,"title":"R and RStudio basics","type":"book"},{"authors":null,"categories":null,"content":"    1. Introduction Welcome! Welcome to our second tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nThe labs are designed to reinforce the material covered during the lectures by introducing you to hands-on applications.\nThe practical nature of our class means that our labs will be data-centered. Throughout our class, we will get acquinted with multiple packages of the tidyverse.\nThough we expect that some of you may already know them, the tidyverse is a collection of R packages that share an underlying design, syntax, and structure. They will definitely make your life easier!!\nToday, we will start with a brief introduction to data manipulation through the dplyr package.\nIn this tutorial, you will learn to:\n identify the purpose of a set of dplyr verbs write statements in tidy syntax apply dplyr verbs to solve your data manipulation challenges  This tutorial is partly based on R for Data Science, section 5.2, and Quantitative Politics with R, chapter 3.\n What we will need today We‚Äôll practice some wrangling in dplyr using data for penguin sizes recorded by Dr.¬†Kristen Gorman and others at several islands in the Palmer Archipelago, Antarctica. Data are originally published in: Gorman KB, Williams TD, Fraser WR (2014) PLoS ONE 9(3): e90081. doi:10.1371/journal.pone.0090081\nYou do not need to import the data to work through this tutorial - the data are already here waiting behind the scenes.\nBut if you do ever want to use the penguins data outside of this tutorial, they now exist in the palmerpenguins package in R.\nLet‚Äôs begin!\n 2. Data Structure Tidy data Generally, we will encounter data in a tidy format. Tidy data refers to a way of mapping the structure of a data set. In a tidy data set:\n Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table  The penguins data set The 3 species of penguins in this data set are Adelie, Chinstrap and Gentoo. The data set contains 8 variables:\n species: a factor denoting the penguin species (Adelie, Chinstrap, or Gentoo) island: a factor denoting the island (in Palmer Archipelago, Antarctica) where observed culmen_length_mm: a number denoting length of the dorsal ridge of penguin bill (millimeters) culmen_depth_mm: a number denoting the depth of the penguin bill (millimeters) flipper_length_mm: an integer denoting penguin flipper length (millimeters) body_mass_g: an integer denoting penguin body mass (grams) sex: a factor denoting penguin sex (MALE, FEMALE) year an integer denoting the year of the record  *Illustration by @allisonhorst* Let‚Äôs explore the data set. head() is a function that returns the first couple rows from a data frame. Write the R code required to explore the first observations of the penguins data set:\nNotice that when you press ‚ÄòRun,‚Äô the output of the code is returned below it! So by pressing ‚ÄòRun,‚Äô you‚Äôve run your first R code of the class!\nhead(penguins)    species  island  bill\\_length\\_mm  bill\\_depth\\_mm  flipper\\_length\\_mm  body\\_mass\\_g  sex  year      Adelie  Torgersen  39.1  18.7  181  3750  male  2007    Adelie  Torgersen  39.5  17.4  186  3800  female  2007    Adelie  Torgersen  40.3  18.0  195  3250  female  2007    Adelie  Torgersen  NA  NA  NA  NA  NA  2007    Adelie  Torgersen  36.7  19.3  193  3450  female  2007    Adelie  Torgersen  39.3  20.6  190  3650  male  2007      3. Manipulating data with dplyr What we will learn today In this tutorial, you‚Äôll learn and practice examples using some functions in dplyr to work with data. Those are:\n select(): keep or exclude some columns filter(): keep rows that satisfy your conditions mutate(): add columns from existing data or edit existing columns group_by(): lets you define groups within your data set summarize(): get summary statistics arrange(): reorders the rows according to single or multiple variables  Let‚Äôs get to work.\n 3.1. select() The first verb (function) we will utilize is select(). We can employ it to manipulate our data based on columns. If you recall from our initial exploration of the data set there were eight variables attached to every observation. Do you recall them? If you do not, there is no problem. You can utilize names() to retrieve the names of the variables in a data frame.\nnames(penguins)  ## [1] \u0026quot;species\u0026quot; \u0026quot;island\u0026quot; \u0026quot;bill_length_mm\u0026quot; ## [4] \u0026quot;bill_depth_mm\u0026quot; \u0026quot;flipper_length_mm\u0026quot; \u0026quot;body_mass_g\u0026quot; ## [7] \u0026quot;sex\u0026quot; \u0026quot;year\u0026quot;  Say we are only interested in the species, island, and year variables of these data, we can utilize the following syntax:\n select(data, columns)   Activity The following code chunk would select the species, island, and year variables. What should we do to keep the body_mass_g and sex variables as well?\ndplyr::select(penguins, species, island, year)   Answer # you just need to type the names of the columns dplyr::select(penguins, species, island, year, body_mass_g, sex)     To drop variables, use - before the variable name.\nFor example, select(penguins, -year) will drop the year column.\n   3.2. filter() The second verb (function) we will employ is filter(). filter() lets you use a logical test to extract specific rows from a data frame. To use filter(), pass it the data frame followed by one or more logical tests. filter() will return every row that passes each logical test.\nThe more commonly used logical operators are:\n ==: Equal to !=: Not equal to \u0026gt;, \u0026gt;=: Greater than, greater than or equal to \u0026lt;, \u0026lt;=: Less than, less than or equal to \u0026amp;, |: And, or  Say we are interested in retrieving the observations from the year 2007. We would do:\ndplyr::filter(penguins, year == 2007)   Activity Can you adapt the code to retrieve all the observations of Chinstrap penguins from 2007 (remember that species contains character units)\nAnswer # you just need to utilize \u0026amp; and type the logical operator for the species dplyr::filter(penguins, year == 2007 \u0026amp; species == \u0026quot;Chinstrap\u0026quot;)     3.3. The Pipe Operator: %\u0026gt;% The pipe, %\u0026gt;%, comes from the magrittr package by Stefan Milton Bache. Packages in the tidyverse load %\u0026gt;% for you automatically, so you don‚Äôt usually load magrittr explicitly. This will be one of your best friends in R. \u0026gt;Pipes are a powerful tool for clearly expressing a sequence of multiple operations. Let‚Äôs think about baking for a second.\n Activity We can leverage the pipe operator to sequence our code in a logical manner. Can you adapt the following code chunk with the pipe and conditional logical operators we discussed?\nonly_2009 \u0026lt;- dplyr::filter(penguins, year == 2009) only_2009_chinstraps \u0026lt;- dplyr::filter(only_2009, species == \u0026quot;Chinstrap\u0026quot;) only_2009_chinstraps_species_sex_year \u0026lt;- dplyr::select(only_2009_chinstraps, species, sex, year) final_df \u0026lt;- only_2009_chinstraps_species_sex_year final_df #to print it in our console  Answer penguins %\u0026gt;% #we start off with out df dplyr::filter(year == 2009 \u0026amp; species == \u0026quot;Chinstrap\u0026quot;) %\u0026gt;% #filter dplyr::select(species, sex, year) #select     3.4. mutate() mutate() lets us create, modify, and delete columns. The most common use for now will be to create new variables based on existing ones. Say we are working with a U.S. American client and they feel more confortable with assessing the weight of the penguins in pounds. We would utilize mutate() as such:\n  mutate(new\\_var\\_name = conditions)  Activity Can you edit the following code chunk to render a new variable body_mass_kg?\npenguins %\u0026gt;% dplyr::mutate(body_mass_lbs = body_mass_g/453.6)   Answer penguins %\u0026gt;% dplyr::mutate(body_mass_kg = body_mass_g/1000) #grams divided by 1000     3.5. group_by() and summarize() These two verbs group_by() and summarize() tend to go together. When combined , ‚Äôsummarize()` will create a new data frame. It will have one (or more) rows for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. For example:\n summarize():  penguins %\u0026gt;% dplyr::summarize(heaviest_penguin = max(body_mass_g, na.rm = T)) #max() does not know how to deal with NAs very well    heaviest\\_penguin      6300      group_by() + summarize():  penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarize(heaviest_penguin = max(body_mass_g, na.rm = T))    species  heaviest\\_penguin      Adelie  4775    Chinstrap  4800    Gentoo  6300     Activity Can you get the weight of the lightest penguin of each species? You can use min(). What happens when in addition to species you also group by year group_by(species, year)?\nAnswers penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarize(lightest_penguin = min(body_mass_g, na.rm = T))    species  lightest\\_penguin      Adelie  2850    Chinstrap  2700    Gentoo  3950     penguins %\u0026gt;% dplyr::group_by(species, year) %\u0026gt;% dplyr::summarize(lightest_penguin = max(body_mass_g, na.rm = T))  ## `summarise()` has grouped output by 'species'. You can override using the `.groups` argument.    3.6. arrange() The arrange() verb is pretty self-explanatory. arrange() orders the rows of a data frame by the values of selected columns in ascending order. You can use the desc() argument inside to arrange in descending order. The following chunk arranges the data frame based on the length of the penguins‚Äô bill. You hint tab contains the code for the descending order alternative.\n arrange(variable\\_of\\_interest)  penguins %\u0026gt;% dplyr::arrange(bill_length_mm)   penguins %\u0026gt;% dplyr::arrange(desc(bill_length_mm))   Activity Can you create a data frame arranged by body_mass_g of the penguins observed in the ‚ÄúDream‚Äù island?\nAnswer penguins %\u0026gt;% dplyr::filter(island == \u0026quot;Dream\u0026quot;) %\u0026gt;% dplyr::arrange(desc(body_mass_g))    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0e5dbbf98949e49081cad709093af1a5","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-2/data-manipulation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-2/data-manipulation/","section":"materials","summary":"1. Introduction Welcome! Welcome to our second tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nThe labs are designed to reinforce the material covered during the lectures by introducing you to hands-on applications.","tags":null,"title":"Introduction to Data Manipulation","type":"book"},{"authors":null,"categories":null,"content":" # Load packages. Install them first, in case you don\u0026#39;t have them yet. library(palmerpenguins) # To get our example\u0026#39;s dataset library(tidyverse) # To use dplyr functions and the pipe operator when needed library(ggplot2) # To visualize data (this package is also loaded by library(tidyverse)) library(ggdag) # To create our DAGs Welcome This week's tutorial will be divided in two broader camps.\n First, we will learn some basics of data visualization with ggplot. Second, we will start our exploration of directed acyclic graphs (DAGs) for causal inference.   Introduction to ggplot2 ggplot2 is by far the most popular visualization package in R. ggplot2 implements the grammar of graphics to render a versatile syntax of creating visuals. The underlying logic of the package relies on deconstructing the structure of graphs (if you are interested in this you can read this article).\nFor the purposes of this introduction to visualization with ggplot, we care about the layered nature of visualizing with ggplot2.\n*This tutorial is based largely on chapters 7 to 10 from the QPOLR book\nOur building blocks During this week, we will learn about the following building blocks:\n Data: the data frame, or data frames, we will use to plot Aesthetics: the variables we will be working with Geometric objects: the type of visualization Theme adjustments: size, text, colors etc  Data The first building block for our plots are the data we intend to map. In ggplot2, we always have to specify the object where our data lives. In other words, you will always have to specify a data frame, as such:\nggplot(name_of_your_df) In the future, we will see how to combine multiple data sources to build a single plot. For now, we will work under the assumption that all your data live in the same object.\n Aesthetics The second building block for our plots are the aesthetics. We need to specify the variables in the data frame we will be using and what role they play.\nTo do this we will use the function aes() within the ggplot() function after the data frame (remember to add a comma after the data frame).\nggplot(name_of_your_df, aes(x = your_x_axis_variable, y = your_y_axis_variable)) Beyond your axis, you can add more aesthetics representing further dimensions of the data in the two dimensional graphic plane, such as: size, color, fill, to name but a few.\n Geometric objects The third layer to render our graph is a geomethic object. To add one, we need to add a plus (+) at the end of the initial line and state the type of geometric object we want to add, for example, geom_point() for a scatter plot, or geom_bar() for barplots.\nggplot(name_of_your_df, aes(x = your_x_axis_variable, y = your_y_axis_variable)) + geom_point()  Theme At this point our plot may just need some final thouches. We may want to fix the axes names or get rid of the default gray background. To do so, we need to add an additional layer preceded by a plus sign (+).\nIf we want to change the names in our axes, we can utilize the labs() function.\nWe can also employ some of the pre-loaded themes, for example, theme_minimal().\nggplot(name_of_your_df, aes(x = your_x_axis_variable, y = your_y_axis_variable)) + geom_point() + theme_minimal() + labs(x = \u0026quot;Name you want displayed\u0026quot;, y = \u0026quot;Name you want displayed\u0026quot;)  Our first plot For our very first plot using ggplot2, we will use the penguins data from last week.\nWe would like to create a scatterplot that illustrates the relationship between the length of a penguin's flipper and their weight.\nTo do so, we need three of our building blocks: a) data, b) aesthetics, and c) a geometric object (geom_point()).\nggplot(penguins, aes(x = flipper_length_mm, y=body_mass_g)) + geom_point()  EXERCISE:   Once we have our scatterplot. Can you think of a way to adapt the code to:\n convey another dimension through color, the species of penguin  change the axes names  render the graph with theme_minimal().   Answer ggplot(penguins, aes(x = flipper_length_mm, y=body_mass_g, color=species)) + geom_point() + theme_minimal() + labs(x = \u0026quot;Flipper Length (mm)\u0026quot;, y = \u0026quot;Body mass (g)\u0026quot;, color = \u0026quot;Species\u0026quot;)     Visualizing effectively Plotting distributions If we are interested in plotting distributions of our data, we can leverage geometric objects, such as:\n geom_histogram(): visualizes the distribution of a single continuous variable by dividing the x axis into bins and counting the number of observations in each bin (the default is 30 bins). geom_density(): computes and draws kernel density estimate, which is a smoothed version of the histogram. geom_bar(): renders barplots and in plotting distributions behaves in a very similar way from geom_histogram() (can also be used with two dimensions)  This is a histogram presenting the weight distribution of penguins in our sample. .\nggplot(penguins, aes(x = body_mass_g)) + geom_histogram()  EXERCISE:   Let's adapt the code of our histogram:\n add bins = 15 argument (type different numbers)  add fill = \u0026quot;#FF6666\u0026quot; (type \u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, instead of #FF6666)  change the geom to _density and _bar   Answer  Histogram with bins argument   ggplot(penguins, aes(x = body_mass_g)) + geom_histogram(bins = 15)  Histogram with bins and fill arguments   ggplot(penguins, aes(x = body_mass_g)) + geom_histogram(bins = 25, fill = \u0026quot;#FF6666\u0026quot;)  geom_density() and geom_bar()   ggplot(penguins, aes(x = body_mass_g)) + geom_density(alpha = 0.5, fill = \u0026quot;#FF6666\u0026quot;) ggplot(penguins, aes(x = body_mass_g)) + geom_bar(fill = \u0026quot;#FF6666\u0026quot;)  Plotting relationships We can utilize graphs to explore how different variables are related. In fact, we did so before in our scatterplot. We can also use box plots and lines to show some of these relationships.\nFor example, this boxplot showcasing the distribution of weight by species:\nggplot(penguins, aes(x = species, y = body_mass_g)) + geom_boxplot() + theme_minimal() + labs(x = \u0026quot;Species\u0026quot;, y = \u0026quot;Body mass (g)\u0026quot;) Or this adaptation of our initial plot with a line of best fit for the observed data by each species:\nggplot(penguins, aes(x= flipper_length_mm, y = body_mass_g, color = species)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + labs(x = \u0026quot;Length of the flipper\u0026quot;, y = \u0026quot;Body mass (g)\u0026quot;, color = \u0026quot;Species\u0026quot;)  Next steps Now that you have been introduced to some of the basics of ggplot2, the best way to move forward is to experiment. As we have discussed before, the R community is very open. Perhaps, you can gather some inspiration from the Tidy Tuesday social data project in R where users explore a new dataset each week and share their visualizations and code on Twitter under #TidyTuesday. You can explore some of the previous visualizations here and try to replicate their code.\nHere is a curated list of awesome ggplot2 resources.\n  Directed Acyclic Graphs (DAGs) This week we learned that directed acyclic graphs (DAGs) are very useful to express our beliefs about relationships among variables.\nDAGs are compatible with the potential outcomes framework. They give us a more convinient and intuitive way of laying out causal models. Next week we will learn how they can help us develop a modeling strategy.\nToday, we will focus on their structure and some DAG basics with the ggdag package.\nCreating DAGs in R To create our DAGs in R we will use the ggdag packages.\nThe first thing we will need to do is to create a dagified object. That is an object where we state our variables and the relationships they have to each other. Once we have our dag object we just need to plot with the ggdag() function.\nLet's say we want to re-create this DAG:\nWe would like to express the following links:\n P -\u0026gt; D D -\u0026gt; M D -\u0026gt; Y M -\u0026gt; Y  To do so in R with ggdag, we would use the following syntax:\ndag_object \u0026lt;- ggdag::dagify(variable_being_pointed_at ~ variable_pointing, variable_being_pointed_at ~ variable_pointing, variable_being_pointed_at ~ variable_pointing) After this we would just:\nggdag::ggdag(dag_object) Let's plot this DAG\nour_dag \u0026lt;- ggdag::dagify(d ~ p, m ~ d, y ~ d, y ~ m) ggdag::ggdag(our_dag)  EXERCISE:   See what happens when you add + theme_minimal(), + theme_void(), or + theme_dag() to the DAG. What package do you think is laying behind the mappings ofggdag`?\nAnswer our_dag \u0026lt;- ggdag::dagify(d ~ p, m ~ d, y ~ d, y ~ m) ggdag::ggdag(our_dag) + theme_minimal() ggdag::ggdag(our_dag) + theme_void()   Polishing our DAGs in R As you may have seen, the DAG is not rendered with the nodes in the positions we want.\nIf you ever want to explicitly tell ggdag where to position each node, you can tell it in a Cartesian coordinate plane.\nLet's take P as the point (0,0):\ncoord_dag \u0026lt;- list( x = c(p = 0, d = 1, m = 2, y = 3), y = c(p = 0, d = 0, m = 1, y = 0) ) our_dag \u0026lt;- ggdag::dagify(d ~ p, m ~ d, y ~ d, y ~ m, coords = coord_dag) ggdag::ggdag(our_dag) + theme_void()  More complex example: Let's say we're looking at the relationship between smoking and cardiac arrest. We might assume that smoking causes changes in cholesterol, which causes cardiac arrest:\nsmoking_ca_dag \u0026lt;- ggdag::dagify(cardiacarrest ~ cholesterol, cholesterol ~ smoking + weight, smoking ~ unhealthy, weight ~ unhealthy, labels = c(\u0026quot;cardiacarrest\u0026quot; = \u0026quot;Cardiac\\n Arrest\u0026quot;, \u0026quot;smoking\u0026quot; = \u0026quot;Smoking\u0026quot;, \u0026quot;cholesterol\u0026quot; = \u0026quot;Cholesterol\u0026quot;, \u0026quot;unhealthy\u0026quot; = \u0026quot;Unhealthy\\n Lifestyle\u0026quot;, \u0026quot;weight\u0026quot; = \u0026quot;Weight\u0026quot;) ) ggdag::ggdag(smoking_ca_dag, # the dag object we created text = FALSE, # this means the original names won\u0026#39;t be shown use_labels = \u0026quot;label\u0026quot;) + # instead use the new names theme_void() In this example, we:\n Used more meaningful variable names  Created a variable that was the result of two variables vs. just one (cholesterol)  Used the \u0026quot;labels\u0026quot; argument to rename our variables (this is useful if your desired final variable name is more than one word)     Common DAG path structures coord_dag \u0026lt;- list( x = c(d = 0, x = 1, y = 2), y = c(d = 0, x = 1, y = 0) ) our_dag \u0026lt;- ggdag::dagify(x ~ d, y ~ d, y ~ x, coords = coord_dag) ggdag::ggdag(our_dag) + theme_void()    EXERCISE:   Let's adapt the code to make X a confounder and a collider.\nAnswer  X as a confounder   coord_dag \u0026lt;- list( x = c(d = 0, x = 1, y = 2), y = c(d = 0, x = 1, y = 0) ) our_dag \u0026lt;- ggdag::dagify(d ~ x, #line from x to d y ~ d, #line from d to y y ~ x, #line from x to y coords = coord_dag)\nggdag::ggdag(our_dag) + theme_void()\n X as a collider   coord_dag \u0026lt;- list( x = c(d = 0, x = 1, y = 2), y = c(d = 0, x = 1, y = 0) ) our_dag \u0026lt;- ggdag::dagify(x ~ d, #line from d to x y ~ d, #line from d to y x ~ y, #line from y to x coords = coord_dag)\nggdag::ggdag(our_dag) + theme_void()\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9efba60adab01b945cf07df2871d117f","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-3/03-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-3/03-online-tutorial/","section":"materials","summary":"# Load packages. Install them first, in case you don\u0026#39;t have them yet. library(palmerpenguins) # To get our example\u0026#39;s dataset library(tidyverse) # To use dplyr functions and the pipe operator when needed library(ggplot2) # To visualize data (this package is also loaded by library(tidyverse)) library(ggdag) # To create our DAGs Welcome This week's tutorial will be divided in two broader camps.","tags":null,"title":"Basics of Data Visualization and DAGs in R","type":"book"},{"authors":null,"categories":null,"content":"   Welcome Introduction! Welcome to our fifth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you reviewed randomization in experimental setups. You also learned how matching can be leveraged to gather causal estimates.\nIn this lab session we will:\n Take a step back to review how to compare the means of two groups in R Learn how to perform matching with the MatchIt package Illustrate the mechanics of propensity score matching with gml()  Packages # These are the libraries we will use today. Make sure to install them in your console in case you have not done so previously. library(tidyverse) # To use dplyr functions and the pipe operator when needed library(ggplot2) # To create plots (this package is also loaded by library(tidyverse)) library(purrr) # To repeat code across our list in the balance table purrr::map() (this package is also loaded by library(tidyverse)) library(broom) # To format regression output library(stargazer) # To format model output library(knitr) # To create HTML tables with kable() library(kableExtra) # To format the HTML tables library(MatchIt) # To match    Our data Today we will work with data from the Early Childhood Longitudinal Study (ECLS).\necls_df \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/seramirezruiz/stats-ii-lab/master/Session%204/data/ecls.csv\u0026quot;) %\u0026gt;% dplyr::select(-childid, -race, -w3daded, -w3momed, -w3inccat) #drop these columns (-) names(ecls_df) #checking the names of the variables in the data frame ## [1] \u0026quot;catholic\u0026quot; \u0026quot;race_white\u0026quot; \u0026quot;race_black\u0026quot; \u0026quot;race_hispanic\u0026quot; ## [5] \u0026quot;race_asian\u0026quot; \u0026quot;p5numpla\u0026quot; \u0026quot;p5hmage\u0026quot; \u0026quot;p5hdage\u0026quot; ## [9] \u0026quot;w3daded_hsb\u0026quot; \u0026quot;w3momed_hsb\u0026quot; \u0026quot;w3momscr\u0026quot; \u0026quot;w3dadscr\u0026quot; ## [13] \u0026quot;w3income\u0026quot; \u0026quot;w3povrty\u0026quot; \u0026quot;p5fstamp\u0026quot; \u0026quot;c5r2mtsc\u0026quot; ## [17] \u0026quot;c5r2mtsc_std\u0026quot; (Example inspired by Simon Ejdemyr: https://sejdemyr.github.io/r-tutorials/statistics/tutorial8.html)\nReference links: MatchIt: https://cran.r-project.org/web/packages/MatchIt/vignettes/matchit.pdf Cobalt: (optional library for matching plots and extra features): https://cran.r-project.org/web/packages/cobalt/vignettes/cobalt_A0_basic_use.html kableExtra: (for formatting tables): https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html Stargazer: (for formatting model output): https://www.jakeruss.com/cheatsheets/stargazer/ Video overview of matching concepts: https://fr.coursera.org/lecture/crash-course-in-causality/overview-of-matching-JQfPC    Comparing Differences in Means and Balance Tables As you may have seen in this week‚Äôs application paper, balance tables feature prominently in work that utilizes matching.\nIn binary treatment setups, balance tables present an overview of the difference in means for the groups accross covariates.\nLet‚Äôs take the dataset as an example to review how to compare differences in means and build balance tables in R. There are multiple ways to explore these kinds of questions. In this lab we will leverage t-tests to check the statistical significance of the difference in means.\nIn other words, we want to know whether the observed differences in average value of a variable between the two groups or samples can be due to random chance (our null hypothesis).\nT-tests in R In this case, let‚Äôs imagine we want to check the statistical significance of the differences in the composition of the catholic and public school samples in the w3povrty (under the poverty line) variable.\nThe general syntax for a t-test is simply as follows. The vectors refer to the variables whose mean you want to compare.\nt.test(y ~ x, data = your_data) Exercise  t.test(w3povrty ~ catholic, data = ecls_df) ## ## Welch Two Sample t-test ## ## data: w3povrty by catholic ## t = 26.495, df = 4034.6, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## 0.1678107 0.1946307 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.21918633 0.03796562  What does the group 0 mean tell us? What does the group 1 mean tell us? Is the difference between catholic school and public school students statistically significant?   Balance tables in R Now let‚Äôs take a look at how can we create a simple and good looking balance table. The idea here is to compare the mean values of different variables across populations or groups. In our case, let‚Äôs see whether the catholic and public school groups differ across the covariates in the dataset:\nHere is one way to create the table (you can adapt this code for the assignment).\n# create a list with the covariates list_cov \u0026lt;- c(\u0026quot;race_white\u0026quot;, \u0026quot;race_black\u0026quot;, \u0026quot;race_hispanic\u0026quot;, \u0026quot;race_asian\u0026quot;, \u0026quot;p5numpla\u0026quot;, \u0026quot;p5hmage\u0026quot;, \u0026quot;p5hdage\u0026quot;, \u0026quot;w3daded_hsb\u0026quot;, \u0026quot;w3momed_hsb\u0026quot;, \u0026quot;w3momscr\u0026quot;, \u0026quot;w3dadscr\u0026quot;, \u0026quot;w3income\u0026quot;, \u0026quot;w3povrty\u0026quot;, \u0026quot;p5fstamp\u0026quot;, \u0026quot;c5r2mtsc\u0026quot;, \u0026quot;c5r2mtsc_std\u0026quot;) ecls_df %\u0026gt;% # our data frame dplyr::summarize_at(list_cov, funs(list(broom::tidy(t.test(. ~ catholic))))) %\u0026gt;% # sequentially run t-tests across all the covariates in the list_cov (note that you have to change the \u0026quot;treatment\u0026quot;) purrr::map(1) %\u0026gt;% # maps into a list dplyr::bind_rows(.id=\u0026#39;variables\u0026#39;) %\u0026gt;% # binds list into a single data frame and names the id column \u0026quot;variables\u0026quot; dplyr::select(variables, estimate1, estimate2, p.value) %\u0026gt;% # select only the names, group means, and p-values dplyr::mutate_if(is.numeric, round, 3) %\u0026gt;% # round numeric variables to three places knitr::kable(col.names = c(\u0026quot;Variable\u0026quot;, \u0026quot;(Catholic = 0)\u0026quot;, \u0026quot;(Catholic = 1)\u0026quot;, \u0026quot;P value\u0026quot;)) %\u0026gt;% # create kable table and remane headings kableExtra::kable_styling() # style kable table for our knitted document   Variable  (Catholic = 0)  (Catholic = 1)  P value      race_white  0.556  0.725  0.000    race_black  0.136  0.054  0.000    race_hispanic  0.181  0.131  0.000    race_asian  0.066  0.052  0.025    p5numpla  1.133  1.093  0.000    p5hmage  37.561  39.575  0.000    p5hdage  40.392  41.988  0.000    w3daded_hsb  0.488  0.259  0.000    w3momed_hsb  0.464  0.227  0.000    w3momscr  43.114  47.492  0.000    w3dadscr  42.713  46.356  0.000    w3income  54889.159  82074.301  0.000    w3povrty  0.219  0.038  0.000    p5fstamp  0.129  0.015  0.000    c5r2mtsc  50.209  52.389  0.000    c5r2mtsc_std  -0.031  0.194  0.000     Exercise   Are the differences in means significant at conventional levels? What differences strike you from the composition of the two samples?    The Effect of Catholic School on Student Achievement  In this tutorial we‚Äôll analyze the effect of going to Catholic school, as opposed to public school, on student achievement. Because students who attend Catholic school on average are different from students who attend public school, we will use matching to get more credible causal estimates of Catholic schooling.\n Exploration of the data ecls_df %\u0026gt;% dplyr::group_by(catholic) %\u0026gt;% dplyr::summarize(n_students = n(), avg_math = mean(c5r2mtsc_std), std_error = sd(c5r2mtsc_std) / sqrt(n_students)) %\u0026gt;% round(3) %\u0026gt;% # round the results knitr::kable() %\u0026gt;% # create kable table kableExtra::kable_styling() # view kable table   catholic  n_students  avg_math  std_error      0  9568  -0.031  0.010    1  1510  0.194  0.022     We can see that we have many more students that did not attend Catholic school than those who did. Also, the Catholic school students have a higher average math score.\n Naive Average Treatment Effect (NATE) We can naively compare the students on their standardized math scores (c5r2mtsc_std). As you remember, the Naive Average Treatment Effect (NATE) is the difference in the means of the observed outcomes of the two groups:\n\\[NATE = E(y^1 | D = 1) - E(y^0 | D = 0)\\]\nIn this case, the NATE would be difference between the average math scores.\nNATE manually Do you remember what we did in the last section?\nWe could substract the avg_math for the catholic = 1 and the avg_math for the catholic = 0\n\\[NATE = 0.194 - (-0.031)\\] \\[NATE = 0.194 + 0.031 = 0.225\\]\n NATE with t.test() t.test(c5r2mtsc_std ~ catholic, data = ecls_df) ## ## Welch Two Sample t-test ## ## data: c5r2mtsc_std by catholic ## t = -9.1069, df = 2214.5, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -0.2727988 -0.1761292 ## sample estimates: ## mean in group 0 mean in group 1 ## -0.03059583 0.19386817  NATE with lm() lm(c5r2mtsc_std ~ catholic, data = ecls_df) %\u0026gt;% stargazer::stargazer(.,type = \u0026quot;html\u0026quot;)       Dependent variable:           c5r2mtsc_std       catholic   0.224***      (0.028)         Constant   -0.031***      (0.010)            Observations   11,078    R2   0.006    Adjusted R2   0.006    Residual Std. Error   0.997 (df = 11076)    F Statistic   66.096*** (df = 1; 11076)       Note:  p\u0026lt;0.1; p\u0026lt;0.05; p\u0026lt;0.01    Exercise   What parallels do you find between substracting the manually extracted means, the t-test, and the linear regression?     Matching with MatchIt MatchIt is designed for causal inference with a dichotomous treatment variable and a set of pretreatment control variables. Any number or type of dependent variables can be used.\nWe have three steps:\n Perform the match with MatchIt::matchit() Create a new data frame with the matched data with MatchIt::match.data() or MatchIt::get_matches() Model  The basic syntax is as follows:\nmatch_process \u0026lt;- MatchIt::matchit(treatment ~ x1 + x2, data = mydata) # NOTE. We include treatment ~ covariates matched_df \u0026lt;- MatchIt::get_matches(match_process) #when matching with replacement matched_df \u0026lt;- MatchIt::match.data(match_process) #for other cases matched_model \u0026lt;- lm(outcome ~ trearment, data = matched_df) where treat is the dichotomous treatment variable, and x1 and x2 are pre-treatment co-variates, all of which are contained in the data frame mydata.\n As you can see, the outcome variable is not included in the matching procedure.   MatchIt is capable of using several matching methods:\n Exact (method = ‚Äúexact‚Äù): The simplest version of matching is exact. This technique matches each treated unit to all possible control units with exactly the same values on all the covariates, forming subclasses such that within each subclass all units (treatment and control) have the same covariate values.\n Subclassification (method = ‚Äúsubclass‚Äù): When there are many covariates (or some covariates can take a large number of values), finding sufficient exact matches will often be impossible. The goal of subclassification is to form subclasses, such that in each of them the distribution (rather than the exact values) of covariates for the treated and control groups are as similar as possible.\n Nearest Neighbor (method = ‚Äúnearest‚Äù): Nearest neighbor matching selects the best control matches for each individual in the treatment group. Matching is done using a distance measure (propensity score) specified by the distance option (default = logit).\n As well as optimal matching, full matching, genetic matching, and coarsened exact matching, all of which are detailed in the documentation.\n  A few additional arguments are important to know about:\n distance: this refers to propensity scores. There are many options for how to calculate these within MatchIt.\n discard: specifies whether to discard units that fall outside some measure of support of the distance measure (default is ‚Äúnone‚Äù, discard no units). For example, if some treated units have extremely high propensity scores that are higher than any control units, we could drop those.\n replace: a logical value indicating whether each control unit can be matched to more than one treated unit (default is replace = FALSE, each control unit is used at most once).\n ratio: the number of control units to match to each treated unit (default is ratio = 1).\n There are also some optional arguments for most of the matching methods, which you can read about in the documentation if you are interested.\n  Exact Matching We can use a combination of the results from our balance table and theory to identify which variables to use for matching. Let‚Äôs perform an exact match with:\n race_white: Is the student white (1) or not (0)? p5hmage: Mother‚Äôs age w3income: Family income p5numpla: Number of places the student has lived for at least 4 months w3momed_hsb: Is the mother‚Äôs education level high-school or below (1) or some college or more (0)?  # first we must omit missing values (MatchIt does not allow missings) match_data \u0026lt;- ecls_df %\u0026gt;% dplyr::select(catholic, c5r2mtsc_std, race_white, p5hmage, w3income, p5numpla, w3momed_hsb) %\u0026gt;% na.omit() # perform exact match exact_match \u0026lt;- MatchIt::matchit(catholic ~ race_white + p5hmage + w3income + p5numpla + w3momed_hsb, method = \u0026quot;exact\u0026quot;, data = match_data) # Try seeing the output in the console with summary(exact_match) # grab the matched data into a new data frame data_exact_match \u0026lt;- MatchIt::match.data(exact_match) # estimate effect again with new data frame exact_match_model \u0026lt;- lm(c5r2mtsc_std ~ catholic, data = data_exact_match) stargazer::stargazer(exact_match_model, type = \u0026quot;html\u0026quot;)       Dependent variable:           c5r2mtsc_std       catholic   -0.101***      (0.030)         Constant   0.319***      (0.015)            Observations   5,405    R2   0.002    Adjusted R2   0.002    Residual Std. Error   0.934 (df = 5403)    F Statistic   11.340*** (df = 1; 5403)       Note:  p\u0026lt;0.1; p\u0026lt;0.05; p\u0026lt;0.01    Now we can see that the mean in the group that did not attend Catholic school is actually about 0.10 higher than the mean for those who did. The results are statistically significant given that the confidence interval does not contain zero, and we have a fairly small p-value.\n Propensity Scores If we want to perform non-exact matching, we can use propensity scores. We can generate these manually using a logit model on the unmatched data set.\nWhen we extract propensity scores, we model the propensity of each unit of falling under the treatment group based on their values on a set of covariates. This is how we would do this manually:\n# create a new column with income by the thousands for more interpretable output ecls_df \u0026lt;- ecls_df %\u0026gt;% dplyr::mutate(w3income_1k = w3income / 1000) # estimate logit model m_ps \u0026lt;- glm(catholic ~ race_white + w3income_1k + p5hmage + p5numpla + w3momed_hsb, family = binomial(link = \u0026quot;logit\u0026quot;), # you can also use a probit link here data = ecls_df) # extract predicted probabilities # type = \u0026quot;response\u0026quot; option tells R to output probabilities of the form P(Y = 1|X) prs_df \u0026lt;- dplyr::tibble(pr_score = predict(m_ps, type = \u0026quot;response\u0026quot;), catholic = m_ps$model$catholic) # the actual values Let‚Äôs plot the propensity scores by treatment group to explore common support:\n# Histogram ggplot(prs_df, aes(x = pr_score, fill = factor(catholic))) + geom_histogram(alpha = 0.5) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(title = \u0026quot;Propensity Score Distribution: Treatment and Control Groups\u0026quot;, x = \u0026quot;Propensity Score\u0026quot;, y = \u0026quot;Count\u0026quot;, fill = \u0026quot;Catholic School Attendance\u0026quot;) # Density plot ggplot(prs_df, aes(x = pr_score, fill = factor(catholic))) + geom_density(alpha = 0.5) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(title = \u0026quot;Propensity Score Distribution: Treatment and Control Groups\u0026quot;, x = \u0026quot;Propensity Score\u0026quot;, y = \u0026quot;Density\u0026quot;, fill = \u0026quot;Catholic School Attendance\u0026quot;)  # Jittered point plot ggplot(prs_df, aes(x = pr_score, y = factor(catholic), color = factor(catholic))) + geom_jitter() + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(title = \u0026quot;Propensity Score Distribution: Treatment and Control Groups\u0026quot;, x = \u0026quot;Propensity Score\u0026quot;, y = \u0026quot;Group\u0026quot;, color = \u0026quot;Catholic School Attendance\u0026quot;)  Exercise   What do these plots tell us?   Non-Exact Matching MatchIt can generate propensity scores itself, so we don‚Äôt need to manually go through the process above. Let‚Äôs try putting together a non-exact matching formula yourself! Try:\n nearest neighbor matching with replacement with a one-to-one ratio on the match_data dataset  one_match \u0026lt;- MatchIt::matchit(catholic ~ race_white + w3income + p5hmage + p5numpla + w3momed_hsb, method = \u0026quot;nearest\u0026quot;, ratio = 1, replace = TRUE, data = match_data) summary(one_match) ## ## Call: ## MatchIt::matchit(formula = catholic ~ race_white + w3income + ## p5hmage + p5numpla + w3momed_hsb, data = match_data, method = \u0026quot;nearest\u0026quot;, ## replace = TRUE, ratio = 1) ## ## Summary of Balance for All Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean ## distance 0.1927 0.1379 0.6486 1.0007 0.2086 ## race_white 0.7411 0.5914 0.3418 . 0.1497 ## w3income 82568.9357 55485.0210 0.5777 1.1373 0.1565 ## p5hmage 39.5932 37.5658 0.3874 0.6383 0.0408 ## p5numpla 1.0917 1.1298 -0.1242 0.6132 0.0076 ## w3momed_hsb 0.2234 0.4609 -0.5703 . 0.2375 ## eCDF Max ## distance 0.3109 ## race_white 0.1497 ## w3income 0.3062 ## p5hmage 0.1893 ## p5numpla 0.0277 ## w3momed_hsb 0.2375 ## ## ## Summary of Balance for Matched Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean ## distance 0.1927 0.1927 0.0000 0.9954 0.0000 ## race_white 0.7411 0.7493 -0.0186 . 0.0081 ## w3income 82568.9357 81775.6653 0.0169 1.0052 0.0036 ## p5hmage 39.5932 39.6169 -0.0045 1.0179 0.0015 ## p5numpla 1.0917 1.0777 0.0459 1.1580 0.0031 ## w3momed_hsb 0.2234 0.2226 0.0018 . 0.0007 ## eCDF Max Std. Pair Dist. ## distance 0.0030 0.0001 ## race_white 0.0081 0.0625 ## w3income 0.0081 0.0396 ## p5hmage 0.0074 0.1131 ## p5numpla 0.0126 0.0846 ## w3momed_hsb 0.0007 0.0586 ## ## Percent Balance Improvement: ## Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 100.0 -552.2 100.0 99.0 ## race_white 94.6 . 94.6 94.6 ## w3income 97.1 95.9 97.7 97.3 ## p5hmage 98.8 96.0 96.2 96.1 ## p5numpla 63.1 70.0 59.2 54.6 ## w3momed_hsb 99.7 . 99.7 99.7 ## ## Sample Sizes: ## Control Treated ## All 7915. 1352 ## Matched (ESS) 187.29 1352 ## Matched 510. 1352 ## Unmatched 7405. 0 ## Discarded 0. 0 We can interpret the resulting output as follows:\n Summary of balance for all data: Comparison of the means for all the data without matching Summary of balance for matched data: Comparison of means for matched data. Looking for them to become similar. Percent balance improvement: Higher is better, close to 100 is ideal. Sample sizes: How many units were matched in the control/treatment groups.  Now, let‚Äôs plot the propensity scores for the treated and untreated units.\n# simple plot - check out the cobalt package for nicer options, or use ggplot2 to create your own! plot(one_match, type = \u0026quot;hist\u0026quot;) Let‚Äôs extract the data from one_match and creating a balance table like the one we did before, just this time using the new data. Scroll down for the answer when you‚Äôre ready.\n# grab data set data_prop_match \u0026lt;- MatchIt::get_matches(one_match) # create list of covariates for the table list_cov \u0026lt;- c(\u0026quot;race_white\u0026quot;, \u0026quot;p5hmage\u0026quot;, \u0026quot;w3income\u0026quot;, \u0026quot;p5numpla\u0026quot;, \u0026quot;w3momed_hsb\u0026quot;) data_prop_match %\u0026gt;% # our data frame dplyr::summarize_at(list_cov, funs(list(broom::tidy(t.test(. ~ catholic))))) %\u0026gt;% # sequentially run t-tests across all the covariates in the list_cov (note that you have to change the \u0026quot;treatment\u0026quot;) purrr::map(1) %\u0026gt;% # maps into a list dplyr::bind_rows(.id=\u0026#39;variables\u0026#39;) %\u0026gt;% # binds list into a single data frame and names the id column \u0026quot;variables\u0026quot; dplyr::select(variables, estimate1, estimate2, p.value) %\u0026gt;% # select only the names, group means, and p-values dplyr::mutate_if(is.numeric, round, 3) %\u0026gt;% # round numeric variables to three places knitr::kable(col.names = c(\u0026quot;Variable\u0026quot;, \u0026quot;Control (Catholic = 0)\u0026quot;, \u0026quot;Treat (Catholic = 1)\u0026quot;, \u0026quot;P value\u0026quot;)) %\u0026gt;% # create kable table and rename headings kableExtra::kable_styling() # style kable table for our knitted document   Variable  Control (Catholic = 0)  Treat (Catholic = 1)  P value      race_white  0.749  0.741  0.628    p5hmage  39.617  39.593  0.906    w3income  81775.665  82568.936  0.659    p5numpla  1.078  1.092  0.216    w3momed_hsb  0.223  0.223  0.963     Those means look very close. Hooray.\nFinally, let‚Äôs estimate on the matched data set:\nprop_match_model \u0026lt;- lm(c5r2mtsc_std ~ catholic, data = data_prop_match) stargazer::stargazer(prop_match_model, type = \u0026quot;text\u0026quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## c5r2mtsc_std ## ----------------------------------------------- ## catholic -0.038 ## (0.037) ## ## Constant 0.248*** ## (0.026) ## ## ----------------------------------------------- ## Observations 2,704 ## R2 0.0004 ## Adjusted R2 0.00003 ## Residual Std. Error 0.955 (df = 2702) ## F Statistic 1.068 (df = 1; 2702) ## =============================================== ## Note: *p\u0026lt;0.1; **p\u0026lt;0.05; ***p\u0026lt;0.01 As with the exact matching, we can see that those that did not attend Catholic school performed better on the test than those who did. Still, we see that our results in this instance are not statistically significant.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9fd079130cd1bf6c9a57d9d1d257c687","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-5/05-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-5/05-online-tutorial/","section":"materials","summary":"Welcome Introduction! Welcome to our fifth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you reviewed randomization in experimental setups.","tags":null,"title":"Matching in R","type":"book"},{"authors":null,"categories":null,"content":" Welcome Introduction! Welcome to our fourth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week's lecture you reviewed bivariate and multiple linear regressions. You also learned how Directed Acyclic Graphs (DAGs) can be leveraged to gather causal estimates.\nIn this lab session we will:\n Use the ggdag and dagitty packages to assess your modeling strategy Review how to run regression models using R Illustrate omitted variable and collider bias  Packages # These are the libraries we will use today. Make sure to install them in your console in case you have not done so previously. set.seed(42) #for consistent results in randomization library(wooldridge) # To get our example\u0026#39;s dataset library(tidyverse) # To use dplyr functions and the pipe operator when needed library(ggplot2) # To visualize data (this package is also loaded by library(tidyverse)) library(ggdag) # To dagify and plot our DAG objects in R library(dagitty) # To perform analysis in our DAG objects in R library(stargazer) # To render nicer regression output data(\u0026quot;wage1\u0026quot;) # calls the wage1 dataset from the woorldridge package    Working with DAGs in R Last week we learned about the general syntax of the ggdag package:\n We created dagified objects with ggdag::dagify() We plotted our DAGs with ggdag::ggdag() We discussed how to specify the coordinates of our nodes with a coordinate list  Today, we will learn how the ggdag and dagitty packages can help us illustrate our paths and adjustment sets to fulfill the backdoor criterion\nLet's take one of the DAGs from our review slides:\ncoord_dag \u0026lt;- list( x = c(d = 0, p = 0, b = 1, a = 1 , c = 2, y = 2), y = c(d = 0, p = 2, b = 1, a = -1, c = 2, y = 0) ) our_dag \u0026lt;- ggdag::dagify(d ~ p + a, # p and a pointing at d b ~ p + c, # p and c pointing at b y ~ d + a + c, # d, a, and c pointing at y coords = coord_dag, # our coordinates from the list up there exposure = \u0026quot;d\u0026quot;, # we declare out exposure variable outcome = \u0026quot;y\u0026quot;) # we declare out outcome variable ggdag::ggdag(our_dag) + theme_dag() # equivalent to theme_void() Learning about our paths and what adjustments we need As you have seen, when we dagify a DAG in R a dagitty object is created. These objects tell R that we are dealing with DAGs.\nThis is very important because in addition to plotting them, we can do analyses on the DAG objects. A package that complements ggdag is the dagitty package.\nToday, we will focus on two functions from the dagitty package:\n dagitty::paths(): Returns a list with two components: paths, which gives the actual paths, and open, which shows whether each path is open (d-connected) or closed (d-separated). dagitty::adjustmentSets(): Lists the sets of covariates that would allow for unbiased estimation of causal effects, assuming that the causal graph is correct.  We just need to input our DAG object.\nPaths Let's see how the output of the dagitty::paths function looks like:\ndagitty::paths(our_dag) ## $paths ## [1] \u0026quot;d -\u0026gt; y\u0026quot; \u0026quot;d \u0026lt;- a -\u0026gt; y\u0026quot; \u0026quot;d \u0026lt;- p -\u0026gt; b \u0026lt;- c -\u0026gt; y\u0026quot; ## ## $open ## [1] TRUE TRUE FALSE We see under $paths the three paths we declared during the manual exercise:\n d -\u0026gt; y d \u0026lt;- a -\u0026gt; y d \u0026lt;- p -\u0026gt; b \u0026lt;- c -\u0026gt; y  Additionally, $open tells us whether each path is open. In this case, we see that the second path is the only open non-causal path, so we would need to condition on a to close it.\nWe can also use ggdag to present the open paths visually with the ggdag_paths() function, as such:\nggdag::ggdag_paths(our_dag) + theme_dag()  Covariate adjustment In addition to listing all the paths and sorting the backdoors manually, we can use the dagitty::adjustmentSets() function.\nWith this function, we just need to input our DAG object and it will return the different sets of adjustments.\ndagitty::adjustmentSets(our_dag) ## { a } For example, in this DAG there is only one option. We need to control for a.\nWe can also use ggdag to present the open paths visually with the ggdag_adjustment_set() function, as such:\nAlso, do not forget to set the argument shadow = TRUE, so that the arrows from the adjusted nodes are included.\nggdag::ggdag_adjustment_set(our_dag, shadow = T) + theme_dag() If you want to learn more about DAGs in R   ggdag documentation: https://ggdag.malco.io/ dagitty vignette: https://cran.r-project.org/web/packages/dagitty/dagitty.pdf What is `dagitty: https://cran.r-project.org/web/packages/dagitty/vignettes/dagitty4semusers.html  NOW LET'S MOVE TO REGRESSION\n   Introduction to Regression Linear regression is largely used to predict the value of an outcome variable based on one or more input explanatory variables. As we previously discussed, regression addresses a simple mechanical problem, namely, what is our best guess of y given an observed x.\n Regression can be utilized without thinking about causes as a predictive or summarizing tool It would not be appropiate to give causal interpretations to any \\(\\beta\\), unless we establish the fulfilment of centain assumptions  Bivariate regression In bivariate regression, we are modeling a variable \\(y\\) as a mathematical function of one variable \\(x\\). We can generalize this in a mathematical equation as such:\n\\[y = \\beta_{0} + \\beta{1}x + œµ\\]\n Multiple linear regression In multiple linear regression, we are modeling a variable \\(y\\) as a mathematical function of multiple variables \\((x, z, m)\\). We can generalize this in a mathematical equation as such:\n\\[y = \\beta_{0} + \\beta_{1}x + \\beta_{2}z + \\beta_{3}m + œµ\\]\n  Exploratory questions Let's illustrate this with an example\nWe will use the wage1 dataset from the wooldridge package. These are data from the 1976 Current Population Survey used by Jeffrey M. Wooldridge with pedagogical purposes in his book on Introductory Econometrics.\nIf you want to check the contents of the wage1 data frame, you can type ?wage1 in your console\nVisualizing With regression we can answer EXPLORATORY QUESTIONS. For example:   What is the relationship between education and respondents' salaries?  We can start by exploring the relationship visually with our newly attained ggplot2 skills:\nggplot(wage1, aes(x = educ, y = wage)) + geom_point(color = \u0026quot;grey60\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F, color = \u0026quot;#CC0055\u0026quot;) + theme_minimal() + labs(x = \u0026quot;Years of education\u0026quot;, y = \u0026quot;Hourly wage (USD)\u0026quot;)   The lm() function This question can be formalized mathematically as:\n\\[Hourly\\ wage = \\beta_0 + \\beta_1Years\\ of\\ education + œµ\\]\nOur interest here would be to build a model that predicts the hourly wage of a respondent (our outcome variable) using the years of education (our explanatory variable). Fortunately for us, R provides us with a very intuitive syntax to model regressions.\nThe general syntax for running a regression model in R is the following:\nyour_model_biv \u0026lt;- lm(outcome_variable ~ explanarory_variable, data = your_dataset) #for a bivariate regression your_model_mult \u0026lt;- lm(outcome_variable ~ explanarory_variable_1 + explanarory_variable_2, data = your_dataset) #for multiple regression Now let's create our own model and save it into the model_1 object, based on the bivariate regression we specified above in which wage is our outcome variable, educ is our explanatory variable, and our data come from the wage1 object:\nmodel_1 \u0026lt;- lm(wage ~ educ, data = wage1)  summary() and broom::tidy() We have created an object that contains the coefficients, standard errors and further information from your model. In order to see the estimates, you could use the base R function summary(). This function is very useful when you want to print your results in your console.\nAlternatively, you can use the tidy() function from the broom package. The function constructs a data frame that summarizes the model‚Äôs statistical findings. You can see what else you can do with broom by running: vignette(‚Äúbroom‚Äù). The broom::tidy() function is useful when you want to store the values for future use (e.g., visualizing them)\nLet's try both options in the console up there. You just need to copy this code below the model_1 code.\nsummary(model_1) broom::tidy(model_1)  Exercise \\[Hourly\\ wage = \\beta_0 + \\beta_1Years\\ of\\ education + œµ\\]\n       Dependent variable:           Hourly wage       Years of education   0.541***      (0.053)         Constant   -0.905      (0.685)            Observations   526    R2   0.165    Adjusted R2   0.163    Residual Std. Error   3.378 (df = 524)    F Statistic   103.363*** (df = 1; 524)       Note:  p\u0026lt;0.1; p\u0026lt;0.05; p\u0026lt;0.01     How would you interpret the results of our model_1?  What does the constant mean? What does the educ coefficient mean? Do these coefficient carry any causal meaning?    Adding more nuance to our models As we have discussed in previous sessions we live in a very complex world. It is very likely that our exploration of the relationship between education and respondents' salaries is open to multiple sources of bias.\nLooking back at 1976 US, can you think of possible variables inside the mix?\n How about the sex or the ethnicity of a worker? Let's explore this visually  What is the relationship between education and respondents' salaries conditional on the sex of the worker?  ggplot(wage1, aes(x = educ, y = wage, color = as.factor(female))) + geom_point(alpha = 0.3) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + labs(x = \u0026quot;Years of education\u0026quot;, y = \u0026quot;Hourly wage (USD)\u0026quot;, color = \u0026quot;Female\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) Check what happens when we replace the color = as.factor(female) for color = female\n What insights can we gather from this graph?   Multiple linear regression This question can be formalized mathematically as:\n\\[Hourly\\ wage = \\beta_0 + \\beta_1Years\\ of\\ education + \\beta_2Female + œµ\\]\nOur interest here would be to build a model that predicts the hourly wage of a respondent (our outcome variable) using the years of education and their sex (our explanatory variables).\nLet's remember the syntax for running a regression model in R:\nyour_model_biv \u0026lt;- lm(outcome_variable ~ explanarory_variable, data = your_dataset) #for a bivariate regression your_model_mult \u0026lt;- lm(outcome_variable ~ explanarory_variable_1 + explanarory_variable_2, data = your_dataset) #for multiple regression Now let's create our own model, save it into the model_2 object, and print the results based on the formula regression we specified above in which wage is our outcome variable, educ and female are our explanatory variables, and our data come from the wage1 object:\nmodel_2 \u0026lt;- lm(wage ~ educ + female, data = wage1) summary(model_2) ## ## Call: ## lm(formula = wage ~ educ + female, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.9890 -1.8702 -0.6651 1.0447 15.4998 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.62282 0.67253 0.926 0.355 ## educ 0.50645 0.05039 10.051 \u0026lt; 2e-16 *** ## female -2.27336 0.27904 -8.147 2.76e-15 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 3.186 on 523 degrees of freedom ## Multiple R-squared: 0.2588, Adjusted R-squared: 0.256 ## F-statistic: 91.32 on 2 and 523 DF, p-value: \u0026lt; 2.2e-16  Exercise \\[Hourly\\ wage = \\beta_0 + \\beta_1Years\\ of\\ education + \\beta_2Female + œµ\\]\n      Dependent variable:           Hourly wage       Years of education   0.506***      (0.050)         Female   -2.273***      (0.279)         Constant   0.623      (0.673)            Observations   526    R2   0.259    Adjusted R2   0.256    Residual Std. Error   3.186 (df = 523)    F Statistic   91.315*** (df = 2; 523)       Note:  p\u0026lt;0.1; p\u0026lt;0.05; p\u0026lt;0.01    How would you interpret the results of our model_2?  What does the constant mean? What does the educ coefficient mean? What does the female coefficient mean?   Predicting from our models As we discussed previously, when we do not have our causal inference hats on, the main goal of linear regression is to predict an outcome value on the basis of one or multiple predictor variables.\nR has a generic function predict() that helps us arrive at the predicted values on the basis of our explanatory variables.\nThe syntax of predict() is the following:\npredict(name_of_the_model, newdata = data.frame(explanatory1 = value, explanatory2 = value))  Say that based on our model_2, we are interested in the expected average hourly wage of a woman with 15 years of education.\n predict(model_2, newdata = data.frame(educ = 15, female = 1)) ## 1 ## 5.946237  What does this result tell us? What happens when you change female to 0? What does the result mean? Can you think of a way to find the difference in the expected hourly wage between a male with 16 years of education and a female with 17?  predict(model_2, newdata = data.frame(educ = 16, female = 0)) - predict(model_2, newdata = data.frame(educ = 15, female =0)) ## 1 ## 0.5064521   Quiz Here are some questions for you. Note that there are multiple ways to reach the same answer:\nWhat is the expected hourly wage of a male with 15 years of education?\n$8.22 $9.50 5.34 $3  How much more on average does a male worker earn than a female counterpart?\u0026quot;,\n$2.27 In our data, males on average earn less than females $1.20 $4.50  How much more is a worker expected to earn for every additional year of education, keeping sex constant?\n$0.90 $1.20 $0.5   DAGs and modeling As we can remember from our slides, we were introduced to a set of key rules in understanding how to employ DAGs to guide our modeling strategy.\n A path is open or unblocked at non-colliders (confounders or mediators) A path is (naturally) blocked at colliders An open path induces statistical association between two variables Absence of an open path implies statistical independence Two variables are d-connected if there is an open path between them Two variables are d-separated if the path between them is blocked  In this portion of the tutorial we will demonstrate how different bias come to work when we model our relationships of interest.\n What happens when we control for a collider? The case for beauty, talent, and celebrity (What happens when we control for a collider?)  As it is showcased from our DAG, we assume that earning celebrity status is a function of an individuals beauty and talent.\nWe will simulate data that reflects this assumptions. In our world, someone gains celebrity status if the sum of units of beauty and celebrity are greater than 8.\n# beauty - 1000 observations with mean 5 units of beauty and sd 1.5 (arbitrary scale) beauty \u0026lt;- rnorm(1000, 5, 1.5) # talent - 1000 observations with mean 3 units of talent and sd 1 (arbitrary scale) talent \u0026lt;- rnorm(1000, 3, 1) # celebrity - binary celebrity_status \u0026lt;- ifelse(beauty + talent \u0026gt; 8, \u0026quot;Celebrity\u0026quot; , \u0026quot;Not Celebrity\u0026quot;) # celebrity if the sum of units are greater than 8 celebrity_df \u0026lt;- dplyr::tibble(beauty, talent, celebrity_status) # we make a df with our values head(celebrity_df, 10) ## # A tibble: 10 x 3 ## beauty talent celebrity_status ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 7.06 5.33 Celebrity ## 2 4.15 3.52 Not Celebrity ## 3 5.54 3.97 Celebrity ## 4 5.95 3.38 Celebrity ## 5 5.61 2.00 Not Celebrity ## 6 4.84 2.40 Not Celebrity ## 7 7.27 3.17 Celebrity ## 8 4.86 0.0715 Not Celebrity ## 9 8.03 2.15 Celebrity ## 10 4.91 3.80 Celebrity In this case, as our simulation suggest, we have a collider structure. We can see that celebrity can be a function of beauty or talent. Also, we can infer from the way we defined the variables that beauty and talent are d-separated (ie. the path between them is closed because celebrity is a collider).\nSay you are interested in researching the relationship between beauty and talent for your Master's thesis, while doing your literature review you encounter a series of papers that find a negative relationship between the two and state that more beautiful people tend to be less talented. The model that these teams of the researchers used was the following:\n\\[Y_{Talent} = \\beta_0 + \\beta_1Beauty + \\beta_2Celebrity\\]\nYour scientific hunch makes you believe that celebrity is a collider and that by controlling for it in their models, the researchers are inducing collider bias, or endogenous bias. You decide to move forward with your thesis by laying out a criticism to previous work on the field, given that you consider the formalization of their models is erroneous. You utilize the same data previous papers used, but based on your logic, you do not control for celebrity status. This is what you find:\nTrue model true_model_celebrity \u0026lt;- lm(talent ~ beauty, data = celebrity_df) summary(true_model_celebrity) ## ## Call: ## lm(formula = talent ~ beauty, data = celebrity_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.9225 -0.6588 -0.0083 0.6628 3.5877 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 2.962209 0.107595 27.531 \u0026lt;2e-16 *** ## beauty 0.006545 0.020755 0.315 0.753 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.9865 on 998 degrees of freedom ## Multiple R-squared: 9.964e-05, Adjusted R-squared: -0.0009023 ## F-statistic: 0.09945 on 1 and 998 DF, p-value: 0.7526 ggplot(celebrity_df, aes(x=beauty, y=talent)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(x = \u0026quot;Beauty\u0026quot;, y = \u0026quot;Talent\u0026quot;)  Biased model from previous literature Let's see:\nbiased_model_celibrity \u0026lt;- lm(talent ~ beauty + celebrity_status, data = celebrity_df) summary(biased_model_celibrity) ## ## Call: ## lm(formula = talent ~ beauty + celebrity_status, data = celebrity_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4244 -0.5394 0.0110 0.5064 2.9429 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.37834 0.13983 38.46 \u0026lt;2e-16 *** ## beauty -0.32668 0.02265 -14.43 \u0026lt;2e-16 *** ## celebrity_statusNot Celebrity -1.51375 0.06808 -22.24 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.807 on 997 degrees of freedom ## Multiple R-squared: 0.3316, Adjusted R-squared: 0.3302 ## F-statistic: 247.3 on 2 and 997 DF, p-value: \u0026lt; 2.2e-16 ggplot(celebrity_df, aes(x=beauty, y=talent, color = celebrity_status)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(x = \u0026quot;Beauty\u0026quot;, y = \u0026quot;Talent\u0026quot;, color = \u0026quot;\u0026quot;)  As we can see, by controlling for a collider, the previous literature was inducing to a non-existent association between beauty and talent, also known as collider or endogenous bias.\n   What happens when we fail to control for a confounder? Shoe size and salary (What happens when we fail to control for a confounder?)  # sex - replicate male and female 500 times each sex \u0026lt;- rep(c(\u0026quot;Male\u0026quot;, \u0026quot;Female\u0026quot;), each = 500) # shoe size - random number with mean 38 and sd 4, plus 4 if the observation is male shoesize \u0026lt;- rnorm(1000, 38, 2) + (4 * as.numeric(sex == \u0026quot;Male\u0026quot;)) # salary - a random number with mean 25 and sd 2, plus 5 if the observation is male salary \u0026lt;- rnorm(1000, 25, 2) + (5 * as.numeric(sex == \u0026quot;Male\u0026quot;)) salary_df \u0026lt;- dplyr::tibble(sex, shoesize, salary) head(salary_df, 10) ## # A tibble: 10 x 3 ## sex shoesize salary ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Male 42.5 28.6 ## 2 Male 41.4 28.4 ## 3 Male 38.6 29.2 ## 4 Male 38.0 27.7 ## 5 Male 39.4 32.2 ## 6 Male 42.7 28.2 ## 7 Male 41.7 32.6 ## 8 Male 40.5 27.1 ## 9 Male 40.4 31.7 ## 10 Male 43.1 28.8 Say now one of your peers tells you about this new study that suggests that shoe size has an effect on an individuals' salary. You are a bit skeptic and read it. The model that these researchers apply is the following:\n\\[Y_{Salary} = \\beta_0 + \\beta_1ShoeSize\\]\nYour scientific hunch makes you believe that this relationship could be confounded by the sex of the respondent. You think that by failing to control for sex in their models, the researchers are inducing omitted variable bias. You decide to open their replication files and control for sex. This is what you find:\n\\[Y_{Salary} = \\beta_0 + \\beta_1ShoeSize + \\beta_2Sex\\]\nTrue model true_model_salary \u0026lt;- lm(salary ~ shoesize + sex, data = salary_df) summary(true_model_salary) ## ## Call: ## lm(formula = salary ~ shoesize + sex, data = salary_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.2341 -1.3698 -0.0501 1.3595 6.4303 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 25.73879 1.15886 22.210 \u0026lt;2e-16 *** ## shoesize -0.02030 0.03044 -0.667 0.505 ## sexMale 5.05924 0.17616 28.720 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.981 on 997 degrees of freedom ## Multiple R-squared: 0.6129, Adjusted R-squared: 0.6121 ## F-statistic: 789.2 on 2 and 997 DF, p-value: \u0026lt; 2.2e-16 ggplot(salary_df, aes(x=shoesize, y=salary, color = sex)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(x = \u0026quot;Shoe size\u0026quot;, y = \u0026quot;Salary\u0026quot;, color = \u0026quot;\u0026quot;)  Biased model from previous literature biased_model_salary \u0026lt;- lm(salary ~ shoesize, data = salary_df) summary(biased_model_salary) ## ## Call: ## lm(formula = salary ~ shoesize, data = salary_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.8777 -1.9101 -0.0511 1.8496 7.9774 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.68865 1.17280 3.145 0.00171 ** ## shoesize 0.59429 0.02925 20.319 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.676 on 998 degrees of freedom ## Multiple R-squared: 0.2926, Adjusted R-squared: 0.2919 ## F-statistic: 412.9 on 1 and 998 DF, p-value: \u0026lt; 2.2e-16 ggplot(salary_df, aes(x=shoesize, y=salary)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + labs(x = \u0026quot;Shoe size\u0026quot;, y = \u0026quot;Salary\u0026quot;)  As we can see, by failing to control for a confounder, the previous literature was creating a non-existent association between shoe size and salary, incurring in ommited variable bias.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e2b7b722f70588957b6d443be9de254","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-4/04-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-4/04-online-tutorial/","section":"materials","summary":"Welcome Introduction! Welcome to our fourth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week's lecture you reviewed bivariate and multiple linear regressions.","tags":null,"title":"The Backdoor Criterion and Basics of Regression in R","type":"book"},{"authors":null,"categories":null,"content":"  The POF in practice Let's revisit the example from our slides once again.\nSay we are interested in assessing the premise of Allport's hypothesis about interpersonal contact being conducive to reducing intergroup prejudice.\nWe are studying a set of (\\(n=8\\)) students assigned to a dorm room with a person from their own ethnic group (contact=0) and from a different group (contact=1).\n  Student (i) Prejudice (C=0) Prejudice (C=1)    1 6 5  2 4 2  3 4 4  4 6 7  5 3 1  6 2 2  7 8 7  8 4 5    Data set Today we will work with the prejudice_df object. The data frame contains the following four variables:\n student_id: numeric student identification prej_0: prejudice level under \\(Y_{0i}\\) (Contact=0) prej_1: prejudice level under \\(Y_{1i}\\) (Contact=1) dorm_type: binary for actual treatment state  ## # A tibble: 8 x 4 ## student_id prej_0 prej_1 dorm_type ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 6 5 0 ## 2 2 4 2 1 ## 3 3 4 4 0 ## 4 4 6 7 0 ## 5 5 3 1 1 ## 6 6 2 2 1 ## 7 7 8 7 0 ## 8 8 4 5 0   Treatment Effects a) Individual Treatment Effect (ITE) We assume from the potential outcomes framework that each subject has a potential outcome under both treatment states. Let's take the first student in the list as an example.\nThe figure illustrates the potential outcomes for Student 1.\nWe see that in a reality where Student 1 is assigned to in-group dorm (contact=0) their levels of prejudice are 6. On the contrary, in a reality where Student 1 is assigned to co-ethnic dorm (contact=1) their levels of prejudice are 5.\nFrom this illustration, we can gather the individual treatment effect (ITE) for student one. The ITE is equal to the values under treatment (contact=1) minus to the values without treatment (contact=0) or \\(ITE = y_{1i} - y_{0i}\\).\n\\[ITE = 5 - 6 = -1\\]\n As it was put in Cunningham\u0026rsquo;s book:\nThe ITE is a ‚Äúcomparison of two states of the world‚Äù (Cunningham, 2021): individuals are exposed to contact, and not exposed to it.\n  Evidently, each subject can only be observed in one treatment state at any point in time in real life. This is known as the fundamental problem (Holland, 1986) of causal inference. The Individual Treatment Effect (ITE) in reality is unattainable. Still, it provides us with a conceptual foundation for causal estimation.\nExercise: Our data are coming from a world with perfect information. In that sense, we have both potential outcomes prej_0 and prej_1. Can you think of a way to calculate the ITE for the eight students with one of the dplyr verbs we learned in the previous section?\nHint Can you think of a way we can use the verb mutate()\n Answer #you can employ dplyr::mutate() to create the new variable ite prejudice_df %\u0026gt;% dplyr::mutate(ite = prej_1 - prej_0)   student_id  prej_0  prej_1  dorm_type  ite      1  6  5  0  -1    2  4  2  1  -2    3  4  4  0  0    4  6  7  0  1    5  3  1  1  -2    6  2  2  1  0    7  8  7  0  -1    8  4  5  0  1       Average Treatment Effect (ATE) Normally, we are not interested in the estimates of individual subjects, but rather a population. The Average Treatment Effect (ATE) is the difference in the average potential outcomes of the population.\n\\[ATE = E(Y_{1i}) - E(Y_{0i})\\]\nIn other words, the ATE is the average ITE of all the subjects in the population. As you can see, the ATE as defined in the formula is also not attainable. Can you think why?\nExercise: Since our data are coming from a world with perfect information. Can you think of a way to calculate the ATE for the eight students based on what we learned last week?\nHint We have already extracted the ite with mutate(). We know that the the ATE is the averge of every subject's ITE. Do you remember summarize()?\n Answer #we know that the ATE is the averge of every subject\u0026#39;s ITE. Do you remember dplyr::summarize()? #how can we use the verbs from last week to get the average treatment effect? prejudice_df %\u0026gt;% dplyr::mutate(ite = prej_1 - prej_0) %\u0026gt;% dplyr::summarize(ate=mean(ite))   ate      -0.5       The Average Treatment Effect Among the Treated and Control (ATT) and (ATC) The names for these two estimates are very self-explanatory. These two estimates are simply the average treatment effects conditional on the group subjects are assigned to.\nThe average treatment effect on the treated ATT is defined as the difference in the average potential outcomes for those subjects who were treated: \\[ATT = E(Y_{1i}-Y_{0i} | D = 1)\\]\nThe average treatment effect under control ATC is defined as the difference in the average potential outcomes for those subjects who were not treated: \\[ATC = E(Y_{1i}-Y_{0i} | D = 0)\\]\nExercise: Since our data are coming from a world with perfect information. Can you think of a way to calculate the ATT and ATC for the eight students based on what we learned last week?\nHint We have already extracted the ite with mutate(). We know that the ATT and ATC are the average of every subject's ITE grouped by their treatment status. Do you remember how the combination of group_by() and summarize() worked?\n Answer #we know that the ATT and ATC are the average of every subject\u0026#39;s ITE grouped by their treatment status. Do you remember how the combination of dplyr::group_by() and dplyr::summarize() worked? #how can we use the verbs from last week to get the average treatment effect on the treated and untreated? prejudice_df %\u0026gt;% dplyr::mutate(ite = prej_1 - prej_0) %\u0026gt;% dplyr::group_by(dorm_type) %\u0026gt;% dplyr::summarize(treatment_effects=mean(ite))   dorm_type  treatment_effects      0  0.000000    1  -1.333333       The Naive Average Treatment Effect (NATE) So far, we have worked with perfect information. Still, we know that in reality we can only observe subjects in one treatment state. This is the information we do have.\n The Naive Average Treatment Effect (NATE) is the calculation we can compute based on the observed outcomes.   \\[NATE = E(Y_{1i}|D{i}=1) - E(Y_{0i}|D{i}=0)\\] *reads in English as: \u0026quot;The expected average outcome under treatment for those treated minus the expected average outcome under control for those not treated\u0026quot;\nExercise: Can you think of a way to calculate the NATE for the eight students employing the new observed_prej variable?\nprejudice_df %\u0026gt;% dplyr::mutate(observed_prej = ifelse(dorm_type == 1, prej_1, prej_0))   student_id  prej_0  prej_1  dorm_type  observed_prej      1  6  5  0  6    2  4  2  1  2    3  4  4  0  4    4  6  7  0  6    5  3  1  1  1    6  2  2  1  2    7  8  7  0  8    8  4  5  0  4     Hint We have already extracted the average observed outcomes depending on the treatment status with mutate(). We know that the NATE is the difference in average observed outcomes grouped by their treatment status. Do you remember how the combination of group_by() and summarize() worked?\n Answer #we know that the NATE is the difference in average observed outcomes grouped by their treatment status. Do you remember how the combination of dplyr::group_by() and dplyr::summarize() worked? prejudice_df %\u0026gt;% dplyr::mutate(observed_prej = ifelse(dorm_type == 1, prej_1, prej_0)) %\u0026gt;% dplyr::group_by(dorm_type) %\u0026gt;% dplyr::summarize(mean(observed_prej)) #You can just substract the values   dorm_type  mean(observed_prej)      0  5.600000    1  1.666667     You can just substract the values\n Note. The √¨felse() function is a very handy tool to have. It allows us to generate conditional statements. The syntax is the following:\nifelse(condition_to_meet, what_to_do_if_met, what_to_do_if_not_met) In the case of observed_prej, we ask R to create a new variable, where if the subject is in a co-ethnic dorm, we print the prejudice value under treatment. If that condition is not met, we print the prejudice value under control.\n  Bias Bias During the lecture, we met two sources of bias:\n Baseline bias Baseline bias‚Äîalso known as selection bias‚Äî is difference in expected outcomes in the absence of treatment for the actual treatment and control group. In other words, these are the underlying differences that individuals in either group start off with.\n Differential treatment effect bias Differential treatment effect bias ‚Äî also known as Heterogeneous Treatment Effect (HTE) bias ‚Äî is the difference in returns to treatment (the treatment effect) between the treatment and control group, multiplied by the share of the population in control. In other words, this type of bias relates to the dissimilarities stemming for ways in which individuals in either group are affected differently by the treatment.\nWe will let you think about these for the mock assignment\nExercise: Since our data are coming from a world with perfect information. Can you think of a way to explore the existence baseline bias in our data?\nHint We know that the baseline bias is the difference in average observed outcomes under control grouped by their treatment status. Do you remember how the combination of dplyr::group_by() and dplyr::summarize() worked?\n Exercise: Since our data are coming from a world with perfect information. Can you think of a way to explore the existence differential treatment effect bias in our data?\nHint We know that the differential treatment effect bias is the difference in difference in the average of every subject's ITE grouped by their treatment status (or the difference between ATT and ATCs). Maybe you can go back an check how to get the average treatment effect on the treated and untreated.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"78779ae7cd8fe70a383794e67e25c038","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-2/pof/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-2/pof/","section":"materials","summary":"The POF in practice Let's revisit the example from our slides once again.\nSay we are interested in assessing the premise of Allport's hypothesis about interpersonal contact being conducive to reducing intergroup prejudice.","tags":null,"title":"The Potential Outcomes Framework","type":"book"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/2022-spring-stats2/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/2022-spring-stats2/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/2022-spring-stats2/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]