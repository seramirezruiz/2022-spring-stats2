[{"authors":null,"categories":null,"content":"Adri√°n Santonja is a Ph.D. student at the DIW Berlin Graduate Center since October 2020. He obtained both his M.Sc. and B.Sc degree in economics at the University of Mannheim. Adri√°n¬¥s research interests lie in the fields of climate and environmental economics, applied microeconometrics and policy evaluation. Before joining DIW, Adri√°n worked as a research assistant for the Chair of Quantitative Economics and the Chair of Econometrics at the University of Mannheim. In addition, he gained professional experience at the Directorate-General for Climate Action of the European Commission in Brussels and the economic consultancy Frontier Economics in Madrid.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b8cfebb0d61cc10c64f61b7b56223eb0","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/adrian-santonja-di-fonzo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/adrian-santonja-di-fonzo/","section":"authors","summary":"Adri√°n Santonja is a Ph.D. student at the DIW Berlin Graduate Center since October 2020. He obtained both his M.Sc. and B.Sc degree in economics at the University of Mannheim. Adri√°n¬¥s research interests lie in the fields of climate and environmental economics, applied microeconometrics and policy evaluation.","tags":null,"title":"Adri√°n Santonja di Fonzo","type":"authors"},{"authors":null,"categories":null,"content":"Kindye Adugna graduated from the Hertie School in July 2021. He also holds a BSc degree in Economics from Mekelle University. Before moving to Berlin, Kindye was an analyst at the Tony Blair Institute, advising central governments across Africa. Between September 2020 and December 2021, he worked as a research and teaching assistant to Dr. Bechar√° at the Hertie School Data Science Lab.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"45433e734a380c25d99bd7773b1ef854","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/kindye-adugna/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/kindye-adugna/","section":"authors","summary":"Kindye Adugna graduated from the Hertie School in July 2021. He also holds a BSc degree in Economics from Mekelle University. Before moving to Berlin, Kindye was an analyst at the Tony Blair Institute, advising central governments across Africa.","tags":null,"title":"Kindye Adugna","type":"authors"},{"authors":null,"categories":null,"content":"Lisa Oswald is pursuing a PhD in Governance at the Hertie School in Berlin. She graduated from the University of Oxford with a MSc degree in Social Data Science, and from the University of Kassel with a BSc and MSc degree in Psychology. She is interested in online communication and deliberation, the public perception of climate change, political opinion formation and the emergence of collective behaviour.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6efcaa8911b4c0459710d83df8d3e3e8","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/lisa-oswald/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/lisa-oswald/","section":"authors","summary":"Lisa Oswald is pursuing a PhD in Governance at the Hertie School in Berlin. She graduated from the University of Oxford with a MSc degree in Social Data Science, and from the University of Kassel with a BSc and MSc degree in Psychology.","tags":null,"title":"Lisa Oswald","type":"authors"},{"authors":null,"categories":null,"content":"Sebastian Ramirez Ruiz is a Phd Researcher and a Research Associate to Prof. Simon Munzert. He holds a Master\u0026rsquo;s of Public Policy from the Hertie School and B.A.s in Sociology and Political Science from Stony Brook University. He is particularly interested in causal inference, the use of evidence in decision-making, and most importantly, bicycles.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d68d186b5437c0054f698afb12d818b3","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/sebastian-ramirez-ruiz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/sebastian-ramirez-ruiz/","section":"authors","summary":"Sebastian Ramirez Ruiz is a Phd Researcher and a Research Associate to Prof. Simon Munzert. He holds a Master\u0026rsquo;s of Public Policy from the Hertie School and B.A.s in Sociology and Political Science from Stony Brook University.","tags":null,"title":"Sebastian Ramirez Ruiz","type":"authors"},{"authors":null,"categories":null,"content":"Simon Munzert is Assistant Professor of Data Science and Public Policy at the Hertie School and member of the Hertie School Data Science Lab. His research interests include attitude formation in the digital age, public opinion, and the use of online data in social research. He received his Doctoral Degree in Political Science from the University of Konstanz.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"39676266dc03f7a9ebc998657bf4cd2d","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/simon-munzert/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/simon-munzert/","section":"authors","summary":"Simon Munzert is Assistant Professor of Data Science and Public Policy at the Hertie School and member of the Hertie School Data Science Lab. His research interests include attitude formation in the digital age, public opinion, and the use of online data in social research.","tags":null,"title":"Simon Munzert","type":"authors"},{"authors":null,"categories":null,"content":"Till Koeveker is a PhD candidate at DIW Graduate Center and in the Climate Policy department of DIW. He holds a Master in Economics (Economic Theory and Econometrics) from Toulouse School of Economics and a Bachelor in Sociology, Politics \u0026amp; Economics from Zeppelin University in Friedrichshafen. Furthermore, he has experience from internships in different sectors (e.g. at KfW Development Bank, at the √ñko-Insitut ‚Äì Institute for Applied Ecology and at the economic consultancy Frontier Economics). In his current research, Till studies policy instruments for decarbonizing the industry sector, in particular carbon border adjustment mechanisms.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5292b55c488db0476cec94b4b16c4a92","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/author/till-koeveker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/author/till-koeveker/","section":"authors","summary":"Till Koeveker is a PhD candidate at DIW Graduate Center and in the Climate Policy department of DIW. He holds a Master in Economics (Economic Theory and Econometrics) from Toulouse School of Economics and a Bachelor in Sociology, Politics \u0026amp; Economics from Zeppelin University in Friedrichshafen.","tags":null,"title":"Till Koeveker","type":"authors"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references      Today\u0026rsquo;s session  Practice how to simulate data with R Learn how to iterate with for loops Write our own functions (my_power_function) Learn the basics of power analysis with R  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html Helpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \n Lisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about Moderation and Heterogeneous Effects in R\n  Power Analysis Introduction! Welcome to our eleventh tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. In this lab session we will develop our own simulation to understand the concept of power analysis a bit better.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1619395200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1619395200,"objectID":"4f726686b43d9e703fd0bf29286da434","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-11/","publishdate":"2021-04-26T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-11/","section":"materials","summary":"Welcome to our tutorial about Statistical Power and Power Analysis in R","tags":null,"title":"üìä 11 - Statistical Power and Power Analysis","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references      Today\u0026rsquo;s session  Learn how to perform interaction models with lm() Learn how to extract marginal/partial effects with margins::margins() and predictive margins with ggeffects::::ggeffect() Learn how to vectorize multiple ifelse() statements with dplyr::case_when()  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html Helpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \n Lisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about Moderation and Heterogeneous Effects in R\n  Moderation and Heterogeneous Effects Welcome Introduction! Welcome to our tenth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. During this week‚Äôs lecture you were introduced to Moderation and Heterogeneous Effects.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1618790400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1618790400,"objectID":"57394c3ff8258a011cbcc5229129e9ff","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-10/","publishdate":"2021-04-19T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-10/","section":"materials","summary":"Welcome to our tutorial about Moderation and Heterogeneous Effects in R","tags":null,"title":"üìä 10 - Moderation and Heterogeneous Effects","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references   Meet your instructors    Today\u0026rsquo;s session  Create three-way tables with janitor::tabyl() Visualize trends across time with ggplot2 Learn how to extract our pooled, unit-fixed, and unit- and time-fixed effects estimates with Least Squares Dummy Variables (LSDV) estimation (with lm() and the de-meaning approach with plm())  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html Helpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \n Meet your instructors Lisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about Panel Data and FE in R\n  Panel Data and Fixed Effects Welcome Introduction! Welcome to our ninth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. During this week‚Äôs lecture you were introduced to Panel Data and Fixed Effects.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1618272000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1618272000,"objectID":"94be13707062f226f54ed56c650fbfca","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-9/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-9/","section":"materials","summary":"Welcome to our tutorial about Panel Data and FE in R","tags":null,"title":"üìä 09 - Panel Data and Fixed Effects","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references      Today\u0026rsquo;s session  Learn how to transform our dataframes from wide to long format with tidyr:pivot_longer() Leverage visualizations with ggplot2 to explore changes between groups and across time Learn how to extract our DiD estimates through manual calculation, first differences, and the regression formulation of the DiD model  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html Helpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \n Lisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about DiD\n  Difference in Differences Welcome Introduction! Welcome to our eighth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. During this week‚Äôs lecture you were introduced to Difference in Differences (DiD).\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1617580800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1617580800,"objectID":"8842f07bc1e2b9b1aed03fb407444b0b","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-8/","publishdate":"2021-04-05T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-8/","section":"materials","summary":"Welcome to our tutorial about DiD in R","tags":null,"title":"üìä 08 - Difference in Differences","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references   Meet your instructors    Today\u0026rsquo;s session  Leverage visualizations with ggplot2 to explore our discontinuity setups Learn how to model our discontinuity setups under different functional forms with lm() Learn how to model our discontinuity setups under different functional forms with rdrobust::rdrobust()  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nTo explore more about rdrobust::rdrobust(): rdrobust: https://cran.r-project.org/web/packages/rdrobust/rdrobust.pdf\nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html Helpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \n Meet your instructors Lisa Oswald \u0026amp; Sebastian Ramirez Ruiz\n Slides Welcome to our tutorial about instrumental variables in R\n  Regression Discontinuity Designs Welcome Introduction! Welcome to our seventh tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. During this week‚Äôs lecture you were introduced to Regression Discontinuity Designs (RDDs).\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1617062400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1617062400,"objectID":"3de69c8f7a8d04108c8292a13581b448","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-7/","publishdate":"2021-03-30T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-7/","section":"materials","summary":"Welcome to our tutorial about RDD in R","tags":null,"title":"üìä 07 - Regression Discontinuity Designs","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references      Today\u0026rsquo;s session  Review how to manually extract the LATE through the wald estimator Learn how to perform Two-stage Least Squares regression (2SLS) with ivreg() from the AER package Illustrate the mechanics of Two-stage Least Squares regression (2SLS) with lm()  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nTo explore more about AER::ivreg(): Instrumental variables regression: https://rpubs.com/wsundstrom/t_ivreg\nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html Helpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \n Lisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about instrumental variables in R\n  Instrumental Variables Welcome Introduction! Welcome to our sixth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. During this week's lecture you reviewed what happens when experiments break due to non-compliance.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1615766400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1615766400,"objectID":"18de2d685036c6be4039c90c9fa4833c","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-6/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-6/","section":"materials","summary":"Welcome to our tutorial about instrumental variables in R","tags":null,"title":"üìä 06 - Instrumental Variables","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references      Today\u0026rsquo;s session  Take a step back to review how to compare the means of two groups in R Learn how to perform matching with the MatchIt package Illustrate the mechanics of propensity score matching with gml()  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nTo explore more about MatchIt: MatchIt: Getting Started: https://cran.r-project.org/web/packages/MatchIt/vignettes/MatchIt.html\nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html Helpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \nLisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about matching in R\n  Matching in R Welcome Introduction! Welcome to our fifth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. During this week‚Äôs lecture you reviewed randomization in experimental setups.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1615161600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1615161600,"objectID":"2211b25d6069d4c59ebe1dc415ed9cb6","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-5/","publishdate":"2021-03-08T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-5/","section":"materials","summary":"Welcome to our tutorial about matching in R","tags":null,"title":"üìä 05 - Matching","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references      Today\u0026rsquo;s session  use the ggdag and dagitty packages to assess your modeling strategy review how to run regression models using R illustrate omitted variable and collider bias  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html Helpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \n Lisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about performing analyses on DAGs in R and regression\n  The Backdoor Criterion and Basics of Regression in R Welcome Introduction! Welcome to our fourth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. During this week's lecture you reviewed bivariate and multiple linear regressions.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1614470400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1614470400,"objectID":"3ead50f0ec2b4ec49a2fc5cdbd1e4d9e","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-4/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-4/","section":"materials","summary":"Welcome to our tutorial about performing analyses on DAGs in R and regression","tags":null,"title":"üìä 04 - The Backdoor Criterion and Regression","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session  Further references      Today\u0026rsquo;s session  identify the rationale of the grammar of graphics in ggplot2 create visualizations with ggplot2 identify basic functionalities of ggdag for DAG creation in R  Download slides - PDF\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nFor learning more about DAGs and ggdag:  An Introduction to DAGs: https://ggdag.netlify.com/articles/intro-to-dags.html Introduction to ggdag: https://ggdag.netlify.com/articles/intro-to-ggdag.html Bias structures: https://ggdag.netlify.com/articles/bias-structures.html More info about ggplot2 The complete ggplot tutorial: http://r-statistics.co/Complete-Ggplot2-Tutorial-Part1-With-R-Code.html \nHelpful cheatsheets  Data visualization with ggplot: https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx \n Lisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about data visualization with ggplot2 and DAGs\n  Basics of Data Visualization and DAGs in R # Load packages. Install them first, in case you don\u0026#39;t have them yet. library(palmerpenguins) # To get our example\u0026#39;s dataset library(tidyverse) # To use dplyr functions and the pipe operator when needed library(ggplot2) # To visualize data (this package is also loaded by library(tidyverse)) library(ggdag) # To create our DAGs Welcome This week's tutorial will be divided in two broader camps.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1614038400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1614038400,"objectID":"402a6112202baa8a57ee5b66cff17ea6","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-3/","publishdate":"2021-02-23T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-3/","section":"materials","summary":"Welcome to our tutorial about data visualization with `ggplot2` and DAGs","tags":null,"title":"üìä 03 - Visualization and Directed Acyclic Graphs (DAGs)","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session Further references    Today\u0026rsquo;s session  identify the purpose of a set of dplyr verbs write statements in tidy syntax apply dplyr verbs to solve your data manipulation challenges  Download slides - PDF\n Further references For dplyr Data wrangling with dplyr: https://tinyurl.com/vyrv596 dplyr video tutorial: https://www.youtube.com/watch?v=jWjqLW-u3hc \nHelpful cheatsheets Data wrangling with dplyr and tidyr: https://tinyurl.com/s6zxfqh RMarkdown cheatsheet: https://tinyurl.com/uqoelrx\nLisa Oswald \u0026 Sebastian Ramirez Ruiz ## Courses in this program  Slides Welcome to our tutorial about data manipulation with dplyr and the Potential Outcomes Framework\n  Introduction to Data Manipulation 1. Introduction Welcome! Welcome to our second tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course. The labs are designed to reinforce the material covered during the lectures by introducing you to hands-on applications.\n  The Potential Outcomes Framework The POF in practice Let's revisit the example from our slides once again. Say we are interested in assessing the premise of Allport's hypothesis about interpersonal contact being conducive to reducing intergroup prejudice.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1612742400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1612742400,"objectID":"dbc7d94f5edd7bc1d5484cb1286fe5cd","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-2/","publishdate":"2021-02-08T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-2/","section":"materials","summary":"Welcome to our tutorial about data manipulation with `dplyr` and the Potential Outcomes Framework","tags":null,"title":"üìä 02 - Foundations","type":"book"},{"authors":null,"categories":null,"content":"Table of Contents  Today\u0026rsquo;s session Program overview Further references    Today\u0026rsquo;s session  Discuss the logistics of the drop-in tutorials Explain the assignment submission process Introduce you to RMarkdown  Download slides - PDF\n Program overview Welcome to our introductory week. During this session we will go over the basic set-up for this semester.\n Further references For R and RMarkdown Reminder of the basics: https://tinyurl.com/vkebh2f RMarkdown: The definitive guide https://tinyurl.com/y4tyfqmg RMarkdown cheatsheet: https://tinyurl.com/uqoelrx  Slides Welcome to our introduction to the tutorials\n  R and RStudio basics Welcome! The practical component of the Statistics II: Statistical Modeling and Causal Inference course relies largely in R programming. Today we will center on some of the necessary skills to perform the assignments for the course.\n     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1612310400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1612310400,"objectID":"c5c2b8f3885bac92985371787ae0c913","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-1/","publishdate":"2021-02-03T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-1/","section":"materials","summary":"Welcome to our introduction to the tutorials","tags":null,"title":"üìä 01 - Introduction","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1618790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618790400,"objectID":"e7b903a6c1a75acfd2ac7e4a019e688c","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-10/slides/","publishdate":"2021-04-19T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-10/slides/","section":"materials","summary":"Welcome to our tutorial about Moderation and Heterogeneous Effects in R","tags":null,"title":"10 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1618790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618790400,"objectID":"2fa5ae15c8c9a76656ded34bb242f19d","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-11/slides/","publishdate":"2021-04-19T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-11/slides/","section":"materials","summary":"Welcome to our tutorial about Moderation and Heterogeneous Effects in R","tags":null,"title":"10 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1618272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618272000,"objectID":"88c62440057b78091ca95017e8790a9d","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-9/slides/","publishdate":"2021-04-13T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-9/slides/","section":"materials","summary":"Welcome to our tutorial about Panel Data and FE in R","tags":null,"title":"08 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1617580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617580800,"objectID":"9a6e9e31261910d63f33be18a5d71e94","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-8/slides/","publishdate":"2021-04-05T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-8/slides/","section":"materials","summary":"Welcome to our tutorial about DiD","tags":null,"title":"08 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1617062400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617062400,"objectID":"d9f1892cf6dd22e2d1d571130a49b40e","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-7/slides/","publishdate":"2021-03-30T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-7/slides/","section":"materials","summary":"Welcome to our tutorial about instrumental variables in R","tags":null,"title":"07 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"9987d55e31a539209528662c3ccec303","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-6/slides/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-6/slides/","section":"materials","summary":"Welcome to our tutorial about instrumental variables in R","tags":null,"title":"05 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1615161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615161600,"objectID":"b82f18c3fac1daa6da6651055b484027","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-5/slides/","publishdate":"2021-03-08T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-5/slides/","section":"materials","summary":"Welcome to our tutorial about matching in R","tags":null,"title":"05 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1614470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614470400,"objectID":"c0ba5cb612cf233f7b2c4a49603ec587","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-4/slides/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-4/slides/","section":"materials","summary":"Welcome to our tutorial about performing analyses on DAGs in R and regression","tags":null,"title":"04 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1614038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614038400,"objectID":"ac69de0c16ff1ab65a31c6637942a5f7","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-3/slides/","publishdate":"2021-02-23T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-3/slides/","section":"materials","summary":"Welcome to our tutorial about data visualization with `ggplot2` and DAGs","tags":null,"title":"03 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1612742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612742400,"objectID":"9213c49561d9f1b07797ec92baaaa4a1","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-2/slides/","publishdate":"2021-02-08T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-2/slides/","section":"materials","summary":"Welcome to our tutorial about data manipulation with `dplyr` and the Potential Outcomes Framework","tags":null,"title":"02 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Slides     The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   -- ","date":1612310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612310400,"objectID":"54210879398f2cecb50f62ab00bbb544","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-1/slides/","publishdate":"2021-02-03T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-1/slides/","section":"materials","summary":"Welcome to our introduction to the tutorials","tags":null,"title":"01 - Slides","type":"book"},{"authors":null,"categories":null,"content":"Welcome! The practical component of the Statistics II: Statistical Modeling and Causal Inference course relies largely in R programming. Today we will center on some of the necessary skills to perform the assignments for the course.\nIn this tutorial, you will learn to:\n identify the some of the basic functionalities of R and RStudio import data sets into your R environment  R and RStudio: Basics The RStudio layout RStudio is an integrated development environment (IDE) for R. Think of RStudio as a front that allows us to interact, compile, and render R code in a more instinctive way. The following image shows what the standard RStudio interface looks like:\n    Console: The console provides a means to interact directly with R. You can type some code at the console and when you press ENTER, R will run that code. Depending on what you type, you may see some output in the console or if you make a mistake, you may get a warning or an error message.\n  Script editor: You will utilize the script editor to complete your assignments. The script editor will be the space where files will be displayed. For example, once you download and open the bi-weekly assignment .Rmd template, it will appear here. The editor is a where you should place code you care about, since the code from the console cannot be saved into a script.\n  Environment: This area holds the abstractions you have created with your code. If you run myresult \u0026lt;- 5+3+2, the myresult object will appear there.\n  Plots and files: This area will be where graphic output will be generated. Additionally, if you write a question mark before any function, (i.e. ?mean) the online documentation will be displayed here.\n   R Packages For the most part, R Packages are collections of code and functions that leverage R programming to expand on the basic functionalities. Last week we met dplyr that aids R programmers in the process of data cleaning and manipulation. There are a plethora of packages in R designed to facilite the completion of tasks. In fact, this website is built with the blogdown package that lets you create websites using RMarkdown and Hugo\nUnlike other programming languages, in R you only need to install a package once. The following times you will only need to \u0026ldquo;require\u0026rdquo; the package. As a good practice I recommend running the code to install packages only in your R console, not in the code editor. You can install a package with the following syntax\ninstall.packages(\u0026quot;name_of_your_package\u0026quot;) #note that the quotation marks are mandatory at this stage  Once the package has been installed, you just need to \u0026ldquo;call it\u0026rdquo; every time you want to use it in a file by running:\nlibrary(\u0026quot;name_of_your_package\u0026quot;) #either of this lines will require the package library(name_of_your_package) #library understands the code with, or without, quotation marks   It is extremely important that you do not have any lines installing packages for your assignments because the file will fail to knit    Working directory The working directory is just a file path on your computer that sets the default location of any files you read into R, or save out of R. Normally, when you open RStudio it will have a default directory (a folder in your computer). You can check you directory by running getwd() in your console:\n#this is the default in my case getwd() #[1] \u0026quot;/Users/sebastianramirezruiz\u0026quot;  When your RStudio is closed and you open a file from your finder in MacOS or file explorer in Windows, the default working directory will be the folder where the file is hosted\n Setting your working directory You can set you directory manually from RStudio: use the menu to change your working directory under Session \u0026gt; Set Working Directory \u0026gt; Choose Directory.\n  You can also use the setwd() function:\nsetwd(\u0026quot;/path/to/your/directory\u0026quot;) #in macOS setwd(\u0026quot;c:/path/to/your/directory\u0026quot;) #in windows   Recommended folder structure for the class   We recommend you pay close attention to your folder structure. You will receive a new folder for each assignment. Make the folder your working directory when working on the assignment. This folder will be populated with the template .Rmd and the data for the week. When you knit the file, the .html will be created in this folder.\n We will learn more about the assignment submission workflow next week. Still, avoid changing the name of the files you receive in Github since it will create issues.    Dealing with errors in R Errors in R occur when code is used in a way that it is not intended. For example when you try to add two character strings, you will get the following error:\n\u0026quot;hello\u0026quot; + \u0026quot;world\u0026quot; Error in \u0026quot;hello\u0026quot; + \u0026quot;world\u0026quot;: non-numeric argument to binary operator  Normally, when something has gone wrong with your program, R will notify you of an error in the console. There are errors that will prevent the code from running, while others will only produce warning messages. In the following case, the code will run, but you will notice that the string \u0026ldquo;three\u0026rdquo; is turned into a NA.\nas.numeric(c(\u0026quot;1\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;three\u0026quot;)) Warning: NAs introduced by coercion [1] 1 2 NA  Since we will be utilizing widely used packages and functions in the course of the semester, the errors that you may come across in the process of completing your assignments will be common for other R users. Most errors occur because of typos. A Google search of the error message can take you a long way as well. Most of the times the first entry on stackoverflow.com will solve the problem.\n Importing data Next we will work with data provided by the palmerpenguins package; however, most practical applications will require you to work with your own data. In fact, for most assignments, you will be given a data set to work with. As we will see in the coming weeks, data are messy. You know what else is messy? Data formats. You may be acquainted with a couple of them (.csv, .tsv, .xlsx).\nFortunately for us, the tidyverse has two packages that make the process of loading data sets from different formats very easy.\n readr: The goal of readr is to provide a fast and friendly way to read rectangular data (like csv, tsv, rds, and fwf) haven: The goal of haven is to enable R to read and write various data formats used by other statistical packages (like dta, sas, and sav)  You can read more about how to load different types of data in the respective documentations of the packages ‚Äî readr and haven\n For the assignments you will be required to load datasets. You can do that by installing the readr package and utilizing:\n#with readr your_data_frame \u0026lt;- readr::read_rds(\u0026quot;path_for_the_file\u0026quot;) #you can alternatively use a base R option your_data_frame \u0026lt;- base::readRDS(\u0026quot;path_for_the_file\u0026quot;)  This week\u0026rsquo;s mock assignment will feature a .tsv file. When in doubt just Google \u0026ldquo;How to load x_format data in R?\u0026quot; That will do the trick!\n The double colon operator :: You may have noted in the previous section that the functions were preceded by their package name and two colons, for example: readr::read_rds(). The double colon operator :: helps us ensure that we select functions from a particular package. We utilize the operator to explicitly state where the function is coming. This may become even more important when you are doing data analysis as part of a team further in your careers. Though it is likely that this will not be a problem during the course, we can try to employ the following convention package_name::function() to ensure that we will not encounter errors in our knitting process:\ndplyr::select()   Let\u0026rsquo;s look at what happens when we load tidyverse.\nlibrary(tidyverse) #‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.0 #‚îÄ‚îÄ #‚úì ggplot2 3.3.2 ‚úì purrr 0.3.4 #‚úì tibble 3.0.3 ‚úì dplyr 1.0.2 #‚úì tidyr 1.1.2 ‚úì stringr 1.4.0 #‚úì readr 1.3.1 ‚úì forcats 0.5.0 #‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() #‚îÄ‚îÄ #x dplyr::filter() masks stats::filter() #x dplyr::lag() masks stats::lag()  You may notice that R points out some conflicts where some functions are being masked. The default in this machine will become the filter() from the dplyr package during this session. If you were to run some code that is based on the filter() from the stats package, your code will probably result in errors.\n ","date":1612310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612310400,"objectID":"027efd9630c3bd9e00a3f8413c01c502","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-1/foundations/","publishdate":"2021-02-03T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-1/foundations/","section":"materials","summary":"Welcome! The practical component of the Statistics II: Statistical Modeling and Causal Inference course relies largely in R programming. Today we will center on some of the necessary skills to perform the assignments for the course.","tags":null,"title":"R and RStudio basics","type":"book"},{"authors":null,"categories":null,"content":"    1. Introduction Welcome! Welcome to our second tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nThe labs are designed to reinforce the material covered during the lectures by introducing you to hands-on applications.\nThe practical nature of our class means that our labs will be data-centered. Throughout our class, we will get acquinted with multiple packages of the tidyverse.\nThough we expect that some of you may already know them, the tidyverse is a collection of R packages that share an underlying design, syntax, and structure. They will definitely make your life easier!!\nToday, we will start with a brief introduction to data manipulation through the dplyr package.\nIn this tutorial, you will learn to:\n identify the purpose of a set of dplyr verbs write statements in tidy syntax apply dplyr verbs to solve your data manipulation challenges  This tutorial is partly based on R for Data Science, section 5.2, and Quantitative Politics with R, chapter 3.\n What we will need today We‚Äôll practice some wrangling in dplyr using data for penguin sizes recorded by Dr.¬†Kristen Gorman and others at several islands in the Palmer Archipelago, Antarctica. Data are originally published in: Gorman KB, Williams TD, Fraser WR (2014) PLoS ONE 9(3): e90081. doi:10.1371/journal.pone.0090081\nYou do not need to import the data to work through this tutorial - the data are already here waiting behind the scenes.\nBut if you do ever want to use the penguins data outside of this tutorial, they now exist in the palmerpenguins package in R.\nLet‚Äôs begin!\n 2. Data Structure Tidy data Generally, we will encounter data in a tidy format. Tidy data refers to a way of mapping the structure of a data set. In a tidy data set:\n Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table  The penguins data set The 3 species of penguins in this data set are Adelie, Chinstrap and Gentoo. The data set contains 8 variables:\n species: a factor denoting the penguin species (Adelie, Chinstrap, or Gentoo) island: a factor denoting the island (in Palmer Archipelago, Antarctica) where observed culmen_length_mm: a number denoting length of the dorsal ridge of penguin bill (millimeters) culmen_depth_mm: a number denoting the depth of the penguin bill (millimeters) flipper_length_mm: an integer denoting penguin flipper length (millimeters) body_mass_g: an integer denoting penguin body mass (grams) sex: a factor denoting penguin sex (MALE, FEMALE) year an integer denoting the year of the record  *Illustration by @allisonhorst* Let‚Äôs explore the data set. head() is a function that returns the first couple rows from a data frame. Write the R code required to explore the first observations of the penguins data set:\nNotice that when you press ‚ÄòRun,‚Äô the output of the code is returned below it! So by pressing ‚ÄòRun,‚Äô you‚Äôve run your first R code of the class!\nhead(penguins)    species  island  bill\\_length\\_mm  bill\\_depth\\_mm  flipper\\_length\\_mm  body\\_mass\\_g  sex  year      Adelie  Torgersen  39.1  18.7  181  3750  male  2007    Adelie  Torgersen  39.5  17.4  186  3800  female  2007    Adelie  Torgersen  40.3  18.0  195  3250  female  2007    Adelie  Torgersen  NA  NA  NA  NA  NA  2007    Adelie  Torgersen  36.7  19.3  193  3450  female  2007    Adelie  Torgersen  39.3  20.6  190  3650  male  2007      3. Manipulating data with dplyr What we will learn today In this tutorial, you‚Äôll learn and practice examples using some functions in dplyr to work with data. Those are:\n select(): keep or exclude some columns filter(): keep rows that satisfy your conditions mutate(): add columns from existing data or edit existing columns group_by(): lets you define groups within your data set summarize(): get summary statistics arrange(): reorders the rows according to single or multiple variables  Let‚Äôs get to work.\n 3.1. select() The first verb (function) we will utilize is select(). We can employ it to manipulate our data based on columns. If you recall from our initial exploration of the data set there were eight variables attached to every observation. Do you recall them? If you do not, there is no problem. You can utilize names() to retrieve the names of the variables in a data frame.\nnames(penguins)  ## [1] \u0026quot;species\u0026quot; \u0026quot;island\u0026quot; \u0026quot;bill_length_mm\u0026quot; ## [4] \u0026quot;bill_depth_mm\u0026quot; \u0026quot;flipper_length_mm\u0026quot; \u0026quot;body_mass_g\u0026quot; ## [7] \u0026quot;sex\u0026quot; \u0026quot;year\u0026quot;  Say we are only interested in the species, island, and year variables of these data, we can utilize the following syntax:\n select(data, columns)   Activity The following code chunk would select the species, island, and year variables. What should we do to keep the body_mass_g and sex variables as well?\ndplyr::select(penguins, species, island, year)   Answer # you just need to type the names of the columns dplyr::select(penguins, species, island, year, body_mass_g, sex)     To drop variables, use - before the variable name.\nFor example, select(penguins, -year) will drop the year column.\n   3.2. filter() The second verb (function) we will employ is filter(). filter() lets you use a logical test to extract specific rows from a data frame. To use filter(), pass it the data frame followed by one or more logical tests. filter() will return every row that passes each logical test.\nThe more commonly used logical operators are:\n ==: Equal to !=: Not equal to \u0026gt;, \u0026gt;=: Greater than, greater than or equal to \u0026lt;, \u0026lt;=: Less than, less than or equal to \u0026amp;, |: And, or  Say we are interested in retrieving the observations from the year 2007. We would do:\ndplyr::filter(penguins, year == 2007)   Activity Can you adapt the code to retrieve all the observations of Chinstrap penguins from 2007 (remember that species contains character units)\nAnswer # you just need to utilize \u0026amp; and type the logical operator for the species dplyr::filter(penguins, year == 2007 \u0026amp; species == \u0026quot;Chinstrap\u0026quot;)     3.3. The Pipe Operator: %\u0026gt;% The pipe, %\u0026gt;%, comes from the magrittr package by Stefan Milton Bache. Packages in the tidyverse load %\u0026gt;% for you automatically, so you don‚Äôt usually load magrittr explicitly. This will be one of your best friends in R. \u0026gt;Pipes are a powerful tool for clearly expressing a sequence of multiple operations. Let‚Äôs think about baking for a second.\n Activity We can leverage the pipe operator to sequence our code in a logical manner. Can you adapt the following code chunk with the pipe and conditional logical operators we discussed?\nonly_2009 \u0026lt;- dplyr::filter(penguins, year == 2009) only_2009_chinstraps \u0026lt;- dplyr::filter(only_2009, species == \u0026quot;Chinstrap\u0026quot;) only_2009_chinstraps_species_sex_year \u0026lt;- dplyr::select(only_2009_chinstraps, species, sex, year) final_df \u0026lt;- only_2009_chinstraps_species_sex_year final_df #to print it in our console  Answer penguins %\u0026gt;% #we start off with out df dplyr::filter(year == 2009 \u0026amp; species == \u0026quot;Chinstrap\u0026quot;) %\u0026gt;% #filter dplyr::select(species, sex, year) #select     3.4. mutate() mutate() lets us create, modify, and delete columns. The most common use for now will be to create new variables based on existing ones. Say we are working with a U.S. American client and they feel more confortable with assessing the weight of the penguins in pounds. We would utilize mutate() as such:\n  mutate(new\\_var\\_name = conditions)  Activity Can you edit the following code chunk to render a new variable body_mass_kg?\npenguins %\u0026gt;% dplyr::mutate(body_mass_lbs = body_mass_g/453.6)   Answer penguins %\u0026gt;% dplyr::mutate(body_mass_kg = body_mass_g/1000) #grams divided by 1000     3.5. group_by() and summarize() These two verbs group_by() and summarize() tend to go together. When combined , ‚Äôsummarize()` will create a new data frame. It will have one (or more) rows for each combination of grouping variables; if there are no grouping variables, the output will have a single row summarising all observations in the input. For example:\n summarize():  penguins %\u0026gt;% dplyr::summarize(heaviest_penguin = max(body_mass_g, na.rm = T)) #max() does not know how to deal with NAs very well    heaviest\\_penguin      6300      group_by() + summarize():  penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarize(heaviest_penguin = max(body_mass_g, na.rm = T))    species  heaviest\\_penguin      Adelie  4775    Chinstrap  4800    Gentoo  6300     Activity Can you get the weight of the lightest penguin of each species? You can use min(). What happens when in addition to species you also group by year group_by(species, year)?\nAnswers penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarize(lightest_penguin = min(body_mass_g, na.rm = T))    species  lightest\\_penguin      Adelie  2850    Chinstrap  2700    Gentoo  3950     penguins %\u0026gt;% dplyr::group_by(species, year) %\u0026gt;% dplyr::summarize(lightest_penguin = max(body_mass_g, na.rm = T))  ## `summarise()` has grouped output by 'species'. You can override using the `.groups` argument.    3.6. arrange() The arrange() verb is pretty self-explanatory. arrange() orders the rows of a data frame by the values of selected columns in ascending order. You can use the desc() argument inside to arrange in descending order. The following chunk arranges the data frame based on the length of the penguins‚Äô bill. You hint tab contains the code for the descending order alternative.\n arrange(variable\\_of\\_interest)  penguins %\u0026gt;% dplyr::arrange(bill_length_mm)   penguins %\u0026gt;% dplyr::arrange(desc(bill_length_mm))   Activity Can you create a data frame arranged by body_mass_g of the penguins observed in the ‚ÄúDream‚Äù island?\nAnswer penguins %\u0026gt;% dplyr::filter(island == \u0026quot;Dream\u0026quot;) %\u0026gt;% dplyr::arrange(desc(body_mass_g))    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0e5dbbf98949e49081cad709093af1a5","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-2/data-manipulation/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-2/data-manipulation/","section":"materials","summary":"1. Introduction Welcome! Welcome to our second tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nThe labs are designed to reinforce the material covered during the lectures by introducing you to hands-on applications.","tags":null,"title":"Introduction to Data Manipulation","type":"book"},{"authors":null,"categories":null,"content":" # Load packages. Install them first, in case you don\u0026#39;t have them yet. library(palmerpenguins) # To get our example\u0026#39;s dataset library(tidyverse) # To use dplyr functions and the pipe operator when needed library(ggplot2) # To visualize data (this package is also loaded by library(tidyverse)) library(ggdag) # To create our DAGs Welcome This week's tutorial will be divided in two broader camps.\n First, we will learn some basics of data visualization with ggplot. Second, we will start our exploration of directed acyclic graphs (DAGs) for causal inference.   Introduction to ggplot2 ggplot2 is by far the most popular visualization package in R. ggplot2 implements the grammar of graphics to render a versatile syntax of creating visuals. The underlying logic of the package relies on deconstructing the structure of graphs (if you are interested in this you can read this article).\nFor the purposes of this introduction to visualization with ggplot, we care about the layered nature of visualizing with ggplot2.\n*This tutorial is based largely on chapters 7 to 10 from the QPOLR book\nOur building blocks During this week, we will learn about the following building blocks:\n Data: the data frame, or data frames, we will use to plot Aesthetics: the variables we will be working with Geometric objects: the type of visualization Theme adjustments: size, text, colors etc  Data The first building block for our plots are the data we intend to map. In ggplot2, we always have to specify the object where our data lives. In other words, you will always have to specify a data frame, as such:\nggplot(name_of_your_df) In the future, we will see how to combine multiple data sources to build a single plot. For now, we will work under the assumption that all your data live in the same object.\n Aesthetics The second building block for our plots are the aesthetics. We need to specify the variables in the data frame we will be using and what role they play.\nTo do this we will use the function aes() within the ggplot() function after the data frame (remember to add a comma after the data frame).\nggplot(name_of_your_df, aes(x = your_x_axis_variable, y = your_y_axis_variable)) Beyond your axis, you can add more aesthetics representing further dimensions of the data in the two dimensional graphic plane, such as: size, color, fill, to name but a few.\n Geometric objects The third layer to render our graph is a geomethic object. To add one, we need to add a plus (+) at the end of the initial line and state the type of geometric object we want to add, for example, geom_point() for a scatter plot, or geom_bar() for barplots.\nggplot(name_of_your_df, aes(x = your_x_axis_variable, y = your_y_axis_variable)) + geom_point()  Theme At this point our plot may just need some final thouches. We may want to fix the axes names or get rid of the default gray background. To do so, we need to add an additional layer preceded by a plus sign (+).\nIf we want to change the names in our axes, we can utilize the labs() function.\nWe can also employ some of the pre-loaded themes, for example, theme_minimal().\nggplot(name_of_your_df, aes(x = your_x_axis_variable, y = your_y_axis_variable)) + geom_point() + theme_minimal() + labs(x = \u0026quot;Name you want displayed\u0026quot;, y = \u0026quot;Name you want displayed\u0026quot;)  Our first plot For our very first plot using ggplot2, we will use the penguins data from last week.\nWe would like to create a scatterplot that illustrates the relationship between the length of a penguin's flipper and their weight.\nTo do so, we need three of our building blocks: a) data, b) aesthetics, and c) a geometric object (geom_point()).\nggplot(penguins, aes(x = flipper_length_mm, y=body_mass_g)) + geom_point()  EXERCISE:   Once we have our scatterplot. Can you think of a way to adapt the code to:\n convey another dimension through color, the species of penguin  change the axes names  render the graph with theme_minimal().   Answer ggplot(penguins, aes(x = flipper_length_mm, y=body_mass_g, color=species)) + geom_point() + theme_minimal() + labs(x = \u0026quot;Flipper Length (mm)\u0026quot;, y = \u0026quot;Body mass (g)\u0026quot;, color = \u0026quot;Species\u0026quot;)     Visualizing effectively Plotting distributions If we are interested in plotting distributions of our data, we can leverage geometric objects, such as:\n geom_histogram(): visualizes the distribution of a single continuous variable by dividing the x axis into bins and counting the number of observations in each bin (the default is 30 bins). geom_density(): computes and draws kernel density estimate, which is a smoothed version of the histogram. geom_bar(): renders barplots and in plotting distributions behaves in a very similar way from geom_histogram() (can also be used with two dimensions)  This is a histogram presenting the weight distribution of penguins in our sample. .\nggplot(penguins, aes(x = body_mass_g)) + geom_histogram()  EXERCISE:   Let's adapt the code of our histogram:\n add bins = 15 argument (type different numbers)  add fill = \u0026quot;#FF6666\u0026quot; (type \u0026quot;red\u0026quot;, \u0026quot;blue\u0026quot;, instead of #FF6666)  change the geom to _density and _bar   Answer  Histogram with bins argument   ggplot(penguins, aes(x = body_mass_g)) + geom_histogram(bins = 15)  Histogram with bins and fill arguments   ggplot(penguins, aes(x = body_mass_g)) + geom_histogram(bins = 25, fill = \u0026quot;#FF6666\u0026quot;)  geom_density() and geom_bar()   ggplot(penguins, aes(x = body_mass_g)) + geom_density(alpha = 0.5, fill = \u0026quot;#FF6666\u0026quot;) ggplot(penguins, aes(x = body_mass_g)) + geom_bar(fill = \u0026quot;#FF6666\u0026quot;)  Plotting relationships We can utilize graphs to explore how different variables are related. In fact, we did so before in our scatterplot. We can also use box plots and lines to show some of these relationships.\nFor example, this boxplot showcasing the distribution of weight by species:\nggplot(penguins, aes(x = species, y = body_mass_g)) + geom_boxplot() + theme_minimal() + labs(x = \u0026quot;Species\u0026quot;, y = \u0026quot;Body mass (g)\u0026quot;) Or this adaptation of our initial plot with a line of best fit for the observed data by each species:\nggplot(penguins, aes(x= flipper_length_mm, y = body_mass_g, color = species)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + labs(x = \u0026quot;Length of the flipper\u0026quot;, y = \u0026quot;Body mass (g)\u0026quot;, color = \u0026quot;Species\u0026quot;)  Next steps Now that you have been introduced to some of the basics of ggplot2, the best way to move forward is to experiment. As we have discussed before, the R community is very open. Perhaps, you can gather some inspiration from the Tidy Tuesday social data project in R where users explore a new dataset each week and share their visualizations and code on Twitter under #TidyTuesday. You can explore some of the previous visualizations here and try to replicate their code.\nHere is a curated list of awesome ggplot2 resources.\n  Directed Acyclic Graphs (DAGs) This week we learned that directed acyclic graphs (DAGs) are very useful to express our beliefs about relationships among variables.\nDAGs are compatible with the potential outcomes framework. They give us a more convinient and intuitive way of laying out causal models. Next week we will learn how they can help us develop a modeling strategy.\nToday, we will focus on their structure and some DAG basics with the ggdag package.\nCreating DAGs in R To create our DAGs in R we will use the ggdag packages.\nThe first thing we will need to do is to create a dagified object. That is an object where we state our variables and the relationships they have to each other. Once we have our dag object we just need to plot with the ggdag() function.\nLet's say we want to re-create this DAG:\nWe would like to express the following links:\n P -\u0026gt; D D -\u0026gt; M D -\u0026gt; Y M -\u0026gt; Y  To do so in R with ggdag, we would use the following syntax:\ndag_object \u0026lt;- ggdag::dagify(variable_being_pointed_at ~ variable_pointing, variable_being_pointed_at ~ variable_pointing, variable_being_pointed_at ~ variable_pointing) After this we would just:\nggdag::ggdag(dag_object) Let's plot this DAG\nour_dag \u0026lt;- ggdag::dagify(d ~ p, m ~ d, y ~ d, y ~ m) ggdag::ggdag(our_dag)  EXERCISE:   See what happens when you add + theme_minimal(), + theme_void(), or + theme_dag() to the DAG. What package do you think is laying behind the mappings ofggdag`?\nAnswer our_dag \u0026lt;- ggdag::dagify(d ~ p, m ~ d, y ~ d, y ~ m) ggdag::ggdag(our_dag) + theme_minimal() ggdag::ggdag(our_dag) + theme_void()   Polishing our DAGs in R As you may have seen, the DAG is not rendered with the nodes in the positions we want.\nIf you ever want to explicitly tell ggdag where to position each node, you can tell it in a Cartesian coordinate plane.\nLet's take P as the point (0,0):\ncoord_dag \u0026lt;- list( x = c(p = 0, d = 1, m = 2, y = 3), y = c(p = 0, d = 0, m = 1, y = 0) ) our_dag \u0026lt;- ggdag::dagify(d ~ p, m ~ d, y ~ d, y ~ m, coords = coord_dag) ggdag::ggdag(our_dag) + theme_void()  More complex example: Let's say we're looking at the relationship between smoking and cardiac arrest. We might assume that smoking causes changes in cholesterol, which causes cardiac arrest:\nsmoking_ca_dag \u0026lt;- ggdag::dagify(cardiacarrest ~ cholesterol, cholesterol ~ smoking + weight, smoking ~ unhealthy, weight ~ unhealthy, labels = c(\u0026quot;cardiacarrest\u0026quot; = \u0026quot;Cardiac\\n Arrest\u0026quot;, \u0026quot;smoking\u0026quot; = \u0026quot;Smoking\u0026quot;, \u0026quot;cholesterol\u0026quot; = \u0026quot;Cholesterol\u0026quot;, \u0026quot;unhealthy\u0026quot; = \u0026quot;Unhealthy\\n Lifestyle\u0026quot;, \u0026quot;weight\u0026quot; = \u0026quot;Weight\u0026quot;) ) ggdag::ggdag(smoking_ca_dag, # the dag object we created text = FALSE, # this means the original names won\u0026#39;t be shown use_labels = \u0026quot;label\u0026quot;) + # instead use the new names theme_void() In this example, we:\n Used more meaningful variable names  Created a variable that was the result of two variables vs. just one (cholesterol)  Used the \u0026quot;labels\u0026quot; argument to rename our variables (this is useful if your desired final variable name is more than one word)     Common DAG path structures coord_dag \u0026lt;- list( x = c(d = 0, x = 1, y = 2), y = c(d = 0, x = 1, y = 0) ) our_dag \u0026lt;- ggdag::dagify(x ~ d, y ~ d, y ~ x, coords = coord_dag) ggdag::ggdag(our_dag) + theme_void()    EXERCISE:   Let's adapt the code to make X a confounder and a collider.\nAnswer  X as a confounder   coord_dag \u0026lt;- list( x = c(d = 0, x = 1, y = 2), y = c(d = 0, x = 1, y = 0) ) our_dag \u0026lt;- ggdag::dagify(d ~ x, #line from x to d y ~ d, #line from d to y y ~ x, #line from x to y coords = coord_dag)\nggdag::ggdag(our_dag) + theme_void()\n X as a collider   coord_dag \u0026lt;- list( x = c(d = 0, x = 1, y = 2), y = c(d = 0, x = 1, y = 0) ) our_dag \u0026lt;- ggdag::dagify(x ~ d, #line from d to x y ~ d, #line from d to y x ~ y, #line from y to x coords = coord_dag)\nggdag::ggdag(our_dag) + theme_void()\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9efba60adab01b945cf07df2871d117f","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-3/03-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-3/03-online-tutorial/","section":"materials","summary":"# Load packages. Install them first, in case you don\u0026#39;t have them yet. library(palmerpenguins) # To get our example\u0026#39;s dataset library(tidyverse) # To use dplyr functions and the pipe operator when needed library(ggplot2) # To visualize data (this package is also loaded by library(tidyverse)) library(ggdag) # To create our DAGs Welcome This week's tutorial will be divided in two broader camps.","tags":null,"title":"Basics of Data Visualization and DAGs in R","type":"book"},{"authors":null,"categories":null,"content":"   Welcome Introduction! Welcome to our eighth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you were introduced to Difference in Differences (DiD).\nIn this lab session we will:\n Learn how to transform our dataframes from wide to long format with tidyr:pivot_longer() Leverage visualizations with ggplot2 to explore changes between groups and across time Learn how to extract our DiD estimates through manual calculation, first differences, and the regression formulation of the DiD model    1. Wide and long data formats As we have seen throughout the semester, there are multiple ways to store our data. This week, we will look at the difference between wide and long format data.\nWe will illustrate this with a brief example. The two datasets we will load ‚Äîcity_wide_df and city_long_df‚Äî contain the same information.\nWide # wide data frame city_wide_df %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling()   city  pop_2000  pop_2010  pop_2020      Berlin  3.38  3.45  3.56    Rome  3.70  3.96  4.26    Paris  9.74  10.46  11.01    London  7.27  8.04  9.30      Long # long data frame city_long_df %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling()   city  year  pop      Berlin  2000  3.38    Berlin  2010  3.45    Berlin  2020  3.56    Rome  2000  3.70    Rome  2010  3.96    Rome  2020  4.26    Paris  2000  9.74    Paris  2010  10.46    Paris  2020  11.01    London  2000  7.27    London  2010  8.04    London  2020  9.30     As we can see, the long dataset separates the unit of analysis (city-year) into two separate columns. On the other hand, the wide dataset combines one of the keys (year) with the value variable (population).\nWhy do we care about the data format  In some instances, long format datasets are required for advanced statistical analysis and graphing. For example, if we wanted to run the regression formulation of the difference in differences model, we would need to input our data in long format. Furthermore, having our data in long format is very useful when plotting. Packages such as ggplot2, expect that your data will be in long form for the most part.\n Converting from wide to long format We will learn how to pivot our wide format data to long format with the tidyr package.\nWe will use the tidyr::pivot_longer() function, which ‚Äúlengthens‚Äù data, increasing the number of rows and decreasing the number of columns. The inverse transformation is tidyr::pivot_wider()\nYou can read the vignette here.\nHow to use tidyr::pivot_longer()  your_new_long_df \u0026lt;- tidyr::pivot_longer( your_wide_df, cols, names_to = \u0026quot;name\u0026quot;, values_to = \u0026quot;value\u0026quot; ... ) Let‚Äôs turn the city_wide_df into a long format dataset:\ncity_wide_df %\u0026gt;% tidyr::pivot_longer( cols = c(pop_2000, pop_2010, pop_2020), # -city, !city, dplyr::starts_with(\u0026quot;pop_\u0026quot;), etc... would also work names_to = \u0026quot;year\u0026quot;, # where do we want the names of the columns to go? (year) names_prefix = \u0026quot;pop_\u0026quot;, # names_prefix removes matching text from the start of each variable name (not always necessary) values_to = \u0026quot;pop\u0026quot; # where do we want the values in the columns to go? (pop) ) ## # A tibble: 12 √ó 3 ## city year pop ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Berlin 2000 3.38 ## 2 Berlin 2010 3.45 ## 3 Berlin 2020 3.56 ## 4 Rome 2000 3.7 ## 5 Rome 2010 3.96 ## 6 Rome 2020 4.26 ## 7 Paris 2000 9.74 ## 8 Paris 2010 10.5 ## 9 Paris 2020 11.0 ## 10 London 2000 7.27 ## 11 London 2010 8.04 ## 12 London 2020 9.3 Try to delete the names_prefix = \"pop_\" argument to see what happens.\nLet‚Äôs move to our practical example to see how we can use R for DiD estimation.\n  2. Measuring the effect of a soda tax on sugar-added drink consumption After the very successful impact evaluations you have performed in the past weeks, you are contacted by the local government of Pawnee, Indiana. The city is interested in your advice to assess a policy intervention passed with the support of councilwoman Leslie Knope.\nThe city of Pawnee has been at the spotlight recently, as it has come to be known as the child obesity and diabetes capital of the state of Indiana. Some of the constituents of the city point at the fast food culture and soda sizes across the restaurants in town as a source of the problem. The largest food chain in Pawnee, Paunch Burger, offers its smallest soda size at a whopping 64oz (about 1.9 liters).\nThe ‚Äúsoda tax‚Äù, as it came to be known, came to effect initially at a couple of districts. Fortunately for you, based on an archaic law, residents of Indiana have to demonstrate their residence in the district they intend to dine before being served at any of the restaurants. The latter fact means that Pawnee inhabitants can only buy sugar-added drinks in their respective home district.\nPackages # These are the libraries we will use today. Make sure to install them in your console in case you have not done so previously. set.seed(42) #for consistent results library(dplyr) # to wrangle our data library(tidyr) # to wrangle our data - pivot_longer() library(ggplot2) # to render our graphs library(readr) # for loading the .csv data library(kableExtra) # to render better formatted tables library(stargazer) # for formatting your model output library(estimatr) # to employ lm_robust() soda_tax_df \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/seramirezruiz/hertiestats2/master/data/soda_tax_df.csv\u0026quot;) # simulated data Our dataset soda_tax_df, contains the following information:\n √¨d: A unique number identifier for each of the 7,500 inhabitants of Pawnee district: The name of the district in which the corresponding unit lives treatment: A binary variable that signals whether the subject lived in a district where the tax was implemented pre_tax: The weekly sugar-added drink consumption in ounces before the tax was imposed post_tax: The weekly sugar-added drink consumption in ounces after the tax was imposed  Are these wide or long format data?  soda_tax_df %\u0026gt;% head(10) ## # A tibble: 10 √ó 5 ## id district treatment pre_tax post_tax ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 Snake Lounge 0 1688. 1706. ## 2 2 Snake Lounge 0 427. 438. ## 3 3 Snake Lounge 0 566. 560. ## 4 4 Snake Lounge 0 607. 624. ## 5 5 Snake Lounge 0 573. 607. ## 6 6 Snake Lounge 0 496. 502. ## 7 7 Snake Lounge 0 659. 670. ## 8 8 Snake Lounge 0 498. 522. ## 9 9 Snake Lounge 0 815. 846. ## 10 10 Snake Lounge 0 503. 510. Our soda_tax_df is in wide format. We can convert our data to a long format to render the time and treatment dummy variables and save is to the soda_tax_df_long.\nWe will utilize the pivot_longer() function from tidyr to format our data frame.\nsoda_tax_df_long \u0026lt;- soda_tax_df %\u0026gt;% # the wide format df tidyr::pivot_longer(cols = c(pre_tax, post_tax), # both contain information about soda drank at two points in time names_to = \u0026quot;period\u0026quot;, # grab the names of pre and post and save them to period values_to = \u0026quot;soda_drank\u0026quot;) %\u0026gt;% # grab values from pre and post and put them in soda_drank dplyr::mutate(after_tax = ifelse(period == \u0026quot;post_tax\u0026quot;, 1, 0)) # create dummy for period head(soda_tax_df_long, 10) ## # A tibble: 10 √ó 6 ## id district treatment period soda_drank after_tax ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 Snake Lounge 0 pre_tax 1688. 0 ## 2 1 Snake Lounge 0 post_tax 1706. 1 ## 3 2 Snake Lounge 0 pre_tax 427. 0 ## 4 2 Snake Lounge 0 post_tax 438. 1 ## 5 3 Snake Lounge 0 pre_tax 566. 0 ## 6 3 Snake Lounge 0 post_tax 560. 1 ## 7 4 Snake Lounge 0 pre_tax 607. 0 ## 8 4 Snake Lounge 0 post_tax 624. 1 ## 9 5 Snake Lounge 0 pre_tax 573. 0 ## 10 5 Snake Lounge 0 post_tax 607. 1   Exploring our data We can use our soda_tax_df to explore the distribution of soda consumption at different points in time.\nLet‚Äôs try first to look at the differences in the distribution only at the pre-tax time period:\nggplot(soda_tax_df, aes(x = pre_tax, fill = factor(treatment))) + geom_density(alpha = 0.5) + # density plot with transparency (alpha = 0.5) scale_fill_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;)) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(title = \u0026quot;Distribution of soda consumption before the tax was imposed\u0026quot;, x = \u0026quot;Soda consumtion (oz)\u0026quot;, y = \u0026quot;Density\u0026quot;) Let‚Äôs look at the post-tax period:\nggplot(soda_tax_df, aes(x = post_tax, fill = factor(treatment))) + geom_density(alpha = 0.5) + # density plot with transparency (alpha = 0.5) scale_fill_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;)) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(title = \u0026quot;Distribution of soda consumption after the tax was imposed\u0026quot;, x = \u0026quot;Soda consumtion (oz)\u0026quot;, y = \u0026quot;Density\u0026quot;) Since in our soda_tax_df_long we represent the time and soda consumption dimensions under the same columns, we can create even more complex graphs.\nLet‚Äôs leverage a new layer of our grammar of graphs: Facets\nWe will use facet_grid() which forms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.\nsoda_tax_df_long %\u0026gt;% dplyr::mutate(period = ifelse(period == \u0026quot;post_tax\u0026quot;, \u0026quot;T1 - Post-tax\u0026quot;, \u0026quot;T0 - Pre-tax\u0026quot;), # create more meaningful labels treatment = ifelse(treatment == 1, \u0026quot;Treated (D=1)\u0026quot;, \u0026quot;Untreated (D=0)\u0026quot;)) %\u0026gt;% dplyr::group_by(period, treatment) %\u0026gt;% # group to extract means of each group at each time dplyr::mutate(group_mean = mean(soda_drank)) %\u0026gt;% # extract means of each group at each time ggplot(., aes(x = soda_drank, fill = factor(treatment))) + geom_density(alpha = 0.5) + scale_fill_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#cc0055\u0026quot;, \u0026quot;#a7a8aa\u0026quot;), labels = c(\u0026quot;Treatment\u0026quot;, \u0026quot;Control\u0026quot;)) + facet_grid(treatment~period) + # we specify the matrix (treatment and period) geom_vline(aes(xintercept = group_mean), linetype = \u0026quot;longdash\u0026quot;) + # add vertical line with the mean theme_bw() + theme(legend.position = \u0026quot;none\u0026quot;) + labs(x = \u0026quot;Soda consumed (oz)\u0026quot;, y = \u0026quot;Density\u0026quot;)  Modeling and estimating So far we have ignored time in our estimations. Up until this point, most of the tools we have learned produce estimates of the counterfactual through explicit assignment rules that work randomly or as-if-randomly (e.g.¬†randomized experimental, regression discontinuity, and instrumental variable set-ups).\nDifference-in-differences compares the changes in outcomes over time between units under different treatment states. This allows us to correct for any differences between the treatment and comparison groups that are constant over time assuming that the trends in time are parallel.\na. Calculating without time If we did not have the pre_tax baseline measure, we would likely utilize the post_tax to explore the average effect on the treated. In this case, we would model this as:\nafter_model \u0026lt;- lm(post_tax ~ treatment, data = soda_tax_df) modelsummary::modelsummary(after_model, statistic = \u0026quot;conf.int\u0026quot;, gof_omit=\u0026quot;AIC|BIC|Log.Lik.\u0026quot;)    Model 1      (Intercept)  523.273     [518.008, 528.537]    treatment  -146.918     [-154.363, -139.472]    Num.Obs.  7500    R2  0.166    R2 Adj.  0.166    F  1496.245     We could read this result substantively as: those who lived in districts were the tax was implemented consumed on average 146.9 ounces less of sugar-added drinks per week compared to those who lived in districts were the tax was not put in place. This calculation would give us a comparison of the treatment and control groups after treatment.\nTo believe the results of our after_model, we would need to believe that the mean ignorability of treatment assignment assumption is fulfilled. We would have to think carefully about possible factors that could differentiate our treatment and control groups. We use a treatment indicator based on the districts where the measure was able to be implemented. Treatment was not fully randomly assigned, so there may be lots of potential confounders that create baseline differences in the scores for those living in Old Eagleton compared to those in Snake Lounge, which also affect the after-treatment comparisons.\nIf we think about the mechanics behind this naive calculation, we are just comparing the average observed outcomes for those treated and not treated after the tax was imposed:\n  Treatment  Average after tax      0  523.27    1  376.35     ggplot(soda_tax_df, aes(x = post_tax, fill = factor(treatment))) + geom_density(alpha = 0.5) + scale_fill_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;)) + geom_vline(xintercept = 523.27, linetype = \u0026quot;longdash\u0026quot;, color = \u0026quot;#a7a8aa\u0026quot;) + #avg for the untreated geom_vline(xintercept = 376.35, linetype = \u0026quot;longdash\u0026quot;, color = \u0026quot;#cc0055\u0026quot;) + #avg for the treated theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(title = \u0026quot;Distribution of soda consumption after the tax was imposed\u0026quot;, x = \u0026quot;Soda consumtion (oz)\u0026quot;, y = \u0026quot;Density\u0026quot;)  b. Including the time dimension (Manual extraction of the DiD estimate) During the lecture component of the class, we learned that the \\(\\beta_{DD}\\) is the difference in the differences. You can see it illustrated in the table. We can extract the observed values at each iteration of the treatment and time matrix and then manually subtract the differences.\n   Period      Treatment  Pre-tax  Post-tax  Difference      1  511.13  376.35  -134.78    0  508.31  523.27  14.97     ## [1] -149.74 ## [1] -149.75 We can just manually subtract.\n\\[\\beta_{DD} = -134.79 - 14.97 = -149.76\\]\n c.¬†Including the time dimension (First differences on treatment indicator) We can introduce the time component to our calculation by incorporating the pre-treatment levels of sugar-added drink consumption, which gives us the diff-in-diff estimate. We could calculate this in a fairly straightforward manner by creating a variable assessing the change in our wide format data frame:\n change: The difference in sugar-added drink consumption between post- and pre-tax  soda_tax_df \u0026lt;- soda_tax_df %\u0026gt;% dplyr::mutate(change = post_tax - pre_tax) #simple subtraction did_model \u0026lt;- lm(change ~ treatment, data = soda_tax_df) modelsummary::modelsummary(did_model, statistic = \u0026quot;conf.int\u0026quot;, gof_omit=\u0026quot;AIC|BIC|Log.Lik.\u0026quot;)     Model 1      (Intercept)  14.967     [14.625, 15.308]    treatment  -149.744     [-150.228, -149.261]    Num.Obs.  7500    R2  0.980    R2 Adj.  0.980    F  369242.378     We could read this result substantively as: those who lived in districts were the tax was implemented consumed on average 149.7 ounces less of sugar-added drinks per week compared to those who lived in districts were the tax was not put in place. This calculation would give us the change, or difference, in sugar-added drink consumption for treatment and control groups.\nTo believe the results of our did_model, we would need to believe that there are parallel trends between the two groups.\nNote: when simulating the data the post_tax was defined as: \\(post\\_tax = 15 + pre\\_tax - 150(treatment) + error\\)\n d.¬†Including the time dimension (Regression formulation of the DiD model) Remember the formula from the lecture where we estimate the diff-in-diff effect with time and treatment dummies? We can re-format our data to gather our diff-in-diff estimate.\n\\[Y_{it} = Œ≤_0 + Œ≤_1D^*_i + Œ≤_2P_t + Œ≤_{DD}D^‚àó_i √ó P_t + q_{it} \\]\nwhere \\(D^*_i\\) tell us if subject \\(i\\) is in the treatment group and \\(P_t\\) indicates the point in time (1 for post)\nFor this calculation we need our data in long format to use the time and treatment dummy variables.\nWe can see that under our long format, we have two entries for every individual. We have our variable after_tax, which represents \\(P_t\\), where 0 and 1 are pre- and post-tax periods respectively. We can now render our regression based on the formula:\n\\[Y_{it} = Œ≤_0 + Œ≤_1D^*_i + Œ≤_2P_t + Œ≤_{DD}D^‚àó_i √ó P_t + q_{it}\\]\ndid_long_clustered_se \u0026lt;- estimatr::lm_robust(soda_drank ~ treatment + after_tax + treatment*after_tax, clusters = district, se_type = \u0026quot;stata\u0026quot;, data = soda_tax_df_long) modelsummary::modelsummary(list(did_long_clustered_se), statistic = \u0026quot;conf.int\u0026quot;)    Model 1      (Intercept)  508.306     [508.077, 508.535]    treatment  2.827     [-1.827, 7.480]    after_tax  14.967     [14.890, 15.043]    treatment √ó after_tax  -149.744     [-150.049, -149.440]    Num.Obs.  15000    R2  0.117    R2 Adj.  0.117    se_type  stata     If we apply the switch logic to the results, we would get the same values from the table and plots     Period      Treatment  Pre-tax  Post-tax  Difference      1  511.13  376.35  -134.78    0  508.31  523.27  14.97     soda_tax_df_long %\u0026gt;% dplyr::mutate(period = ifelse(period == \u0026quot;post_tax\u0026quot;, \u0026quot;T1 - Post-tax\u0026quot;, \u0026quot;T0 - Pre-tax\u0026quot;), # create more meaningful labels treatment = ifelse(treatment == 1, \u0026quot;Treated (D=1)\u0026quot;, \u0026quot;Untreated (D=0)\u0026quot;)) %\u0026gt;% dplyr::group_by(period, treatment) %\u0026gt;% # group to extract means of each group at each time dplyr::mutate(group_mean = mean(soda_drank)) %\u0026gt;% # extract means of each group at each time ggplot(., aes(x = soda_drank, fill = factor(treatment))) + geom_density(alpha = 0.5) + scale_fill_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#cc0055\u0026quot;, \u0026quot;#a7a8aa\u0026quot;), labels = c(\u0026quot;Treatment\u0026quot;, \u0026quot;Control\u0026quot;)) + facet_grid(treatment~period) + # we specify the matrix (treatment and period) geom_vline(aes(xintercept = group_mean), linetype = \u0026quot;longdash\u0026quot;) + # add vertical line with the mean theme_bw() + theme(legend.position = \u0026quot;none\u0026quot;) + labs(x = \u0026quot;Soda consumed (oz)\u0026quot;, y = \u0026quot;Density\u0026quot;) soda_tax_df_long %\u0026gt;% dplyr::group_by(period, treatment) %\u0026gt;% # group to extract means of each group at each time dplyr::mutate(group_mean = mean(soda_drank)) %\u0026gt;% ggplot(aes(x = after_tax, y = group_mean, color = factor(treatment))) + geom_point() + geom_line(aes(x = after_tax, y = group_mean)) + scale_x_continuous(breaks = c(0,1)) + scale_color_manual(name = \u0026quot; \u0026quot;, # changes to color dimension values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;)) + labs(x = \u0026quot;Time periods\u0026quot;, y = \u0026quot;Ounces of soda drank per week\u0026quot;, color = \u0026quot;Treatment group\u0026quot;)+ theme_minimal()  The mechanics behind DiD    Drafting some brief recommedations Based on your analysis of the data at hand, you decide to recommend that the tax measure should move forward in the rest of Pawnee. You state that it is a very good example of a pigouvian tax, which captures the negative externalizes not included in the market price of sugar-added drinks. The findings suggest that the tax reduced the weekly sugar-added drink consumption by about 150 liquid ounces (almost 4.5 liters).\nYour evaluation report is so convincing that the Director of the Parks Department, Ron Swanson, is even doubting libertarianism.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"70296dcd1bc23df59a59960d7b8cec7e","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-8/08-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-8/08-online-tutorial/","section":"materials","summary":"Welcome Introduction! Welcome to our eighth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you were introduced to Difference in Differences (DiD).","tags":null,"title":"Difference in Differences","type":"book"},{"authors":null,"categories":null,"content":"  Welcome Introduction! Welcome to our sixth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week's lecture you reviewed what happens when experiments break due to non-compliance. You were also introduced to encouragement experimental setups, and their the observational analogue, instrumental variables. Finally, you learned how you can estimate local average treatment effects by breaking out the variation in D into two parts.\nIn this lab session we will:\n Review how to manually extract the LATE through the wald estimator Learn how to perform Two-stage Least Squares regression (2SLS) with ivreg() from the AER package Illustrate the mechanics of Two-stage Least Squares regression (2SLS) with lm()  Packages # These are the libraries we will use today. Make sure to install them in your console in case you have not done so previously. library(tidyverse) # To load the collection of packages in the tidyverse library(dplyr) # To wrangle our data (this package is also loaded by library(tidyverse)) library(readr) # To load the .csv data (this package is also loaded by library(tidyverse)) library(ggplot2) # To create plots (this package is also loaded by library(tidyverse)) library(tidyr) # To use pivot_wider() (this package is also loaded by library(tidyverse)) library(ggdag) # To dagify and plot our DAG objects in R #library(summarytools) # for ctable() library(broom) # To format regression output library(janitor) # To examine our data with tabyl() library(stargazer) # To format model output library(knitr) # To create HTML tables with kable() library(kableExtra) # To format the HTML tables library(AER) # To run 2SLS with ivreg()    Measuring the effect of mosquito net use on malaria infections Imagine that the organization you work for is laying out a project to distribute mosquito nets to help combat malaria transmitions.\nThe funding agency requires a impact evaluation report from your organization. You are in charge of running the evaluation of this program.\nYou realize that there are potential unobserved counfounders that could bias the observed differences in malaria risk for mosquito net users and non-users. You also think about the ethical considerations of fully randomizing who receives the nets, so you remember your Statistics II lecture on IVs and set up an encouragement design.\nYour benificiaries are scattered across ten villages. You decide to randomly select five villages to send SMS reminders every night encouraging them to use the mosquito nets. (This example is using simulated data) Assumptions To render credible results for the evaluation of this program, we need to fulfill a certain set of assumtions:\na) Relevance: Also known as non-zero average encouragement effect. Does our \\(Z\\) create variation in our \\(D\\)? In other words, is the mosquito net use different under the encouragement group? (Statistically testable)\nb) Exogeneity/Ignorability of the instrument: Potential outcomes and treatments are independent of \\(Z\\). In this case given by out randomization of encouragement by villages.\nc) Exclusion restriction: The instrument only affects the outcome via the treatment. In other words, there are no alternative paths through which our SMS can have an effect on malaria infections other that the use of the mosquito nets.\nd) Monotonicity: No defiers. We assume that non-compliers fall in the camp of always- and never-takers. We would not expect subjects who when encouraged would not use the nets, but would use them if they did not recieve the SMS reminder.\n  Exploring our data evaluation_df \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/seramirezruiz/stats-ii-lab/master/Session%205/data/evaluation_data.csv\u0026quot;) # loading simulated data frame of the intervention You receive the results of your intervention from the M\u0026amp;E officers. There are 1000 inhabitants across the ten villages. This is what the data look like:\n village_name: A character string with the name of the village sms: A binary marker for the SMS encouragement net_use: A binary marker for mosquito net use malaria: A binary marker for malaria infection  Compliance types  You may remember, this table from the lecture:\nWe can crosstabulate our data with janitor::tabyl() and the additional features of the janitor::adorn_* functions.\nWhy janitor::tabyl()? Because as prospective policy analysts we will do a lot of counting.\nAs the vignette of the package even puts it:\nAnalysts do a lot of counting. Indeed, it‚Äôs been said that 'data science is mostly counting things.' But the base R function for counting, table(), leaves much to be desired:\n It doesn‚Äôt accept data.frame inputs (and thus doesn‚Äôt play nicely with the %\u0026gt;% pipe) It doesn‚Äôt output data.frames Its results are hard to format. Compare the look and formatting choices of an R table to a Microsoft Excel PivotTable.  This is how it works. Say we are interested in exploring the number of persons in each of the observed strata, we would do:\nevaluation_df %\u0026gt;% # your data frame janitor::tabyl(net_use, sms) %\u0026gt;% # the two dimensions for the table (D, Z) janitor::adorn_totals(c(\u0026quot;row\u0026quot;, \u0026quot;col\u0026quot;)) %\u0026gt;% # add totals for rows and cols knitr::kable() %\u0026gt;% # turn into a kable table for nice rendering in HTML kableExtra::kable_styling() %\u0026gt;% kableExtra::add_header_above(c(\u0026quot;\u0026quot;, \u0026quot;sms\u0026quot; = 2, \u0026quot;\u0026quot;)) #add header for sms    sms      net_use  0  1  Total      0  335  110  445    1  165  390  555    Total  500  500  1000     If you want to learn more about the syntax of tabyl(), make sure to check the vignette\nExercise  Let's explore the compliance types from this table   Where are our compliers and non-compliers? How many people were encouraged via SMS, but did not use the net? How many people were not encouraged via SMS, yet they utilized the net?  Average malaria infections across strata  We can utilize the tabyl() syntax and our knowledge from the grammar of graphics to table and visualize the distribution of malaria on each stratum:\nTable: Count of malaria infections across strata (Y,Z)  evaluation_df %\u0026gt;% # your data frame janitor::tabyl(malaria, sms) %\u0026gt;% # the two dimensions for the table (Y, Z) janitor::adorn_totals(c(\u0026quot;row\u0026quot;, \u0026quot;col\u0026quot;)) %\u0026gt;% # add totals for rows and cols knitr::kable() %\u0026gt;% # turn into a kable table for nice rendering in HTML kableExtra::kable_styling() %\u0026gt;% kableExtra::add_header_above(c(\u0026quot;\u0026quot;, \u0026quot;sms\u0026quot; = 2, \u0026quot;\u0026quot;)) #add header for sms    sms      malaria  0  1  Total      0  248  410  658    1  252  90  342    Total  500  500  1000     Plot: Distribution of malaria infections across strata  ggplot(evaluation_df, aes(x = factor(sms), y = factor(net_use), color = factor(malaria))) + geom_jitter() + theme_minimal() + scale_x_discrete(labels = c(\u0026quot;SMS = 0\u0026quot;, \u0026quot;SMS = 1\u0026quot;)) + scale_y_discrete(limits = c(\u0026quot;1\u0026quot;,\u0026quot;0\u0026quot;), labels = c(\u0026quot;NET = 1\u0026quot;, \u0026quot;NET = 0\u0026quot;)) + scale_color_manual(values = c(\u0026quot;#CDCDCD\u0026quot;, \u0026quot;#CC0055\u0026quot;), labels = c(\u0026quot;Not infected\u0026quot;, \u0026quot;Infected\u0026quot;)) + labs(x = \u0026quot;Encouragement\u0026quot;, y = \u0026quot;Treatment\u0026quot;, color = \u0026quot;\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) Exercise   What insights can we gather from the table and plot?   Exploring our set-up Let's check whether SMS encouragement is a strong instrument In other words, we are looking at the relevance assumption. Does our SMS encouragement create changes in our mosquito net use?\nlm(net_use ~ sms, data = evaluation_df) %\u0026gt;% stargazer::stargazer(type = \u0026quot;text\u0026quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## net_use ## ----------------------------------------------- ## sms 0.450*** ## (0.028) ## ## Constant 0.330*** ## (0.020) ## ## ----------------------------------------------- ## Observations 1,000 ## R2 0.205 ## Adjusted R2 0.204 ## Residual Std. Error 0.444 (df = 998) ## F Statistic 257.315*** (df = 1; 998) ## =============================================== ## Note: *p\u0026lt;0.1; **p\u0026lt;0.05; ***p\u0026lt;0.01 Economists have established as a \u0026quot;rule-of-thumb\u0026quot; for the case of a single endogenous regressor to be considered a strong instrument should have a F-statistic 1 greater than 10. From this regression, we can see that SMS encouragement is a strong instrument.\nAdditionally, the substantive read in this case is that only 33% of those who did not receive the SMS utilized the mosquito nets, where as 78% of those who got the SMS encouragement did.  Let's gather a na√Øve estimate of mosquito net use and malaria infection. Why do you think we call this a na√Øve estimate?\nnaive_model \u0026lt;- lm(malaria ~ net_use, data = evaluation_df) stargazer::stargazer(naive_model, type = \u0026quot;text\u0026quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## malaria ## ----------------------------------------------- ## net_use -0.615*** ## (0.023) ## ## Constant 0.683*** ## (0.017) ## ## ----------------------------------------------- ## Observations 1,000 ## R2 0.415 ## Adjusted R2 0.414 ## Residual Std. Error 0.363 (df = 998) ## F Statistic 707.002*** (df = 1; 998) ## =============================================== ## Note: *p\u0026lt;0.1; **p\u0026lt;0.05; ***p\u0026lt;0.01 Exercise   What would our expectations be under the na√Øve model?  \n Let's gather our intent-to-treat effect (ITT) This is the effect that our SMS encouragement had on malaria infections. \\[ITT = E(Malaria_i|SMS=1) - E(Malaria_i|SMS=0)\\]\nitt_model \u0026lt;- lm(malaria ~ sms, data = evaluation_df) stargazer::stargazer(itt_model, type = \u0026quot;text\u0026quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## malaria ## ----------------------------------------------- ## sms -0.324*** ## (0.028) ## ## Constant 0.504*** ## (0.020) ## ## ----------------------------------------------- ## Observations 1,000 ## R2 0.117 ## Adjusted R2 0.116 ## Residual Std. Error 0.446 (df = 998) ## F Statistic 131.753*** (df = 1; 998) ## =============================================== ## Note: *p\u0026lt;0.1; **p\u0026lt;0.05; ***p\u0026lt;0.01 Exercise   What does this tell us?  \n  Let's gather out local average treatment effect (LATE) We have several options for this:\n Wald Estimator We can calculate this by hand. Let's try that now using the values from the table we created earlier. We can also calculate the average malaria rates among those who did and did not receive an SMS (no SMS = 0.504, yes SMS = 0.18).  Table: Observations across strata (D,Z)     sms      net_use  0  1  Total      0  335  110  445    1  165  390  555    Total  500  500  1000     Table: Count of malaria infections across strata (Y,Z)     sms      malaria  0  1  Total      0  248  410  658    1  252  90  342    Total  500  500  1000      Local Average Treatment Effect (LATE) manually  \n \\[LATE = \\frac{cov(Y_i,Z_i)}{cov(D_i,Z_i)}\\]\n Let's take a look at our numerator \\(cov(Y_i,Z_i)\\), also \\(ITT\\)\n \\(E(Y|Z = 1) = \\frac{90}{(410+90)} = 0.18\\) \\(E(Y|Z = 0) = \\frac{252}{(252+248)} = 0.504\\)  Let's take a look at our denominator \\(cov(D_i,Z_i)\\)  \\(E(D‚à£Z = 1) = \\frac{390}{(390 + 110)} = 0.78\\) \\(E(D‚à£Z = 0) = \\frac{165}{(165 + 335)} = 0.33\\)  We can calculate our LATE\n \\[LATE = \\frac{(0.18 - 0.504)}{(0.78 - 0.33)} = -0.72\\]\n  Two-stage Least Squares (2SLS). We will learn how to do this with ivreg(), which is part of the AER package. It fits instrumental-variable regression through two-stage least squares. The syntax is the following:  ivreg(outcome ~ treatment | instrument, data) In our case:\nlate_model \u0026lt;- AER::ivreg(malaria ~ net_use | sms, data = evaluation_df) summary(late_model) ## ## Call: ## AER::ivreg(formula = malaria ~ net_use | sms, data = evaluation_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7416 -0.0216 -0.0216 0.2584 0.9784 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.74160 0.03089 24.00 \u0026lt;2e-16 *** ## net_use -0.72000 0.05159 -13.96 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.3671 on 998 degrees of freedom ## Multiple R-Squared: 0.4025, Adjusted R-squared: 0.4019 ## Wald test: 194.8 on 1 and 998 DF, p-value: \u0026lt; 2.2e-16 Exercise   What is the substantive reading of these results? What would you tell the funding partner in your evaluation report?  \n Mechanics behind two-stage least squares (2SLS) What ivreg() is doing in the background is the following:\nnet_use_hat \u0026lt;- lm(net_use ~ sms, evaluation_df)$fitted.values # get fitted values from first stage (the part of x that is exogenously driven by z) summary(lm(evaluation_df$malaria ~ net_use_hat)) # run second stage with instrumented x (careful, the standard errors are wrong; better use ivreg() from AER instead) ## ## Call: ## lm(formula = evaluation_df$malaria ~ net_use_hat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.504 -0.180 -0.180 0.496 0.820 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.74160 0.03757 19.74 \u0026lt;2e-16 *** ## net_use_hat -0.72000 0.06273 -11.48 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.4463 on 998 degrees of freedom ## Multiple R-squared: 0.1166, Adjusted R-squared: 0.1157 ## F-statistic: 131.8 on 1 and 998 DF, p-value: \u0026lt; 2.2e-16 \n Thinking about the validity of instruments We can also adapt the ivreg() syntax to accomodate valid conditional instruments:\nAER::ivreg(Y ~ D + W | Z + W, data = your_df) \n  An F statistic is a value you get when you run an ANOVA test or a regression analysis to find out if the means between two populations are significantly different. It‚Äôs similar to a t-statistic from a t-test; A-T test will tell you if a single variable is statistically significant and an F test will tell you if a group of variables are jointly significant.‚Ü©\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"11762b0936110487221daaf455add366","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-6/06-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-6/06-online-tutorial/","section":"materials","summary":"Welcome Introduction! Welcome to our sixth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week's lecture you reviewed what happens when experiments break due to non-compliance.","tags":null,"title":"Instrumental Variables","type":"book"},{"authors":null,"categories":null,"content":"   Welcome Introduction! Welcome to our fifth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you reviewed randomization in experimental setups. You also learned how matching can be leveraged to gather causal estimates.\nIn this lab session we will:\n Take a step back to review how to compare the means of two groups in R Learn how to perform matching with the MatchIt package Illustrate the mechanics of propensity score matching with gml()  Packages # These are the libraries we will use today. Make sure to install them in your console in case you have not done so previously. library(tidyverse) # To use dplyr functions and the pipe operator when needed library(ggplot2) # To create plots (this package is also loaded by library(tidyverse)) library(purrr) # To repeat code across our list in the balance table purrr::map() (this package is also loaded by library(tidyverse)) library(broom) # To format regression output library(stargazer) # To format model output library(knitr) # To create HTML tables with kable() library(kableExtra) # To format the HTML tables library(MatchIt) # To match    Our data Today we will work with data from the Early Childhood Longitudinal Study (ECLS).\necls_df \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/seramirezruiz/stats-ii-lab/master/Session%204/data/ecls.csv\u0026quot;) %\u0026gt;% dplyr::select(-childid, -race, -w3daded, -w3momed, -w3inccat) #drop these columns (-) names(ecls_df) #checking the names of the variables in the data frame ## [1] \u0026quot;catholic\u0026quot; \u0026quot;race_white\u0026quot; \u0026quot;race_black\u0026quot; \u0026quot;race_hispanic\u0026quot; ## [5] \u0026quot;race_asian\u0026quot; \u0026quot;p5numpla\u0026quot; \u0026quot;p5hmage\u0026quot; \u0026quot;p5hdage\u0026quot; ## [9] \u0026quot;w3daded_hsb\u0026quot; \u0026quot;w3momed_hsb\u0026quot; \u0026quot;w3momscr\u0026quot; \u0026quot;w3dadscr\u0026quot; ## [13] \u0026quot;w3income\u0026quot; \u0026quot;w3povrty\u0026quot; \u0026quot;p5fstamp\u0026quot; \u0026quot;c5r2mtsc\u0026quot; ## [17] \u0026quot;c5r2mtsc_std\u0026quot; (Example inspired by Simon Ejdemyr: https://sejdemyr.github.io/r-tutorials/statistics/tutorial8.html)\nReference links: MatchIt: https://cran.r-project.org/web/packages/MatchIt/vignettes/matchit.pdf Cobalt: (optional library for matching plots and extra features): https://cran.r-project.org/web/packages/cobalt/vignettes/cobalt_A0_basic_use.html kableExtra: (for formatting tables): https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html Stargazer: (for formatting model output): https://www.jakeruss.com/cheatsheets/stargazer/ Video overview of matching concepts: https://fr.coursera.org/lecture/crash-course-in-causality/overview-of-matching-JQfPC    Comparing Differences in Means and Balance Tables As you may have seen in this week‚Äôs application paper, balance tables feature prominently in work that utilizes matching.\nIn binary treatment setups, balance tables present an overview of the difference in means for the groups accross covariates.\nLet‚Äôs take the dataset as an example to review how to compare differences in means and build balance tables in R. There are multiple ways to explore these kinds of questions. In this lab we will leverage t-tests to check the statistical significance of the difference in means.\nIn other words, we want to know whether the observed differences in average value of a variable between the two groups or samples can be due to random chance (our null hypothesis).\nT-tests in R In this case, let‚Äôs imagine we want to check the statistical significance of the differences in the composition of the catholic and public school samples in the w3povrty (under the poverty line) variable.\nThe general syntax for a t-test is simply as follows. The vectors refer to the variables whose mean you want to compare.\nt.test(y ~ x, data = your_data) Exercise  t.test(w3povrty ~ catholic, data = ecls_df) ## ## Welch Two Sample t-test ## ## data: w3povrty by catholic ## t = 26.495, df = 4034.6, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## 0.1678107 0.1946307 ## sample estimates: ## mean in group 0 mean in group 1 ## 0.21918633 0.03796562  What does the group 0 mean tell us? What does the group 1 mean tell us? Is the difference between catholic school and public school students statistically significant?   Balance tables in R Now let‚Äôs take a look at how can we create a simple and good looking balance table. The idea here is to compare the mean values of different variables across populations or groups. In our case, let‚Äôs see whether the catholic and public school groups differ across the covariates in the dataset:\nHere is one way to create the table (you can adapt this code for the assignment).\n# create a list with the covariates list_cov \u0026lt;- c(\u0026quot;race_white\u0026quot;, \u0026quot;race_black\u0026quot;, \u0026quot;race_hispanic\u0026quot;, \u0026quot;race_asian\u0026quot;, \u0026quot;p5numpla\u0026quot;, \u0026quot;p5hmage\u0026quot;, \u0026quot;p5hdage\u0026quot;, \u0026quot;w3daded_hsb\u0026quot;, \u0026quot;w3momed_hsb\u0026quot;, \u0026quot;w3momscr\u0026quot;, \u0026quot;w3dadscr\u0026quot;, \u0026quot;w3income\u0026quot;, \u0026quot;w3povrty\u0026quot;, \u0026quot;p5fstamp\u0026quot;, \u0026quot;c5r2mtsc\u0026quot;, \u0026quot;c5r2mtsc_std\u0026quot;) ecls_df %\u0026gt;% # our data frame dplyr::summarize_at(list_cov, funs(list(broom::tidy(t.test(. ~ catholic))))) %\u0026gt;% # sequentially run t-tests across all the covariates in the list_cov (note that you have to change the \u0026quot;treatment\u0026quot;) purrr::map(1) %\u0026gt;% # maps into a list dplyr::bind_rows(.id=\u0026#39;variables\u0026#39;) %\u0026gt;% # binds list into a single data frame and names the id column \u0026quot;variables\u0026quot; dplyr::select(variables, estimate1, estimate2, p.value) %\u0026gt;% # select only the names, group means, and p-values dplyr::mutate_if(is.numeric, round, 3) %\u0026gt;% # round numeric variables to three places knitr::kable(col.names = c(\u0026quot;Variable\u0026quot;, \u0026quot;(Catholic = 0)\u0026quot;, \u0026quot;(Catholic = 1)\u0026quot;, \u0026quot;P value\u0026quot;)) %\u0026gt;% # create kable table and remane headings kableExtra::kable_styling() # style kable table for our knitted document   Variable  (Catholic = 0)  (Catholic = 1)  P value      race_white  0.556  0.725  0.000    race_black  0.136  0.054  0.000    race_hispanic  0.181  0.131  0.000    race_asian  0.066  0.052  0.025    p5numpla  1.133  1.093  0.000    p5hmage  37.561  39.575  0.000    p5hdage  40.392  41.988  0.000    w3daded_hsb  0.488  0.259  0.000    w3momed_hsb  0.464  0.227  0.000    w3momscr  43.114  47.492  0.000    w3dadscr  42.713  46.356  0.000    w3income  54889.159  82074.301  0.000    w3povrty  0.219  0.038  0.000    p5fstamp  0.129  0.015  0.000    c5r2mtsc  50.209  52.389  0.000    c5r2mtsc_std  -0.031  0.194  0.000     Exercise   Are the differences in means significant at conventional levels? What differences strike you from the composition of the two samples?    The Effect of Catholic School on Student Achievement  In this tutorial we‚Äôll analyze the effect of going to Catholic school, as opposed to public school, on student achievement. Because students who attend Catholic school on average are different from students who attend public school, we will use matching to get more credible causal estimates of Catholic schooling.\n Exploration of the data ecls_df %\u0026gt;% dplyr::group_by(catholic) %\u0026gt;% dplyr::summarize(n_students = n(), avg_math = mean(c5r2mtsc_std), std_error = sd(c5r2mtsc_std) / sqrt(n_students)) %\u0026gt;% round(3) %\u0026gt;% # round the results knitr::kable() %\u0026gt;% # create kable table kableExtra::kable_styling() # view kable table   catholic  n_students  avg_math  std_error      0  9568  -0.031  0.010    1  1510  0.194  0.022     We can see that we have many more students that did not attend Catholic school than those who did. Also, the Catholic school students have a higher average math score.\n Naive Average Treatment Effect (NATE) We can naively compare the students on their standardized math scores (c5r2mtsc_std). As you remember, the Naive Average Treatment Effect (NATE) is the difference in the means of the observed outcomes of the two groups:\n\\[NATE = E(y^1 | D = 1) - E(y^0 | D = 0)\\]\nIn this case, the NATE would be difference between the average math scores.\nNATE manually Do you remember what we did in the last section?\nWe could substract the avg_math for the catholic = 1 and the avg_math for the catholic = 0\n\\[NATE = 0.194 - (-0.031)\\] \\[NATE = 0.194 + 0.031 = 0.225\\]\n NATE with t.test() t.test(c5r2mtsc_std ~ catholic, data = ecls_df) ## ## Welch Two Sample t-test ## ## data: c5r2mtsc_std by catholic ## t = -9.1069, df = 2214.5, p-value \u0026lt; 2.2e-16 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -0.2727988 -0.1761292 ## sample estimates: ## mean in group 0 mean in group 1 ## -0.03059583 0.19386817  NATE with lm() lm(c5r2mtsc_std ~ catholic, data = ecls_df) %\u0026gt;% stargazer::stargazer(.,type = \u0026quot;html\u0026quot;)       Dependent variable:           c5r2mtsc_std       catholic   0.224***      (0.028)         Constant   -0.031***      (0.010)            Observations   11,078    R2   0.006    Adjusted R2   0.006    Residual Std. Error   0.997 (df = 11076)    F Statistic   66.096*** (df = 1; 11076)       Note:  p\u0026lt;0.1; p\u0026lt;0.05; p\u0026lt;0.01    Exercise   What parallels do you find between substracting the manually extracted means, the t-test, and the linear regression?     Matching with MatchIt MatchIt is designed for causal inference with a dichotomous treatment variable and a set of pretreatment control variables. Any number or type of dependent variables can be used.\nWe have three steps:\n Perform the match with MatchIt::matchit() Create a new data frame with the matched data with MatchIt::match.data() or MatchIt::get_matches() Model  The basic syntax is as follows:\nmatch_process \u0026lt;- MatchIt::matchit(treatment ~ x1 + x2, data = mydata) # NOTE. We include treatment ~ covariates matched_df \u0026lt;- MatchIt::get_matches(match_process) #when matching with replacement matched_df \u0026lt;- MatchIt::match.data(match_process) #for other cases matched_model \u0026lt;- lm(outcome ~ trearment, data = matched_df) where treat is the dichotomous treatment variable, and x1 and x2 are pre-treatment co-variates, all of which are contained in the data frame mydata.\n As you can see, the outcome variable is not included in the matching procedure.   MatchIt is capable of using several matching methods:\n Exact (method = ‚Äúexact‚Äù): The simplest version of matching is exact. This technique matches each treated unit to all possible control units with exactly the same values on all the covariates, forming subclasses such that within each subclass all units (treatment and control) have the same covariate values.\n Subclassification (method = ‚Äúsubclass‚Äù): When there are many covariates (or some covariates can take a large number of values), finding sufficient exact matches will often be impossible. The goal of subclassification is to form subclasses, such that in each of them the distribution (rather than the exact values) of covariates for the treated and control groups are as similar as possible.\n Nearest Neighbor (method = ‚Äúnearest‚Äù): Nearest neighbor matching selects the best control matches for each individual in the treatment group. Matching is done using a distance measure (propensity score) specified by the distance option (default = logit).\n As well as optimal matching, full matching, genetic matching, and coarsened exact matching, all of which are detailed in the documentation.\n  A few additional arguments are important to know about:\n distance: this refers to propensity scores. There are many options for how to calculate these within MatchIt.\n discard: specifies whether to discard units that fall outside some measure of support of the distance measure (default is ‚Äúnone‚Äù, discard no units). For example, if some treated units have extremely high propensity scores that are higher than any control units, we could drop those.\n replace: a logical value indicating whether each control unit can be matched to more than one treated unit (default is replace = FALSE, each control unit is used at most once).\n ratio: the number of control units to match to each treated unit (default is ratio = 1).\n There are also some optional arguments for most of the matching methods, which you can read about in the documentation if you are interested.\n  Exact Matching We can use a combination of the results from our balance table and theory to identify which variables to use for matching. Let‚Äôs perform an exact match with:\n race_white: Is the student white (1) or not (0)? p5hmage: Mother‚Äôs age w3income: Family income p5numpla: Number of places the student has lived for at least 4 months w3momed_hsb: Is the mother‚Äôs education level high-school or below (1) or some college or more (0)?  # first we must omit missing values (MatchIt does not allow missings) match_data \u0026lt;- ecls_df %\u0026gt;% dplyr::select(catholic, c5r2mtsc_std, race_white, p5hmage, w3income, p5numpla, w3momed_hsb) %\u0026gt;% na.omit() # perform exact match exact_match \u0026lt;- MatchIt::matchit(catholic ~ race_white + p5hmage + w3income + p5numpla + w3momed_hsb, method = \u0026quot;exact\u0026quot;, data = match_data) # Try seeing the output in the console with summary(exact_match) # grab the matched data into a new data frame data_exact_match \u0026lt;- MatchIt::match.data(exact_match) # estimate effect again with new data frame exact_match_model \u0026lt;- lm(c5r2mtsc_std ~ catholic, data = data_exact_match) stargazer::stargazer(exact_match_model, type = \u0026quot;html\u0026quot;)       Dependent variable:           c5r2mtsc_std       catholic   -0.101***      (0.030)         Constant   0.319***      (0.015)            Observations   5,405    R2   0.002    Adjusted R2   0.002    Residual Std. Error   0.934 (df = 5403)    F Statistic   11.340*** (df = 1; 5403)       Note:  p\u0026lt;0.1; p\u0026lt;0.05; p\u0026lt;0.01    Now we can see that the mean in the group that did not attend Catholic school is actually about 0.10 higher than the mean for those who did. The results are statistically significant given that the confidence interval does not contain zero, and we have a fairly small p-value.\n Propensity Scores If we want to perform non-exact matching, we can use propensity scores. We can generate these manually using a logit model on the unmatched data set.\nWhen we extract propensity scores, we model the propensity of each unit of falling under the treatment group based on their values on a set of covariates. This is how we would do this manually:\n# create a new column with income by the thousands for more interpretable output ecls_df \u0026lt;- ecls_df %\u0026gt;% dplyr::mutate(w3income_1k = w3income / 1000) # estimate logit model m_ps \u0026lt;- glm(catholic ~ race_white + w3income_1k + p5hmage + p5numpla + w3momed_hsb, family = binomial(link = \u0026quot;logit\u0026quot;), # you can also use a probit link here data = ecls_df) # extract predicted probabilities # type = \u0026quot;response\u0026quot; option tells R to output probabilities of the form P(Y = 1|X) prs_df \u0026lt;- dplyr::tibble(pr_score = predict(m_ps, type = \u0026quot;response\u0026quot;), catholic = m_ps$model$catholic) # the actual values Let‚Äôs plot the propensity scores by treatment group to explore common support:\n# Histogram ggplot(prs_df, aes(x = pr_score, fill = factor(catholic))) + geom_histogram(alpha = 0.5) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(title = \u0026quot;Propensity Score Distribution: Treatment and Control Groups\u0026quot;, x = \u0026quot;Propensity Score\u0026quot;, y = \u0026quot;Count\u0026quot;, fill = \u0026quot;Catholic School Attendance\u0026quot;) # Density plot ggplot(prs_df, aes(x = pr_score, fill = factor(catholic))) + geom_density(alpha = 0.5) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(title = \u0026quot;Propensity Score Distribution: Treatment and Control Groups\u0026quot;, x = \u0026quot;Propensity Score\u0026quot;, y = \u0026quot;Density\u0026quot;, fill = \u0026quot;Catholic School Attendance\u0026quot;)  # Jittered point plot ggplot(prs_df, aes(x = pr_score, y = factor(catholic), color = factor(catholic))) + geom_jitter() + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(title = \u0026quot;Propensity Score Distribution: Treatment and Control Groups\u0026quot;, x = \u0026quot;Propensity Score\u0026quot;, y = \u0026quot;Group\u0026quot;, color = \u0026quot;Catholic School Attendance\u0026quot;)  Exercise   What do these plots tell us?   Non-Exact Matching MatchIt can generate propensity scores itself, so we don‚Äôt need to manually go through the process above. Let‚Äôs try putting together a non-exact matching formula yourself! Try:\n nearest neighbor matching with replacement with a one-to-one ratio on the match_data dataset  one_match \u0026lt;- MatchIt::matchit(catholic ~ race_white + w3income + p5hmage + p5numpla + w3momed_hsb, method = \u0026quot;nearest\u0026quot;, ratio = 1, replace = TRUE, data = match_data) summary(one_match) ## ## Call: ## MatchIt::matchit(formula = catholic ~ race_white + w3income + ## p5hmage + p5numpla + w3momed_hsb, data = match_data, method = \u0026quot;nearest\u0026quot;, ## replace = TRUE, ratio = 1) ## ## Summary of Balance for All Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean ## distance 0.1927 0.1379 0.6486 1.0007 0.2086 ## race_white 0.7411 0.5914 0.3418 . 0.1497 ## w3income 82568.9357 55485.0210 0.5777 1.1373 0.1565 ## p5hmage 39.5932 37.5658 0.3874 0.6383 0.0408 ## p5numpla 1.0917 1.1298 -0.1242 0.6132 0.0076 ## w3momed_hsb 0.2234 0.4609 -0.5703 . 0.2375 ## eCDF Max ## distance 0.3109 ## race_white 0.1497 ## w3income 0.3062 ## p5hmage 0.1893 ## p5numpla 0.0277 ## w3momed_hsb 0.2375 ## ## ## Summary of Balance for Matched Data: ## Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean ## distance 0.1927 0.1927 0.0000 0.9954 0.0000 ## race_white 0.7411 0.7493 -0.0186 . 0.0081 ## w3income 82568.9357 81775.6653 0.0169 1.0052 0.0036 ## p5hmage 39.5932 39.6169 -0.0045 1.0179 0.0015 ## p5numpla 1.0917 1.0777 0.0459 1.1580 0.0031 ## w3momed_hsb 0.2234 0.2226 0.0018 . 0.0007 ## eCDF Max Std. Pair Dist. ## distance 0.0030 0.0001 ## race_white 0.0081 0.0625 ## w3income 0.0081 0.0396 ## p5hmage 0.0074 0.1131 ## p5numpla 0.0126 0.0846 ## w3momed_hsb 0.0007 0.0586 ## ## Percent Balance Improvement: ## Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max ## distance 100.0 -552.2 100.0 99.0 ## race_white 94.6 . 94.6 94.6 ## w3income 97.1 95.9 97.7 97.3 ## p5hmage 98.8 96.0 96.2 96.1 ## p5numpla 63.1 70.0 59.2 54.6 ## w3momed_hsb 99.7 . 99.7 99.7 ## ## Sample Sizes: ## Control Treated ## All 7915. 1352 ## Matched (ESS) 187.29 1352 ## Matched 510. 1352 ## Unmatched 7405. 0 ## Discarded 0. 0 We can interpret the resulting output as follows:\n Summary of balance for all data: Comparison of the means for all the data without matching Summary of balance for matched data: Comparison of means for matched data. Looking for them to become similar. Percent balance improvement: Higher is better, close to 100 is ideal. Sample sizes: How many units were matched in the control/treatment groups.  Now, let‚Äôs plot the propensity scores for the treated and untreated units.\n# simple plot - check out the cobalt package for nicer options, or use ggplot2 to create your own! plot(one_match, type = \u0026quot;hist\u0026quot;) Let‚Äôs extract the data from one_match and creating a balance table like the one we did before, just this time using the new data. Scroll down for the answer when you‚Äôre ready.\n# grab data set data_prop_match \u0026lt;- MatchIt::get_matches(one_match) # create list of covariates for the table list_cov \u0026lt;- c(\u0026quot;race_white\u0026quot;, \u0026quot;p5hmage\u0026quot;, \u0026quot;w3income\u0026quot;, \u0026quot;p5numpla\u0026quot;, \u0026quot;w3momed_hsb\u0026quot;) data_prop_match %\u0026gt;% # our data frame dplyr::summarize_at(list_cov, funs(list(broom::tidy(t.test(. ~ catholic))))) %\u0026gt;% # sequentially run t-tests across all the covariates in the list_cov (note that you have to change the \u0026quot;treatment\u0026quot;) purrr::map(1) %\u0026gt;% # maps into a list dplyr::bind_rows(.id=\u0026#39;variables\u0026#39;) %\u0026gt;% # binds list into a single data frame and names the id column \u0026quot;variables\u0026quot; dplyr::select(variables, estimate1, estimate2, p.value) %\u0026gt;% # select only the names, group means, and p-values dplyr::mutate_if(is.numeric, round, 3) %\u0026gt;% # round numeric variables to three places knitr::kable(col.names = c(\u0026quot;Variable\u0026quot;, \u0026quot;Control (Catholic = 0)\u0026quot;, \u0026quot;Treat (Catholic = 1)\u0026quot;, \u0026quot;P value\u0026quot;)) %\u0026gt;% # create kable table and rename headings kableExtra::kable_styling() # style kable table for our knitted document   Variable  Control (Catholic = 0)  Treat (Catholic = 1)  P value      race_white  0.749  0.741  0.628    p5hmage  39.617  39.593  0.906    w3income  81775.665  82568.936  0.659    p5numpla  1.078  1.092  0.216    w3momed_hsb  0.223  0.223  0.963     Those means look very close. Hooray.\nFinally, let‚Äôs estimate on the matched data set:\nprop_match_model \u0026lt;- lm(c5r2mtsc_std ~ catholic, data = data_prop_match) stargazer::stargazer(prop_match_model, type = \u0026quot;text\u0026quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## c5r2mtsc_std ## ----------------------------------------------- ## catholic -0.038 ## (0.037) ## ## Constant 0.248*** ## (0.026) ## ## ----------------------------------------------- ## Observations 2,704 ## R2 0.0004 ## Adjusted R2 0.00003 ## Residual Std. Error 0.955 (df = 2702) ## F Statistic 1.068 (df = 1; 2702) ## =============================================== ## Note: *p\u0026lt;0.1; **p\u0026lt;0.05; ***p\u0026lt;0.01 As with the exact matching, we can see that those that did not attend Catholic school performed better on the test than those who did. Still, we see that our results in this instance are not statistically significant.\n  ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9fd079130cd1bf6c9a57d9d1d257c687","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-5/05-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-5/05-online-tutorial/","section":"materials","summary":"Welcome Introduction! Welcome to our fifth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you reviewed randomization in experimental setups.","tags":null,"title":"Matching in R","type":"book"},{"authors":null,"categories":null,"content":"   Welcome Introduction! Welcome to our tenth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you were introduced to Moderation and Heterogeneous Effects.\nIn this lab session we will:\n Learn how to perform interaction models with lm() Learn how to extract marginal/partial effects with margins::margins() and predictive margins with ggeffects::::ggeffect() Learn how to vectorize multiple ifelse() statements with dplyr::case_when()    Measuring the effect of an information intervention on peace agreement support The country of Absurdistan has had an ongoing civil conflict for the past 60 years. The fighting between national government forces and guerrilla members from the National Revolutionary Army (NRA) has lead to more than 200,000 deaths, 8 million internally displaced persons, and countless victims between 1960 to 2020.\nThe civil war in Absurdistan has been a low-intensity asymmetric war. The legacies of the conflict have been bore largely by regions in the periphery of the country. Large cities and industrial regions have been spared from most of the fighting.\nThe government and the leadership of the NRA reached a peace agreement a couple of months ago; however, the agreement has been received poorly by the general population of Absurdistan. The opposition party the Undemocratic Center (UC) has allegedly ran campaigns misrepresenting the contents of the agreement in partisan media outlets and social media.\nThe Special Envoy for Peace has established a taskforce to design a strategy to increase support for the peace agreement. Many in the taskforce suspect that if the public were properly informed about the agreement reached, the levels of support would be higher.\nYou are hired as a scientific consultant for the taskforce. You run a survey experiment on a sample of 1000 respondents. You randomly assign respondents to watch an educational 2 minute video about the peace agreement.\nPackages set.seed(42) #for consistent results library(dplyr) # to wrangle our data library(tidyr) # to wrangle our data - pivot_longer() library(ggplot2) # to render our graphs library(readr) # for loading the .csv data library(janitor) # for data management and tabyl() library(kableExtra) # to render better formatted tables library(modelsummary) # to format your model output library(margins) #for calculating MARGINAL/PARTIAL EFFECT library(ggeffects) # easily calculating PREDICTIVE MARGINS   Exploratory analysis moderation_df \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/seramirezruiz/hertiestats2/master/data/moderation_df.csv\u0026quot;) # simulated data names(moderation_df) # to check the names of the variables in our data ## [1] \u0026quot;subject_id\u0026quot; \u0026quot;treatment\u0026quot; \u0026quot;victim_verbose\u0026quot; ## [4] \u0026quot;victim\u0026quot; \u0026quot;female\u0026quot; \u0026quot;female_verbose\u0026quot; ## [7] \u0026quot;support_thermometer\u0026quot; Our dataset moderation_df, contains the following information:\n subject_id: A unique numeric identification for each respondent treatment: A binary marker for treatment victim_verbose: A verbose binary marker of the respondents‚Äô victimhood status (Not a Victim-Victim) victim: A numeric binary marker of the respondents‚Äô victimhood status (0-1) female: A numeric binary marker of the respondents‚Äô sex (0-1) female_verbose: A verbose binary marker of the respondents‚Äô sex (Male-Female) support_thermometer: A continuous measure of support for the agreement (0-100)  Let‚Äôs explore who is in our sample We can use what we have learned about the janitor::tabyl() function, to check who was in our sample:\nmoderation_df %\u0026gt;% janitor::tabyl(treatment) %\u0026gt;% knitr::kable(col.names = c(\u0026quot;Treatment\u0026quot;, \u0026quot;N\u0026quot;, \u0026quot;Percent, %\u0026quot;)) %\u0026gt;% # create kable table kableExtra::kable_styling() # view kable table   Treatment  N  Percent, %      0  500  0.5    1  500  0.5     moderation_df %\u0026gt;% janitor::tabyl(treatment, victim_verbose) %\u0026gt;% janitor::adorn_totals(c(\u0026quot;row\u0026quot;, \u0026quot;col\u0026quot;)) %\u0026gt;% knitr::kable(col.names = c(\u0026quot;Treatment\u0026quot;, \u0026quot;Not a victim\u0026quot;, \u0026quot;Victim\u0026quot;, \u0026quot;Total\u0026quot;)) %\u0026gt;% # create kable table kableExtra::kable_styling() %\u0026gt;% # view kable table kableExtra::add_header_above(c(\u0026quot;\u0026quot;, \u0026quot;Victimization status\u0026quot; = 2, \u0026quot;\u0026quot;))    Victimization status      Treatment  Not a victim  Victim  Total      0  250  250  500    1  250  250  500    Total  500  500  1000      Let‚Äôs explore visually the observed levels of public support for the agreement ggplot(moderation_df, aes(x = support_thermometer)) + geom_density(fill = \u0026quot;#af8dc3\u0026quot;, alpha = 0.5) + theme_minimal() + labs(x = \u0026quot;Support thermometer\u0026quot;, y = \u0026quot;Density\u0026quot;) What do we see in this graph?\n Let‚Äôs explore visually the observed levels of public support for the agreement conditional on the treatment status ggplot(moderation_df, aes(x = support_thermometer, fill = factor(treatment))) + geom_density(alpha = 0.5) + theme_minimal() + labs(x = \u0026quot;Support thermometer\u0026quot;, y = \u0026quot;Density\u0026quot;, fill = \u0026quot;Treatment\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) + scale_fill_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;))  What do we see through these distributions?\n Let‚Äôs explore visually the observed levels of public support for the agreement conditional on the treatment and victimhood status ggplot(moderation_df, aes(x = support_thermometer, fill = factor(treatment))) + geom_density(alpha = 0.5) + facet_grid(treatment~victim_verbose) + theme_bw() + labs(x = \u0026quot;Support thermometer\u0026quot;, y = \u0026quot;Density\u0026quot;, fill = \u0026quot;Treatment\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) + scale_fill_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;))  What patterns do we see here?\n  Modeling and estimating During this week‚Äôs lecture, we learned that we can explicitly model heterogeneity in treatment effects for subgroups. Thus, we can address the tension between having to do inference at the group level, and the recognition of individual differences.\nThe analysis of heterogeneity can be very important for the design of our strategy. There are competing theories about the effects of conflict victimization on political behavior and attitudes. Some of the literature points to victims developing pro-social attitudes, while others suggest that victims become intransigent towards out-groups.\nCould factual information about the peace agreement be received differently by victims and non-victims?\nHow to estimate heterogenous treatment effects Heterogeneous treatment effects are usually estimated with regression models that include an interaction between the treatment and the moderator. In our case, the formula would look like this:\n\\[Y_{i} = Œ≤_0 + Œ≤_1D_{i} + Œ≤_2Victim_{i} + Œ≤_3D_i * Victim_{i}+ œµ_i\\]\n \\(Œ≤_0\\): Constant \\(Œ≤_1\\): Effect of \\(D_i\\) on \\(Y_i\\) if \\(Victim_i\\) is zero \\(Œ≤_2\\): Effect of \\(Victim_i\\) on \\(Y_i\\) if \\(D_i\\) is zero \\(Œ≤_3\\): Difference in treatment effects of \\(D_i\\) depending on \\(Victim_i\\)  or in lm() terms (same result):\nlm(outcome ~ treatment + moderator + (treatment*moderator)) lm(outcome ~ treatment + moderator + treatment:moderator) lm(outcome ~ treatment * moderator) Think of the switch logic posed by Prof.¬†Munzert\n Let‚Äôs model our results We will move forward by creating two models:\n A naive model, where we will regress support_thermometer on treatment. An interaction model, where we will add an interaction between treatment and victim   Naive modeling naive_model \u0026lt;- lm(support_thermometer ~ treatment, data = moderation_df) modelsummary::modelsummary(naive_model, fmt = 2, gof_omit = \u0026quot;AIC|BIC|Log.Lik.\u0026quot;, statistic = \u0026quot;conf.int\u0026quot;, stars = T)    Model 1      (Intercept)  35.05***     [33.63, 36.48]    treatment  18.90***     [16.88, 20.91]    Num.Obs.  1000    R2  0.254    R2 Adj.  0.253    F  339.360       + p \u0026lt; 0.1, * p \u0026lt; 0.05, ** p \u0026lt; 0.01, *** p \u0026lt; 0.001     What does this model tell us?\nThe naive model suggests that the subjects that the support for the peace agreement is about 18.9 percentage points higher on average for the subjects that watched the educational video. We suspect, however, that the video may affect differently victims from non-victims of the conflict.\nHere is a visual illustration of the values rendered by the naive regression ggplot(moderation_df, aes(x = support_thermometer, fill = factor(treatment))) + geom_density(alpha = 0.5) + theme_minimal() + geom_vline(xintercept = 35.055, linetype = \u0026quot;longdash\u0026quot;, color = \u0026quot;#a7a8aa\u0026quot;) + #D=0 (just beta0) geom_vline(xintercept = 35.055 + 18.898, linetype = \u0026quot;longdash\u0026quot;, color = \u0026quot;#cc0055\u0026quot;) + #D=1 (beta0+beta1) + geom_text(aes(label = \u0026quot;√ü0\u0026quot;, x = 35.055 + 3, y = 0.04), color = \u0026quot;#a7a8aa\u0026quot;) + # we add the 3 to repel from the line geom_text(aes(label = \u0026quot;√ü0 + √ü1\u0026quot;, x = 35.055 + 18.898 + 6, y = 0.04 ), color = \u0026quot;#cc0055\u0026quot;) + # we add the 6 to repel from the line labs(x = \u0026quot;Support thermometer\u0026quot;, y = \u0026quot;Density\u0026quot;, fill = \u0026quot;Treatment\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) + scale_fill_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;))    Interaction model Following any of the different formats renders the same results.\ninteraction_model \u0026lt;- lm(support_thermometer ~ treatment + victim + (treatment*victim), data = moderation_df) interaction_model_2 \u0026lt;- lm(support_thermometer ~ treatment + victim + treatment:victim, data = moderation_df) interaction_model_3 \u0026lt;- lm(support_thermometer ~ treatment*victim, data = moderation_df) modelsummary::modelsummary(list(interaction_model, interaction_model_2, interaction_model_3), fmt = 2, gof_omit = \u0026quot;AIC|BIC|Log.Lik.\u0026quot;, statistic = \u0026quot;conf.int\u0026quot;, stars = T)    Model 1  Model 2  Model 3      (Intercept)  27.93***  27.93***  27.93***     [26.85, 29.00]  [26.85, 29.00]  [26.85, 29.00]    treatment  8.01***  8.01***  8.01***     [6.49, 9.53]  [6.49, 9.53]  [6.49, 9.53]    victim  14.26***  14.26***  14.26***     [12.73, 15.78]  [12.73, 15.78]  [12.73, 15.78]    treatment √ó victim  21.77***  21.77***  21.77***     [19.62, 23.92]  [19.62, 23.92]  [19.62, 23.92]    Num.Obs.  1000  1000  1000    R2  0.787  0.787  0.787    R2 Adj.  0.786  0.786  0.786    F  1227.193  1227.193  1227.193       + p \u0026lt; 0.1, * p \u0026lt; 0.05, ** p \u0026lt; 0.01, *** p \u0026lt; 0.001     What does this model tell us?\n \\(Œ≤_0\\): Constant = The average support for non-victims who were not exposed to the video was 27.92 \\(Œ≤_1\\): Effect of \\(D_i\\) on \\(Y_i\\) if \\(Victim_i\\) is zero = The educational video results in an increase of about 8 percentage points of the peace agreement for the non-victims \\(Œ≤_2\\): Effect of \\(Victim_i\\) on \\(Y_i\\) if \\(D_i\\) is zero = On average, victims‚Äô support for the peace agreement is 14.3 percentage points higher than that of the non-victims in the control group \\(Œ≤_3\\): Difference in treatment effects of \\(D_i\\) depending on \\(Victim_i\\) = The educational video results in an additional increase for victims of about 21.8 percentage points, compared to the effect for non-victims  Here is a visual illustration of the values rendered by the model with interaction terms moderation_df %\u0026gt;% dplyr::group_by(treatment,victim_verbose) %\u0026gt;% dplyr::mutate(avg_support = mean(support_thermometer)) %\u0026gt;% ggplot(., aes(x = support_thermometer, fill = factor(treatment))) + geom_density(alpha = 0.5) + geom_vline(aes(xintercept = avg_support), linetype = \u0026quot;longdash\u0026quot;) + facet_grid(treatment~victim_verbose) + theme_bw() + labs(x = \u0026quot;Support thermometer\u0026quot;, y = \u0026quot;Density\u0026quot;, fill = \u0026quot;Treatment\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) + scale_fill_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;))    Extracting marginal/partial effects from our interaction models For this portion, we are interested in marginal/partial effects, which we will extract through the margins() function from the margins package.\nSome packages in R aimed at rendering marginal effects do render the predictive margins instead. For the purposes of the class, when asked to render marginal or partial effects you are expected to render them as introduced in the lecture (i.e., \\(\\frac{\\partial Y_i}{\\partial {D}_i}\\)). When asked for this, you will utilize margins::margins().\nYou can check the documentation here. The syntax for the margins() function for extracting partial effects of the treatment at different levels of the moderator is the following:\nmargins::margins(name_of_your_model, variables = \u0026quot;treatment_var\u0026quot;, at = list(moderator_variable = c(\u0026quot;value1\u0026quot;, \u0026quot;value2\u0026quot;, \u0026quot;value3\u0026quot;)) Let‚Äôs say we are interested in the marginal/partial effect of our video treatment for victims and non-victims. We would do:\nsummary(margins::margins(interaction_model, variables = \u0026quot;treatment\u0026quot;, at = list(victim = c(0,1)))) ## factor victim AME SE z p lower upper ## treatment 0.0000 8.0125 0.7757 10.3288 0.0000 6.4921 9.5330 ## treatment 1.0000 29.7831 0.7758 38.3895 0.0000 28.2626 31.3037 Let‚Äôs plot the marginal/partial effect of the treatment for victims and non-victims pe_margins \u0026lt;- margins::margins(interaction_model, variables = \u0026quot;treatment\u0026quot;, at = list(victim = c(0,1))) pe_plotting \u0026lt;- summary(pe_margins) %\u0026gt;% #NOTE we use the summary output, instead of the margins object dplyr::select(victim, AME, lower, upper) %\u0026gt;% # you will need to adapt this based your moderator dplyr::mutate(victim_labels = ifelse(victim == 1, \u0026quot;Victim\u0026quot;, \u0026quot;No Victim\u0026quot;)) # you may not need this to create labels for the assignment ggplot(pe_plotting, aes(x = victim_labels, y = AME)) + geom_point(size = 1.5) + geom_segment(aes(x = victim_labels, xend = victim_labels, y = lower, yend = upper), size = 0.5) + # render whiskers from confidence intervals theme_minimal() + scale_y_continuous(limits = c(0,45)) + # you may need to change the limits for your plots based on the specific effects of your application labs(x = \u0026quot;Respondent status\\n\u0026quot;, y = \u0026quot;\\nPartial effect of educational video\u0026quot;, caption = \u0026quot;Note: You can utilize it to describe what the plot illustrates during your assignment.\u0026quot;) + coord_flip() # to flip the plot    Extracting predictive margins from our interaction models For this portion, we are interested in predictive margins. In here, our interest is to return the expectation for each level of a predictor. We will extract this through the ggeffects() function from the ggeffects package. You can check the documentation here. The syntax is the following:\nggeffects::ggeffect(name_of_your_model, terms = c(\u0026quot;termA\u0026quot;, \u0026quot;termB\u0026quot;)) Let‚Äôs say we are interested in the predictive margins for all out victim and treatment permutations. We would do:\nggeffects::ggeffect(interaction_model, terms = c(\u0026quot;victim\u0026quot;,\u0026quot;treatment\u0026quot;)) ## # Predicted values of support_thermometer ## # x = victim ## ## # treatment = 0 ## ## x | Predicted | 95% CI ## ------------------------------ ## 0 | 27.93 | [26.85, 29.00] ## 1 | 42.18 | [41.11, 43.26] ## ## # treatment = 1 ## ## x | Predicted | 95% CI ## ------------------------------ ## 0 | 35.94 | [34.86, 37.02] ## 1 | 71.97 | [70.89, 73.04] Let‚Äôs plot these predictive margins In this exercise we will meet a very important function from dplyr, dplyr::case_when(). This function allows us to vectorize multiple ifelse() statements. The syntax is the following:\ndplyr::case_when(condition ~ what to do if met) Let‚Äôs see it at play.\nextracted_me \u0026lt;- ggeffects::ggeffect(interaction_model, terms = c(\u0026quot;victim\u0026quot;,\u0026quot;treatment\u0026quot;)) %\u0026gt;% dplyr::mutate(group_labels = dplyr::case_when(x == 1 \u0026amp; group == 1 ~ \u0026quot;Victim (1) - Treatment (1)\u0026quot;, x == 1 \u0026amp; group == 0 ~ \u0026quot;Victim (1) - Treatment (0)\u0026quot;, x == 0 \u0026amp; group == 1 ~ \u0026quot;Victim (0) - Treatment (1)\u0026quot;, x == 0 \u0026amp; group == 0 ~ \u0026quot;Victim (0) - Treatment (0)\u0026quot; )) # adding labels to (x,group) combinations for the plot extracted_me ggplot(extracted_me, aes(x = group_labels, y= predicted, fill = x, alpha = group )) + geom_bar(stat = \u0026quot;identity\u0026quot;) + geom_point(size = 1.5) + geom_segment(aes(x = group_labels, xend = group_labels, y = conf.high, yend = conf.low), size = 0.5) + # render whiskers from confidence intervals theme_minimal() + labs(x = \u0026quot;\\nRespondent status\u0026quot;, y = \u0026quot;Predictive margins\u0026quot;, fill = \u0026quot;Treatment\u0026quot;, caption = \u0026quot;Note: You can utilize it to describe what the plot illustrates during your assignment.\u0026quot;) + theme(legend.position = \u0026quot;note\u0026quot;) + scale_alpha_manual(values=c(0.6, 1))    Drafting some brief recommedations After conducting your experiment, you report back to the taskforce. Based on your results, you suggest that the educational videos may be a useful tool to encourage the wider public to hold a warmer opinion about the peace agreement. You also tell the taskforce that, although the videos have an average positive effect, they affect with a higher intensity victims of the conflict. You suggest to develop alternative strategies to tackle the non-victims, so that they do not fall through the cracks.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"61437d2f71abf02eea85caea6d9a399d","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-10/10-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-10/10-online-tutorial/","section":"materials","summary":"Welcome Introduction! Welcome to our tenth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you were introduced to Moderation and Heterogeneous Effects.","tags":null,"title":"Moderation and Heterogeneous Effects","type":"book"},{"authors":null,"categories":null,"content":"   Introduction! Welcome to our eleventh tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nIn this lab session we will develop our own simulation to understand the concept of power analysis a bit better.\nWe will also learn to apply for-loops and functions!\nThis session is heavily based on material provided by Nick Huntington-Klein. Check out his fantastic materials under https://nickch-k.github.io/EconometricsSlides/\nPackages # These are the libraries we will use today. Make sure to install them in your console in case you have not done so previously. library(tidyverse) library(broom) set.seed(42) # for consistent results   Statistical Power Statistics is an area where the lessons of violent video games are more or less true: if you want to solve a really tough problem, you need to bring a whole lot of firepower.\nOnce we have our study design down, there are a number of things that can turn statistical analysis into a fairly weak tool and make us less likely to find the truth:\nReally tiny effects are really hard to find Most statistical analyses are about looking at variation. If there‚Äôs little variation in the data, we won‚Äôt have much to go on If there‚Äôs a lot of stuff going on other than the effect we‚Äôre looking for, it will be hard to pull the signal from the noise If we have really high standards for what counts as evidence, then a lot of good evidence is going to be ignored so we need more evidence to make up for it  Conveniently, all of these problems can be solved by increasing our firepower, by which we mean sample size. Power analysis is our way of figuring out exactly how much firepower we need to bring. If it‚Äôs more than we‚Äôre willing to provide, we might want to turn around and go back home.\nWhat Power Analysis Does Using X as shorthand for the treatment and Y as shorthand for the outcome, assuming we‚Äôre doing a power analysis for the a study of the relationship between X and Y, power analysis balances five things:\nThe size of the effect (e.g.¬†coefficient in a regression) The amount of variation in the treatment (the variance of X, say) The amount of other variation in Y (the R2, or the variation from the residual after explaining Y with X) Statistical precision (the standard error of the estimate, statistical power, i.e.¬†the true-positive rate) The sample size  A power analysis holds four of these constant and tells you what the fifth can be. So, for example, it might say ‚Äúif we think the effect is probably A, and there‚Äôs B variation in X, and there‚Äôs C variation in Y unrelated to X, and you want to have at least a D% chance of finding an effect if it‚Äôs really there, then you‚Äôll need a sample size of at least E.‚Äù This tells us the minimum sample size necessary to get sufficient statistical power.\nOr we can go in other directions. ‚ÄúIf you‚Äôre going to have a sample size of A, and there‚Äôs B variation in X, and there‚Äôs C variation in Y unrelated to X, and you want to have at least a D% chance of finding an effect if it‚Äôs really there, then the effect must be at least as large as E.‚Äù This tells us the minimum detectable effect, i.e.¬†the smallest effect you can hope to have a chance of reasonably measuring given your sample size.\nHow about that ‚Äústatistical precision‚Äù option? Usually, you have a target level of statistical power (thus the name ‚Äúpower analysis‚Äù). Statistical power is the true-positive rate. That is, if there‚Äôs truly an effect there, and sampling variation means that you have an 80% chance of rejecting the null of no-effect in a given sample, then you have 80% statistical power. Statistical power is dependent on the kind of test you‚Äôre running, too - if you are doing a hypothesis test at the 95% confidence level, you‚Äôre more likely to reject the null (and thus will have higher statistical power) than if you‚Äôre doing a hypothesis test at the 99% confidence level. The more careful you are about false positives, the more likely you are to get a false negative. So there‚Äôs a tradeoff there.\nIn order to do power analysis, you need to be able to fill in the values for four of those five pieces, so that power analysis can tell you the fifth one. How do we know those things? In practical terms, power analysis isn‚Äôt a homework assignment, it‚Äôs guidance. It doesn‚Äôt need to be exact. A little guesswork (although ideally as little as possible) is necessary. After all, even getting the minimum sample size necessary doesn‚Äôt guarantee your analysis is good, it just gives you a pretty good chance of finding a result if it‚Äôs there. Often, people take the results of their power analysis as a baseline and then make sure to overshoot the mark, under the assumption they‚Äôve been too conservative. So don‚Äôt worry about being accurate, just try to make the numbers in the analysis close enough to be useful.\n  Simulation Example If the analysis you‚Äôre planning to do is a standard one, you can use one of many, many available ‚Äúpower analysis calculators‚Äù to do the work for you (for example, see http://powerandsamplesize.com/).\nHowever, running simulations in this context is a great way to i) understand power analysis and ii) apply your R skills!\nStep 1: Make Up Data With The Properties We Want Lets have a look at two useful functions to implement simulations:\nrnorm() takes three arguments: the sample size, the mean, and the standard deviation. rnorm(100, 0, 3) produces 100 random normal draws from a normal distribution with a mean of 0 and a standard deviation of 3.\nsample() takes a bunch of arguments, but the important ones are the set to sample from, the sample size, replace, and prob. sample(c('Treated','Untreated'), 500, replace = TRUE, prob = c(.2,.8)) produces 500 random observations that are either ‚ÄòTreated‚Äô or ‚ÄòUntreated‚Äô. Since prob = c(.2,.8), there‚Äôs a 20% chance of each draw being the first one (‚ÄòTreated‚Äô) and an 80% chance it‚Äôs the second (‚ÄòUntreated‚Äô). You pretty much always want replace = TRUE since that says that you can get the same thing more than once (i.e.¬†more than one ‚ÄòTreated‚Äô observation).\nYou can then design whatever sort of data you like - the data generating process is in your hands! You can use variables generated using random data to create other variables as well - now you have a causal link!\nOur setting:\nIn this case, imagine you have been tasked with analyzing an experiment run by a big multinational firm. A random subset of managers (treatment group) is supposed to receive a workshop on the climate change related consequences of flying and potential ways of replacing in-person meetings with international clients with online alternatives. The outcome of interest is the amount of kilometers flown to and from business meetings.\nLet‚Äôs construct a simulated dataset reflecting the true relationship:\n# Make a tibble (or data.frame) to contain our data tib \u0026lt;- tibble::tibble( # Start by creating the variables that don\u0026#39;t depend on anything else # Around 20% of managers are supposed to take the workshop (1), and the remaining 80% will be the control group (0) #You will receive data on 1000 managers workshop = sample(c(1,0), 1000, replace = TRUE, prob = c(.2,.8)) ) %\u0026gt;% # Then mutate in any variables that depend on earlier variables # Don\u0026#39;t forget to add additional noise! # The *true effect* of workshop on kms_flown is -90km dplyr::mutate(kms_flown = -90*workshop + rnorm(1000, mean = 4000, sd = 1000)) Just from this construction we have set:\nthe true effect of X on Y (i.e, a reduction of 90km flown), the amount of variation in X (by the characteristics of the binomial distribution), the amount of variation in Y not coming from X (it‚Äôs a normal distribution with a mean of 4000km and standard deviation of 1000km), the sample size (1000 managers).  Let‚Äôs see what we simulated  ggplot(tib, aes(x=kms_flown, fill=factor(workshop))) + geom_density(alpha = 0.5) + scale_fill_manual(name = \u0026quot;Workshop\u0026quot;, values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;)) + labs(x = \u0026quot;Distance flown by manager (kms)\u0026quot;, y = \u0026quot;Density\u0026quot;) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) knitr::include_graphics(\u0026quot;https://user-images.githubusercontent.com/54796579/165579120-f28b782e-334f-4375-b536-60d84219cc12.png\u0026quot;)  Step 2: Perform the Analysis Given that we are planning an experiment, we will regress Y on X to get the causal effect of the workshop on kms flown.\nmodel\u0026lt;-lm(kms_flown ~ workshop, data = tib) modelsummary::modelsummary(model, fmt = 2, gof_omit = \u0026quot;AIC|BIC|Log.Lik.\u0026quot;, statistic = \u0026quot;conf.int\u0026quot;, stars = T)    Model 1      (Intercept)  3949.91***     [3880.71, 4019.10]    workshop  4.70     [-151.59, 160.98]    Num.Obs.  1000    R2  0.000    R2 Adj.  -0.001    F  0.003       + p \u0026lt; 0.1, * p \u0026lt; 0.05, ** p \u0026lt; 0.01, *** p \u0026lt; 0.001     tidy(model) ## # A tibble: 2 √ó 5 ## term estimate std.error statistic p.value ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 (Intercept) 3950. 35.3 112. 0 ## 2 workshop 4.70 79.6 0.0590 0.953 # Let\u0026#39;s extract the p-value on the coefficient of interest tidy(model)$p.value[2] ## [1] 0.9529955 # And if we\u0026#39;re just interested in significance, say at the 95% level... sig \u0026lt;- tidy(model)$p.value[2] \u0026lt;= .05 sig ## [1] FALSE With this particular sample of simulated data, our experimental design does not detect any significant effect of the treatment on the outcome. But‚Ä¶\n Step 3: Repeat! ‚Ä¶of course, this is only one generated data set. That doesn‚Äôt tell us much of anything about the sampling variation! So we need to do this all a bunch of times, maybe a few thousand, and see what we get.\nWhile an R purist would probably opt for doing this with one of the apply() functions, for simplicity we‚Äôre just going to use the good ol‚Äô for() loop.\nfor (iterator in range) { code } will run the code code a bunch of times, each time setting the iterator variable to a different value in range, until it‚Äôs tried all of them, like this:\nfor (i in 1:5) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 We‚Äôre going to take all of the steps we‚Äôve done so far and put them inside those curly braces {}. Then we‚Äôre going to repeat the whole process a bunch of times!\nfor (i in 1:2000) { # Have to re-create the data EVERY TIME or it will just be the same data over and over tib \u0026lt;- tibble::tibble( workshop = sample(c(1,0), 1000, replace = TRUE, prob = c(.2,.8)) ) %\u0026gt;% dplyr::mutate(kms_flown = -90*workshop + rnorm(1000, mean = 4000, sd = 1000)) # Run the analysis model \u0026lt;- lm(kms_flown ~ workshop, data = tib) # Get the results coef_on_X \u0026lt;- tidy(model)$estimate[2] print(coef_on_X) sig \u0026lt;- tidy(model)$p.value[2] \u0026lt;= .05 print(sig) } If we run this code, it will generate our data 2000 times (i in 1:2000) and run the analysis on each of those data sets. Then it will get the coefficient on X and whether it‚Äôs significant at the 95% level and print those to screen.\nOf course, this will just leave us with 2000 results printed to screen. Not that useful!\n Step 4: Store the Results So instead of printing to screen, we‚Äôre going to save all of the results so we can look at them afterwards.\nWe‚Äôll start by creating some blank vectors to store our data in, like results \u0026lt;- c(). Then, as we do the analysis over and over, instead of printing the results to screen, we can store them in results[i], which will put it in the \\(i^{th}\\) position of the vector, adding on new elements as i grows higher and higher with our for() loop.\ncoef_results \u0026lt;- c() sig_results \u0026lt;- c() for (i in 1:2000) { # Have to re-create the data EVERY TIME or it will just be the same data over and over tib \u0026lt;- tibble::tibble( workshop = sample(c(1,0), 1000, replace = TRUE, prob = c(.2,.8)) ) %\u0026gt;% dplyr::mutate(kms_flown = -90*workshop + rnorm(1000, mean = 4000, sd = 1000)) # Run the analysis model \u0026lt;- lm(kms_flown ~ workshop, data = tib) # Get the results coef_results[i] \u0026lt;- tidy(model)$estimate[2] sig_results[i] \u0026lt;- tidy(model)$p.value[2] \u0026lt;= .05 }  Step 5: Examine the Results Our true effect is -90km. Our estimate of statistical power is the proportion of the results that are significantly different from zero:\nmean(sig_results) ## [1] 0.1885 So we have statistical power of around 21%. Even though we know the true parameter is -90km, only in 21% of the cases is the coefficient estimate significant!\nWe might also want to check the distribution of the coefficient estimate directly with geom_density() in ggplot2 to make sure it looks appropriate and there aren‚Äôt weird outliers implying a super sensitive analysis.\nresults_tibble \u0026lt;- tibble::tibble(coef = coef_results, sig = sig_results) ggplot(results_tibble, aes(x = coef)) + geom_density() + geom_vline(xintercept = -90, linetype=\u0026quot;dotted\u0026quot;, color = \u0026quot;#cc0065\u0026quot;) + # Prettify! theme_minimal() + labs(x = \u0026#39;Coefficient\u0026#39;, y = \u0026#39;Density\u0026#39;) knitr::include_graphics(\u0026quot;https://user-images.githubusercontent.com/54796579/165579130-dd855e74-9019-4206-8ba3-809fa3884a23.png\u0026quot;) ggplot(results_tibble, aes(x = sig, fill = sig)) + geom_bar() + # Prettify! theme_minimal() + labs(x = \u0026#39;Coefficient\u0026#39;, y = \u0026#39;Count\u0026#39;) + scale_x_discrete(labels = c(\u0026#39;Insignificant\u0026#39;,\u0026#39;Significant\u0026#39;)) + scale_fill_manual(values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;)) + theme(legend.position = \u0026quot;none\u0026quot;) knitr::include_graphics(\u0026quot;https://user-images.githubusercontent.com/54796579/165579137-71f87545-ac16-4bd1-935c-de24ccbef59c.png\u0026quot;)   Change and Compare! The goal of power analysis isn‚Äôt usually to just take one set of data-generating characteristics and generate a single power estimate, it‚Äôs to do things like calculate the minimum detectable effect or smallest sample size for a given power level.\nHow can we do that here? By trying different values of effect size and sample size and seeing what we get.\nTo do this, we‚Äôre first going to take everything we‚Äôve done so far and put it inside a function. This function will be called my_power_function and take two arguments: ‚Äúeffect‚Äù (the effect of X on Y) and ‚Äúsample_size‚Äù. We will be able to later change this inputs when applying the function and compare the results! The function will return the share power of your estimates based on the two given inputs.\nmy_power_function \u0026lt;- function(effect, sample_size) { sig_results \u0026lt;- c() for (i in 1:500) { #Noticed that we only repeat 500 times in order to speed calculations # Have to re-create the data EVERY TIME or it will just be the same data over and over tib \u0026lt;- tibble::tibble( workshop = sample(c(1,0), sample_size, replace = TRUE, prob = c(.2,.8)) ) %\u0026gt;% dplyr::mutate(kms_flown = effect*workshop + rnorm(sample_size, mean = 4000, sd = 1000)) # Run the analysis model \u0026lt;- lm(kms_flown ~ workshop, data = tib) # Get the results sig_results[i] \u0026lt;- tidy(model)$p.value[2] \u0026lt;= .05 } sig_results %\u0026gt;% mean() %\u0026gt;% return() } Now we can just call the function, setting effect and sample_size to whatever we want, and get the power back! Let‚Äôs check it with the values we had before and make sure we‚Äôre in the same range:\nmy_power_function(-90, 1000) ## [1] 0.204 Seems good!\nNow let‚Äôs say we really are stuck with a sample size of 1000 and we want to know the minimum detectable effect size we can get a power of .8 with. To do this, we can just run our function with sample_size = 1000 and a bunch of different effect values until we get back a power above .8!\npower_levels \u0026lt;- c() effects_to_try \u0026lt;- c(-10, -90, -200, -250 ,-300, -400, -500) for (i in 1:length(effects_to_try)) { power_levels[i] \u0026lt;- my_power_function(effects_to_try[i], 1000) } # Where do we cross 80%? power_results \u0026lt;- tibble(effect = effects_to_try, power = power_levels) power_results ## # A tibble: 7 √ó 2 ## effect power ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -10 0.042 ## 2 -90 0.186 ## 3 -200 0.74 ## 4 -250 0.856 ## 5 -300 0.97 ## 6 -400 1 ## 7 -500 1 ggplot(power_results, aes(x = effect, y = power)) + geom_line(color = \u0026#39;#cc0065\u0026#39;, size = 1.5) + # add a horizontal line at 90% geom_hline(aes(yintercept = .8), linetype = \u0026#39;dashed\u0026#39;) + # Prettify! theme_minimal() + scale_y_continuous(labels = scales::percent) + labs(x = \u0026#39;Linear Effect Size\u0026#39;, y = \u0026#39;Power\u0026#39;) knitr::include_graphics(\u0026quot;https://user-images.githubusercontent.com/54796579/165579153-5cda2355-a8b3-4e98-8459-b231392ecd8f.png\u0026quot;) So it looks like we need an effect around ‚Äú-225‚Äù or greater to have an 80% chance of finding a significant result. If we don‚Äôt think the effect is actually likely to be that large, then we need to figure out something else to do - for example, get a bigger sample!\nImagine that you are convinced that the true effect will be a reduction of kms flown of 90km. What sample size would you need to detect this effect?\npower_levels \u0026lt;- c() sample_sizes_to_try \u0026lt;- c(1000, 3000, 5000, 7000) for (i in 1:length(sample_sizes_to_try)) { power_levels[i] \u0026lt;- my_power_function(-90, sample_sizes_to_try[i]) } # Where do we cross 80%? power_results \u0026lt;- tibble(sample_size = sample_sizes_to_try, power = power_levels) power_results ## # A tibble: 4 √ó 2 ## sample_size power ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1000 0.22 ## 2 3000 0.52 ## 3 5000 0.698 ## 4 7000 0.88 ggplot(power_results, aes(x = sample_size, y = power)) + geom_line(color = \u0026#39;red\u0026#39;, size = 1.5) + # add a horizontal line at 90% geom_hline(aes(yintercept = .8), linetype = \u0026#39;dashed\u0026#39;) + # Prettify! theme_minimal() + scale_y_continuous(labels = scales::percent) + labs(x = \u0026#39;Sample Size\u0026#39;, y = \u0026#39;Power\u0026#39;) knitr::include_graphics(\u0026quot;https://user-images.githubusercontent.com/54796579/165579162-b99c0193-7ca8-4502-85ad-86d163fcfda0.png\u0026quot;) You would need a sample size of around 7000 to have an 80% chance of finding a significant result!\nNow it‚Äôs your turn!\nTry and adapt the function so that you can vary other parameters that have an effect on power. For example, you could try to change the proportion of managers getting the treatment, the standard deviation of the noise affecting Y or the significance levels that you are willing to accept for your coefficient of interest.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"37b37ed5f47d8ff3d0adff893f6c4e21","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-11/11-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-11/11-online-tutorial/","section":"materials","summary":"Introduction! Welcome to our eleventh tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nIn this lab session we will develop our own simulation to understand the concept of power analysis a bit better.","tags":null,"title":"Power Analysis","type":"book"},{"authors":null,"categories":null,"content":"   Welcome Introduction! Welcome to our seventh tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you were introduced to Regression Discontinuity Designs (RDDs).\nIn this lab session we will:\n Leverage visualizations with ggplot2 to explore our discontinuity setups Learn how to model our discontinuity setups under different functional forms with lm() Learn how to model our discontinuity setups under different functional forms with rdrobust::rdrobust()  Packages # These are the libraries we will use today. Make sure to install them in your console in case you have not done so previously. library(dplyr) # for data wrangling library(ggplot2) # for creating plots library(rdrobust) # for rdrobust() library(readr) # for loading the .csv data set.seed(42) # for consistent results mlda_df \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/seramirezruiz/stats-ii-lab/master/Session%206/data/mlda.csv\u0026quot;) # loading data from Mastering Metrics    Example 1. Measuring the effect of the minimum legal drinking age (MLDA) on mortality  In an effort to address the social and public health problems associated with underage drinking, a group of American college presidents have lobbied states to return the minimum legal drinking age (MLDA) to the Vietnam era threshold of 18. The theory behind this effort (known as the Amethyst Initiative) is that legal drinking at age 18 discourages binge drinking and promotes a culture of mature alcohol consumption. This contrasts with the traditional view that the age-21 MLDA, while a blunt and imperfect tool, reduces youth access to alcohol, thereby preventing some harm. Angrist and Pischke (2014)\n You have been hired as an outside consultant by Mothers Against Drunk Driving (MADD) to study whether the over-21 drinking minimum in the US helps reduce alcohol consumption by young adults and its harms, or is it just not working. This example is based on data from Carpenter and Dobkin (2011).\nChecking visually whether a sharp-RDD makes sense for the analysis We want to know whether our threshold is in fact the cut-off for treatment. In this case, the law is pretty clear: young adults in the US can legally drink at age 21.\nggplot(mlda_df, aes(x = agecell, # actual age y = treatment, # are they over 21 or not color = factor(treatment))) + geom_point() + labs(x = \u0026quot;Age\u0026quot;, y = \u0026quot;Treatment Probability\u0026quot;) + scale_color_discrete(name = \u0026quot; \u0026quot;, labels = c(\u0026quot;Under legal drinking age\u0026quot;, \u0026quot;Over legal drinking age\u0026quot;)) + geom_vline(xintercept = 21, linetype = \u0026quot;dotted\u0026quot;) + # NEW GEOM A VERTICAL LINE! theme_minimal() We can see from the graph that at the 21-years-of-age threshold, young adults can legally consume and buy alcohol in the US, which would make age a viable forcing variable for a sharp-RDD set-up.\n Running our regression models As was pointed out in the lecture, we must decide on a model that we believe reflects the relationship of our \\(E(Y_i|\\tilde{X}_i)\\):\n linear, common slope linear, different slopes non-linear  Remember that each model corresponds to a particular set of assumptions\nWe will show you how to visualize this with ggplot.\nLET‚ÄôS LOOK AT A SCATTERPLOT TO GET AN IDEA OF WHAT WE ARE DEALING WITH\nggplot(mlda_df, aes(x = agecell, # age y = outcome)) + # mortality rate per 100k accidents geom_point() + geom_vline(xintercept = 21, linetype = \u0026quot;dotted\u0026quot;) + labs(title = \u0026quot;Exploratory plot\u0026quot;, x = \u0026quot;Forcing variable (Age)\u0026quot;, y = \u0026quot;Mortality rate from motor vehicle \\naccidents (per 100,000)\u0026quot;) + theme_minimal() NOTE that we have the variable forcing in this dataset, which is centered at the cutoff. It is nothing but the variable age-21\nLinear model with common slopes Let‚Äôs run a linear model with common slopes and plot our results.\nNOTE that the forcing variable in this case (age) is CENTERED at 0 (age 21) and is the distance from age 21 in years, while treatment is just binary over/under 21.\n# running linear model with common slope linear_common_slope \u0026lt;- lm(outcome ~ treatment + forcing, data = mlda_df) stargazer::stargazer(linear_common_slope, type = \u0026quot;text\u0026quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## outcome ## ----------------------------------------------- ## treatment 4.534*** ## (0.768) ## ## forcing -3.149*** ## (0.337) ## ## Constant 29.356*** ## (0.429) ## ## ----------------------------------------------- ## Observations 48 ## R2 0.703 ## Adjusted R2 0.689 ## Residual Std. Error 1.329 (df = 45) ## F Statistic 53.142*** (df = 2; 45) ## =============================================== ## Note: *p\u0026lt;0.1; **p\u0026lt;0.05; ***p\u0026lt;0.01 WHAT DO THESE RESULTS TELL US?\nIn line with our assumptions for linear models with common slope, we consider that treatment effect \\(D_i\\) does not depend on the forcing \\(X_i\\). We can formalize this model as: \\[E(Y_i|X_i,D_i) = \\tilde{\\beta_0} + \\beta_1 D_i + \\beta_2\\tilde{X}_i\\] Hence we can say, that given our \\(\\beta_1\\) we can expect 4.53 more deaths from motor vehicle accidents per 100,000 for those who can legally drink. We also see that for every year of age increase, the number of expected deaths per 100,000 decreases by 3.15. (Our \\(\\beta_2 = -3.1488\\)).\nWe can graph our results with ggplot() by extracting the predicted values of the model to recreate the linear fit:\nmlda_df$yhat_linear \u0026lt;- predict(linear_common_slope) # we create a new variable containing the predicted mortality rate linear_plot \u0026lt;- mlda_df %\u0026gt;% # for this plot make sure to put the df outside the ggplot() and pipe it ggplot(aes(x = forcing, y = yhat_linear, # notice here the predicted y col = factor(treatment))) + geom_point(aes(x = forcing, y = outcome, # notice here the actual outcome col = factor(treatment))) + geom_vline(xintercept = 0, linetype = \u0026quot;dotted\u0026quot;) + labs(title = \u0026quot;Linear model with common slope\u0026quot;, x = \u0026quot;Forcing variable (Age)\u0026quot;, y = \u0026quot;Mortality rate from motor vehicle \\naccidents (per 100,000)\u0026quot;) + geom_line(data = mlda_df[mlda_df$forcing \u0026gt;= 0,], color = \u0026quot;#cc0055\u0026quot;, # color lines size = 1) + geom_line(data = mlda_df[mlda_df$forcing \u0026lt; 0,], color = \u0026quot;#696969\u0026quot;, # color lines size = 1) + scale_color_manual(name = \u0026quot;\u0026quot;, values = c(\u0026quot;#696969\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;)) + #change colors manually of color argument in aes() theme_minimal() linear_plot  Linear model with different slopes Let‚Äôs run the linear model to gather the slopes for both groups and plot our results. This is achieved by interacting our treatment and forcing variables.\nlinear_different_slope \u0026lt;- lm(outcome ~ treatment*forcing, data = mlda_df) stargazer::stargazer(linear_different_slope, type = \u0026quot;text\u0026quot;) ## ## =============================================== ## Dependent variable: ## --------------------------- ## outcome ## ----------------------------------------------- ## treatment 4.534*** ## (0.751) ## ## forcing -2.568*** ## (0.466) ## ## treatment:forcing -1.162* ## (0.659) ## ## Constant 29.929*** ## (0.531) ## ## ----------------------------------------------- ## Observations 48 ## R2 0.722 ## Adjusted R2 0.703 ## Residual Std. Error 1.299 (df = 44) ## F Statistic 38.125*** (df = 3; 44) ## =============================================== ## Note: *p\u0026lt;0.1; **p\u0026lt;0.05; ***p\u0026lt;0.01 WHAT DO THESE RESULTS TELL US?\nIn line with our assumptions for linear models with different slope, we allow our treatment effect \\(D_i\\) to vary along the forcing \\(X_i\\). We can formalize this model as:\n\\[E(Y_i|X_i,D_i) = \\tilde{\\beta_0} + \\beta_1D_i+ \\beta_2X_i + \\tilde{\\beta_3}D_i\\tilde{X}_i\\]\nHence we can say, that at given our \\(\\beta_1\\), we can expect 4.53 more deaths from motor vehicle accidents per 100,000 for those who can legally drink at the threshold. Now we have two different slopes for year-of-age changes. For under-21 individuals, an increase of one year of age would on average result in 2.57 less deaths from motor vehicle accidents (our \\(\\beta_2 = -2.5676\\)). For those of legal drinking age, we would expect 3.73 less deaths per 100,000 for every one year of age increase (our \\(\\beta_2X_i + \\tilde{\\beta_3}D_i\\tilde{X}_i = -2.5676 + (-1.1624) = - 3.73\\)).\nWe can graph our results with ggplot by just adding a smooth geom. Since we have added treatment to our color aesthetic, ggplot() will automatically create the regression line for each group\ndiff_slopes_plot \u0026lt;- mlda_df %\u0026gt;% ggplot(aes(x = forcing, y = outcome, col = factor(treatment))) + geom_point() + geom_vline(xintercept = 0, linetype = \u0026quot;dotted\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + # NORMAL SMOOTH labs(title = \u0026quot;Linear model with different slopes\u0026quot;, x = \u0026quot;Forcing variable (Age)\u0026quot;, y = \u0026quot;Mortality rate from motor vehicle \\naccidents (per 100,000)\u0026quot;) + scale_color_manual(name = \u0026quot;\u0026quot;, values = c(\u0026quot;#696969\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;)) + #change colors manually of color argument in aes() theme_minimal() diff_slopes_plot Question\nWhere can you see our \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\) in the previous plot?\n Non-linear model Let‚Äôs run a quadratic model and plot our results.\nTHIS IS HOW WE WOULD FORMALIZE A QUADRATIC MODEL\n\\[E(Y_i‚à£X_i, D_i) = \\gamma_0 + + \\tau_1D_i + \\gamma_2\\tilde{X_i} + \\gamma_3\\tilde{X^2_i} + \\alpha_1\\tilde{X_i}D_i + \\alpha_2\\tilde{X^2_i}D_i\\]\nWe can input this in our regression model as follows:\nquadratic \u0026lt;- lm(outcome ~ forcing + I(forcing^2) + # I tells R to interpret \u0026quot;as is\u0026quot; treatment + I(forcing * treatment) + I((forcing^2) * treatment), data = mlda_df) stargazer::stargazer(quadratic, type = \u0026quot;text\u0026quot;) ## ## ===================================================== ## Dependent variable: ## --------------------------- ## outcome ## ----------------------------------------------------- ## forcing -2.933 ## (1.914) ## ## I(forcing2) -0.185 ## (0.940) ## ## treatment 4.663*** ## (1.155) ## ## I(forcing * treatment) -0.823 ## (2.706) ## ## I((forcing2) * treatment) 0.198 ## (1.329) ## ## Constant 29.809*** ## (0.817) ## ## ----------------------------------------------------- ## Observations 48 ## R2 0.722 ## Adjusted R2 0.689 ## Residual Std. Error 1.329 (df = 42) ## F Statistic 21.864*** (df = 5; 42) ## ===================================================== ## Note: *p\u0026lt;0.1; **p\u0026lt;0.05; ***p\u0026lt;0.01 WHAT DO THESE RESULTS TELL US?\nIn line with our assumptions for non-linear models, we allow our treatment effect \\(D_i\\) to vary along the forcing \\(X_i\\). In this case with quadratic interactions. We can formalize this model as:\n\\[E(Y_i‚à£X_i, D_i) = \\gamma_0 + + \\tau_1D_i + \\gamma_2\\tilde{X_i} + \\gamma_3\\tilde{X^2_i} + \\alpha_1\\tilde{X_i}D_i + \\alpha_2\\tilde{X^2_i}D_i\\]\nHence we can say, that at given our \\(\\tau\\), we can expect 4.66 more deaths from motor vehicle accidents per 100,000 for those who can legally drink at the threshold. We could also calculate the expected value of \\(Y\\) at different levels of \\(X\\).\nSay we want to know what the expected value is for 23-year-olds. Since our forcing variable is 0 at 21 years of age, we can think of 23 as 2. Additionally, 23-year-olds are above the legal drinking age minimum, therefore for them the value of \\(D\\) is 1.\n\\[E(Y | X=2, D =1) = 29.8090 + 4.6629(1) - 2.9330(2) -0.1852(2)^2 - 0.8231 (2*1) + 0.1985(2*1)^2 = 27.01\\] Based on this, we would expect a mortality rate from motor vehicle accidents of 27.01 per 100,000 for 23-year-olds.\nWe can graph our results with ggplot by extracting the predicted values of our quadratic model to recreate the fit:\nmlda_df$yhat_quadratic \u0026lt;- predict(quadratic) quadratic_plot \u0026lt;- mlda_df %\u0026gt;% ggplot(aes(x = forcing, y = yhat_quadratic, #note predicted y col = factor(treatment))) + geom_point(aes(x = forcing, y = outcome, col = factor(treatment))) + geom_vline(xintercept = 0, linetype = \u0026quot;dotted\u0026quot;) + labs(title = \u0026quot;Quadratic plot\u0026quot;, x = \u0026quot;Forcing variable (Age)\u0026quot;, y = \u0026quot;Mortality rate from motor vehicle \\naccidents (per 100,000)\u0026quot;) + geom_line(data = mlda_df[mlda_df$forcing \u0026gt;= 0,], color = \u0026quot;#cc0055\u0026quot;, # color lines size = 1) + geom_line(data = mlda_df[mlda_df$forcing \u0026lt; 0,], color = \u0026quot;#696969\u0026quot;, # color lines size = 1) + scale_color_manual(name = \u0026quot;\u0026quot;, values = c(\u0026quot;#696969\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;)) + #change colors manually of color argument in aes() theme_minimal() quadratic_plot   Calculating the LATE with rdrobust() rdrobust() is part of the rdrobust package. It performs local linear regressions to either side of the cutpoint using optimal bandwidth calculation. The syntax is the following:\nmodel \u0026lt;- rdrobust::rdrobust(x, y, c = cutoffvalue, kernel = \u0026quot;tri\u0026quot;, #default bwselect = \u0026quot;mserd\u0026quot;) #default  We have the option to set the cutpoint, kernel type, order of the local polynomial, etc.: https://cran.r-project.org/web/packages/rdrobust/rdrobust.pdf\nllr \u0026lt;- rdrobust::rdrobust(mlda_df$outcome, mlda_df$forcing, c = 0, kernel = \u0026quot;tri\u0026quot;, bwselect = \u0026quot;mserd\u0026quot;) summary(llr) ## Call: rdrobust ## ## Number of Obs. 48 ## BW type mserd ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 24 24 ## Eff. Number of Obs. 6 6 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.487 0.487 ## BW bias (b) 0.738 0.738 ## rho (h/b) 0.660 0.660 ## Unique Obs. 24 24 ## ## ============================================================================= ## Method Coef. Std. Err. z P\u0026gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 4.901 2.059 2.380 0.017 [0.864 , 8.937] ## Robust - - 1.881 0.060 [-0.198 , 9.674] ## ============================================================================= WHAT DO THESE RESULTS TELL US?\nThe model is telling us that based on the calculation, the estimated effect would be 4.90 more deaths per 100,000 for those over-21.\nThe most straight-forward way to graph the output of this model is through the rdrobust::rdplot() function:\nrdrobust::rdplot(mlda_df$outcome, mlda_df$forcing, c = 0, kernel = \u0026quot;tri\u0026quot;, title = \u0026quot;Motor Vehicle Accidents Death\u0026quot;, x.label = \u0026quot;Age from 21\u0026quot;, y.label = \u0026quot;Mortality rate from motor vehicle \\naccidents (per 100,000)\u0026quot; )  Quadratic model with rdrobust() quadratic_rdrobust \u0026lt;- rdrobust::rdrobust(mlda_df$outcome, mlda_df$forcing, c = 0, kernel = \u0026quot;tri\u0026quot;, bwselect = \u0026quot;mserd\u0026quot;, p = 2) #polynomial 2 summary(quadratic_rdrobust) ## Call: rdrobust ## ## Number of Obs. 48 ## BW type mserd ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 24 24 ## Eff. Number of Obs. 10 10 ## Order est. (p) 2 2 ## Order bias (q) 3 3 ## BW est. (h) 0.821 0.821 ## BW bias (b) 1.074 1.074 ## rho (h/b) 0.764 0.764 ## Unique Obs. 24 24 ## ## ============================================================================= ## Method Coef. Std. Err. z P\u0026gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 4.778 2.337 2.044 0.041 [0.197 , 9.360] ## Robust - - 1.627 0.104 [-0.911 , 9.811] ## ============================================================================= rdrobust::rdplot(mlda_df$outcome, mlda_df$forcing, c = 0, kernel = \u0026quot;tri\u0026quot;, p = 2, title = \u0026quot;Motor Vehicle Accidents Death\u0026quot;, x.label = \u0026quot;Age from 21\u0026quot;, y.label = \u0026quot;Mortality rate from motor vehicle \\naccidents (per 100,000)\u0026quot; )    Example 2. Measuring the long term effects of a conditional cash transfer program on educational achievement Imagine that you work as a technical adviser for the Ministry of Education in your country. You are tasked to assess whether a Conditional Cash Transfer (CCT) program established decades before yields positive results on the beneficiaries‚Äô educational attainment. There is a large amount of evidence which suggests that CCTs encourage households to increase the use of educational services.\nYou read the guidelines for the program. Families receive a stipend per child provided they keep their them in school and take them for health checks. Additionally, you note that under the rules of the program, beneficiaries are selected based on a household income threshold of ‚Ç¨20000. You decide to dive into the data with the idea that a discontinuity is created based on the income threshold. (This example utilizes simulated data)\ncct_df \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/seramirezruiz/stats-ii-lab/master/Session%206/data/cct_data.csv\u0026quot;) # loading simulated data frame of the program The dataset consists of:\n hh_income: household income in euros years_of_schooling: years of schooling for respondent treatment: binary variable indicating whether respondent was a beneficiary of the program  Checking visually whether a sharp-RDD makes sense for the analysis What we are looking for in this case is whether our ‚Ç¨20000 threshold is in fact the cut-off for treatment. That is to say, that only those who had a household income of equal or less than ‚Ç¨20000 received the cash transfer.\nggplot(cct_df, aes(x = hh_income, y = years_of_schooling, color = factor(treatment))) + geom_point() + labs(x = \u0026quot;Household Income\u0026quot;, y = \u0026quot;Years of Schooling\u0026quot;) + scale_color_discrete(name = \u0026quot; \u0026quot;, labels = c(\u0026quot;No treatment\u0026quot;, \u0026quot;Treatment\u0026quot;)) + geom_vline(xintercept = 20000, linetype = \u0026quot;dotted\u0026quot;) + theme_minimal() ggplot(cct_df, aes(x = hh_income, y = treatment, color = factor(treatment))) + geom_point() + labs(x = \u0026quot;Household Income\u0026quot;, y = \u0026quot;Treatment\u0026quot;) + scale_color_discrete(name = \u0026quot; \u0026quot;, labels = c(\u0026quot;No treatment\u0026quot;, \u0026quot;Treatment\u0026quot;)) + geom_vline(xintercept = 20000, linetype = \u0026quot;dotted\u0026quot;) + theme_minimal() We can see from the graph that our ‚Ç¨20000 threshold is in fact cutting off the distribution of the treatment. This would make household income a viable forcing variable for a sharp-RDD set-up.\n Estimating our model We can see that the relationship is fairly linear, so we decide to run a linear model with common slope.\n# running linear model with common slope ed_achievement \u0026lt;- lm(years_of_schooling ~ treatment + hh_income, data = cct_df) stargazer::stargazer(ed_achievement, type = \u0026quot;text\u0026quot;) ## ## ================================================ ## Dependent variable: ## ---------------------------- ## years_of_schooling ## ------------------------------------------------ ## treatment 2.460*** ## (0.038) ## ## hh_income 0.001*** ## (0.00000) ## ## Constant -2.648*** ## (0.111) ## ## ------------------------------------------------ ## Observations 5,000 ## R2 0.815 ## Adjusted R2 0.815 ## Residual Std. Error 0.817 (df = 4997) ## F Statistic 11,008.950*** (df = 2; 4997) ## ================================================ ## Note: *p\u0026lt;0.1; **p\u0026lt;0.05; ***p\u0026lt;0.01 WHAT DO THESE RESULTS TELL US?\nIn line with our assumptions for linear models with common slope, we consider that treatment effect, \\(D_i\\), does not depend on the forcing \\(X_i\\). Hence, we can expect that students who received the treatment get on average 2.4 more years of schooling. We also see that for every ‚Ç¨1,000 increase in the household income, students are expected to attain 0.6274 more years of education. (Our \\(\\beta = -6.274e-04*1000\\)).\nGetting familiar with LOESS\nLocally weighted smoothing is a popular tool used in regression analysis that creates a smooth line through a scatter plot to help you to see relationship between variables and foresee trends. We can introduce it to our ggplot() as a part of geom_smooth by calling method ‚Äúloess‚Äù.\nggplot(cct_df, aes(x = hh_income, y = years_of_schooling, color = factor(treatment))) + geom_point(alpha = 0.1) + labs(x = \u0026quot;Household Income\u0026quot;, y = \u0026quot;Years of Schooling\u0026quot;) + geom_smooth(method = \u0026quot;loess\u0026quot;) + # instead of lm, we use loess. See the difference? try with lm scale_color_discrete(name = \u0026quot; \u0026quot;, labels = c(\u0026quot;No treatment\u0026quot;, \u0026quot;Treatment\u0026quot;)) + geom_vline(xintercept = 20000, linetype = \u0026quot;dotted\u0026quot;) + theme_minimal() The LOESS smoothing is not very visible in this relationship because of the way we defined the simulated data. Let‚Äôs look at how it would look in our drinking age example:\nggplot(mlda_df, aes(x = agecell, y = outcome, col = factor(treatment))) + geom_point() + geom_smooth(method = \u0026quot;loess\u0026quot;) + labs(title = \u0026quot;LOESS smoothing\u0026quot;, x = \u0026quot;Forcing variable (Age)\u0026quot;, y = \u0026quot;Mortality rate from motor vehicle \\naccidents (per 100,000)\u0026quot;) + scale_color_manual(name = \u0026quot;\u0026quot;, values = c(\u0026quot;#F8766D\u0026quot;, \u0026quot;#00BFC4\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;)) + theme_minimal() Violations to the assumptions You are made aware by a tax expert from your unit that ‚Ç¨20000 is the upper-boundary for a very well known tax concession. You are afraid that people may be sorting themselves before the household income cut-off to become beneficiaries of multiple programs. You decide to check your data.\nggplot(cct_df, aes(x = hh_income)) + geom_histogram(bins = 50, fill = \u0026quot;#cc0055\u0026quot;) + labs(title = \u0026quot;Income distribution\u0026quot;, x = \u0026quot;Household Income\u0026quot;, y = \u0026quot;Number of respondents\u0026quot;) + geom_vline(xintercept = 20000, linetype = \u0026quot;dotted\u0026quot;) + theme_minimal() This case looks a bit ambiguous. Do you think people are sorting out just before the cut-off? If sorting were to exist which assumptions would be challenged? Would the existence of other programs that have the same threshold affect a causal reading of our results?\nThere are a couple of tests researchers can employ. We will learn two ways. First, a method by which the research chooses a window of sorting to check if the distribution could have occurred by chance and second the McCrary test you met in Cunningham (2021).\nBinomial test  When we apply an exact binomial test. Our interest is to see whether the distribution around the threshold could exist by chance. In this case, let‚Äôs check ¬±500 and ¬±250 around the threshold.\nTo gather only the units that reported household incomes from ‚Ç¨19500 to ‚Ç¨20500, we will use a new function dplyr::between().\ndplyr::between() is a shortcut for x \u0026gt;= left \u0026amp; x \u0026lt;= right. Let‚Äôs look at it at work.\ncct_df %\u0026gt;% dplyr::filter(dplyr::between(hh_income, 19500, 20500)) %\u0026gt;% #filter values between 19500 and 20500 dplyr::group_by(treatment) %\u0026gt;% dplyr::summarize(n = n()) %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling()   treatment  n      0  255    1  300     We have 300 units just below the threshold and 255 units just above. We can use this information to run our exact binomial test. We seek to understand if the observed distributions deviate from expected distribution of observations into the two categories.\nWe can do the same for ‚Ç¨19750 to ‚Ç¨20250\ncct_df %\u0026gt;% dplyr::filter(dplyr::between(hh_income, 19750, 20250)) %\u0026gt;% #filter values between 19750 and 20250 dplyr::group_by(treatment) %\u0026gt;% dplyr::summarize(n = n()) %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling()   treatment  n      0  115    1  175     binom.test(one_of_the_n, total_n, p = probability_of_success) Let‚Äôs see what this would say for ¬±500:\nbinom.test(300, 555, p = 0.5)  ## ## Exact binomial test ## ## data: 300 and 555 ## number of successes = 300, number of trials = 555, p-value = 0.06171 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.4980565 0.5825909 ## sample estimates: ## probability of success ## 0.5405405 WHAT DO THESE RESULTS TELL US?\nAccording to the test, we see that the observed distributions do not deviate from expected distribution of observations into the two categories when we expect that units just around the threshold end up on either group by chance (coin flip logic, i.e., p = 0.5). In other words, this results do not present some evidence of sorting in this window.\nHow about ¬±250?:\nbinom.test(115, 290, p = 0.5)  ## ## Exact binomial test ## ## data: 115 and 290 ## number of successes = 115, number of trials = 290, p-value = 0.0005095 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.3398404 0.4553997 ## sample estimates: ## probability of success ## 0.3965517 WHAT DO THESE RESULTS TELL US?\nAccording to the test, we see that the observed distributions deviate from expected distribution of observations into the two categories when we expect that units just around the threshold end up on either group by chance (coin flip logic, i.e., p = 0.5). In other words, this results present some evidence of sorting in this window.\nMcCrary Sorting test  An alternative way to check for self-sorting is the McCrary Sorting test. In this test, the discretion on window selection is taken away from the researcher (at least in the defaults). The McCrary Sorting test is included in the rdd package. This is the syntax of the test:\nrdd::Dcdensity(runvar=running_variable, cutpoint=cutpoint) Let‚Äôs see it in practice:\nrdd::DCdensity(cct_df$hh_income, cutpoint = 20000) ## [1] 0.04168422 WHAT DO THESE RESULTS TELL US?\nThe default output is a p-value of the test. A p-value below the significance threshold indicates that the user can reject the null hypothesis of no sorting. In other words, this test would suggest that our observed distributions deviate from the expected distribution of observations. This results present some evidence of sorting.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"67815aa18439ce8dd0a31b641738b719","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-7/07-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-7/07-online-tutorial/","section":"materials","summary":"Welcome Introduction! Welcome to our seventh tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you were introduced to Regression Discontinuity Designs (RDDs).","tags":null,"title":"Regression Discontinuity Designs","type":"book"},{"authors":null,"categories":null,"content":" Welcome Introduction! Welcome to our fourth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week's lecture you reviewed bivariate and multiple linear regressions. You also learned how Directed Acyclic Graphs (DAGs) can be leveraged to gather causal estimates.\nIn this lab session we will:\n Use the ggdag and dagitty packages to assess your modeling strategy Review how to run regression models using R Illustrate omitted variable and collider bias  Packages # These are the libraries we will use today. Make sure to install them in your console in case you have not done so previously. set.seed(42) #for consistent results in randomization library(wooldridge) # To get our example\u0026#39;s dataset library(tidyverse) # To use dplyr functions and the pipe operator when needed library(ggplot2) # To visualize data (this package is also loaded by library(tidyverse)) library(ggdag) # To dagify and plot our DAG objects in R library(dagitty) # To perform analysis in our DAG objects in R library(stargazer) # To render nicer regression output data(\u0026quot;wage1\u0026quot;) # calls the wage1 dataset from the woorldridge package    Working with DAGs in R Last week we learned about the general syntax of the ggdag package:\n We created dagified objects with ggdag::dagify() We plotted our DAGs with ggdag::ggdag() We discussed how to specify the coordinates of our nodes with a coordinate list  Today, we will learn how the ggdag and dagitty packages can help us illustrate our paths and adjustment sets to fulfill the backdoor criterion\nLet's take one of the DAGs from our review slides:\ncoord_dag \u0026lt;- list( x = c(d = 0, p = 0, b = 1, a = 1 , c = 2, y = 2), y = c(d = 0, p = 2, b = 1, a = -1, c = 2, y = 0) ) our_dag \u0026lt;- ggdag::dagify(d ~ p + a, # p and a pointing at d b ~ p + c, # p and c pointing at b y ~ d + a + c, # d, a, and c pointing at y coords = coord_dag, # our coordinates from the list up there exposure = \u0026quot;d\u0026quot;, # we declare out exposure variable outcome = \u0026quot;y\u0026quot;) # we declare out outcome variable ggdag::ggdag(our_dag) + theme_dag() # equivalent to theme_void() Learning about our paths and what adjustments we need As you have seen, when we dagify a DAG in R a dagitty object is created. These objects tell R that we are dealing with DAGs.\nThis is very important because in addition to plotting them, we can do analyses on the DAG objects. A package that complements ggdag is the dagitty package.\nToday, we will focus on two functions from the dagitty package:\n dagitty::paths(): Returns a list with two components: paths, which gives the actual paths, and open, which shows whether each path is open (d-connected) or closed (d-separated). dagitty::adjustmentSets(): Lists the sets of covariates that would allow for unbiased estimation of causal effects, assuming that the causal graph is correct.  We just need to input our DAG object.\nPaths Let's see how the output of the dagitty::paths function looks like:\ndagitty::paths(our_dag) ## $paths ## [1] \u0026quot;d -\u0026gt; y\u0026quot; \u0026quot;d \u0026lt;- a -\u0026gt; y\u0026quot; \u0026quot;d \u0026lt;- p -\u0026gt; b \u0026lt;- c -\u0026gt; y\u0026quot; ## ## $open ## [1] TRUE TRUE FALSE We see under $paths the three paths we declared during the manual exercise:\n d -\u0026gt; y d \u0026lt;- a -\u0026gt; y d \u0026lt;- p -\u0026gt; b \u0026lt;- c -\u0026gt; y  Additionally, $open tells us whether each path is open. In this case, we see that the second path is the only open non-causal path, so we would need to condition on a to close it.\nWe can also use ggdag to present the open paths visually with the ggdag_paths() function, as such:\nggdag::ggdag_paths(our_dag) + theme_dag()  Covariate adjustment In addition to listing all the paths and sorting the backdoors manually, we can use the dagitty::adjustmentSets() function.\nWith this function, we just need to input our DAG object and it will return the different sets of adjustments.\ndagitty::adjustmentSets(our_dag) ## { a } For example, in this DAG there is only one option. We need to control for a.\nWe can also use ggdag to present the open paths visually with the ggdag_adjustment_set() function, as such:\nAlso, do not forget to set the argument shadow = TRUE, so that the arrows from the adjusted nodes are included.\nggdag::ggdag_adjustment_set(our_dag, shadow = T) + theme_dag() If you want to learn more about DAGs in R   ggdag documentation: https://ggdag.malco.io/ dagitty vignette: https://cran.r-project.org/web/packages/dagitty/dagitty.pdf What is `dagitty: https://cran.r-project.org/web/packages/dagitty/vignettes/dagitty4semusers.html  NOW LET'S MOVE TO REGRESSION\n   Introduction to Regression Linear regression is largely used to predict the value of an outcome variable based on one or more input explanatory variables. As we previously discussed, regression addresses a simple mechanical problem, namely, what is our best guess of y given an observed x.\n Regression can be utilized without thinking about causes as a predictive or summarizing tool It would not be appropiate to give causal interpretations to any \\(\\beta\\), unless we establish the fulfilment of centain assumptions  Bivariate regression In bivariate regression, we are modeling a variable \\(y\\) as a mathematical function of one variable \\(x\\). We can generalize this in a mathematical equation as such:\n\\[y = \\beta_{0} + \\beta{1}x + œµ\\]\n Multiple linear regression In multiple linear regression, we are modeling a variable \\(y\\) as a mathematical function of multiple variables \\((x, z, m)\\). We can generalize this in a mathematical equation as such:\n\\[y = \\beta_{0} + \\beta_{1}x + \\beta_{2}z + \\beta_{3}m + œµ\\]\n  Exploratory questions Let's illustrate this with an example\nWe will use the wage1 dataset from the wooldridge package. These are data from the 1976 Current Population Survey used by Jeffrey M. Wooldridge with pedagogical purposes in his book on Introductory Econometrics.\nIf you want to check the contents of the wage1 data frame, you can type ?wage1 in your console\nVisualizing With regression we can answer EXPLORATORY QUESTIONS. For example:   What is the relationship between education and respondents' salaries?  We can start by exploring the relationship visually with our newly attained ggplot2 skills:\nggplot(wage1, aes(x = educ, y = wage)) + geom_point(color = \u0026quot;grey60\u0026quot;) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F, color = \u0026quot;#CC0055\u0026quot;) + theme_minimal() + labs(x = \u0026quot;Years of education\u0026quot;, y = \u0026quot;Hourly wage (USD)\u0026quot;)   The lm() function This question can be formalized mathematically as:\n\\[Hourly\\ wage = \\beta_0 + \\beta_1Years\\ of\\ education + œµ\\]\nOur interest here would be to build a model that predicts the hourly wage of a respondent (our outcome variable) using the years of education (our explanatory variable). Fortunately for us, R provides us with a very intuitive syntax to model regressions.\nThe general syntax for running a regression model in R is the following:\nyour_model_biv \u0026lt;- lm(outcome_variable ~ explanarory_variable, data = your_dataset) #for a bivariate regression your_model_mult \u0026lt;- lm(outcome_variable ~ explanarory_variable_1 + explanarory_variable_2, data = your_dataset) #for multiple regression Now let's create our own model and save it into the model_1 object, based on the bivariate regression we specified above in which wage is our outcome variable, educ is our explanatory variable, and our data come from the wage1 object:\nmodel_1 \u0026lt;- lm(wage ~ educ, data = wage1)  summary() and broom::tidy() We have created an object that contains the coefficients, standard errors and further information from your model. In order to see the estimates, you could use the base R function summary(). This function is very useful when you want to print your results in your console.\nAlternatively, you can use the tidy() function from the broom package. The function constructs a data frame that summarizes the model‚Äôs statistical findings. You can see what else you can do with broom by running: vignette(‚Äúbroom‚Äù). The broom::tidy() function is useful when you want to store the values for future use (e.g., visualizing them)\nLet's try both options in the console up there. You just need to copy this code below the model_1 code.\nsummary(model_1) broom::tidy(model_1)  Exercise \\[Hourly\\ wage = \\beta_0 + \\beta_1Years\\ of\\ education + œµ\\]\n       Dependent variable:           Hourly wage       Years of education   0.541***      (0.053)         Constant   -0.905      (0.685)            Observations   526    R2   0.165    Adjusted R2   0.163    Residual Std. Error   3.378 (df = 524)    F Statistic   103.363*** (df = 1; 524)       Note:  p\u0026lt;0.1; p\u0026lt;0.05; p\u0026lt;0.01     How would you interpret the results of our model_1?  What does the constant mean? What does the educ coefficient mean? Do these coefficient carry any causal meaning?    Adding more nuance to our models As we have discussed in previous sessions we live in a very complex world. It is very likely that our exploration of the relationship between education and respondents' salaries is open to multiple sources of bias.\nLooking back at 1976 US, can you think of possible variables inside the mix?\n How about the sex or the ethnicity of a worker? Let's explore this visually  What is the relationship between education and respondents' salaries conditional on the sex of the worker?  ggplot(wage1, aes(x = educ, y = wage, color = as.factor(female))) + geom_point(alpha = 0.3) + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + labs(x = \u0026quot;Years of education\u0026quot;, y = \u0026quot;Hourly wage (USD)\u0026quot;, color = \u0026quot;Female\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) Check what happens when we replace the color = as.factor(female) for color = female\n What insights can we gather from this graph?   Multiple linear regression This question can be formalized mathematically as:\n\\[Hourly\\ wage = \\beta_0 + \\beta_1Years\\ of\\ education + \\beta_2Female + œµ\\]\nOur interest here would be to build a model that predicts the hourly wage of a respondent (our outcome variable) using the years of education and their sex (our explanatory variables).\nLet's remember the syntax for running a regression model in R:\nyour_model_biv \u0026lt;- lm(outcome_variable ~ explanarory_variable, data = your_dataset) #for a bivariate regression your_model_mult \u0026lt;- lm(outcome_variable ~ explanarory_variable_1 + explanarory_variable_2, data = your_dataset) #for multiple regression Now let's create our own model, save it into the model_2 object, and print the results based on the formula regression we specified above in which wage is our outcome variable, educ and female are our explanatory variables, and our data come from the wage1 object:\nmodel_2 \u0026lt;- lm(wage ~ educ + female, data = wage1) summary(model_2) ## ## Call: ## lm(formula = wage ~ educ + female, data = wage1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.9890 -1.8702 -0.6651 1.0447 15.4998 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.62282 0.67253 0.926 0.355 ## educ 0.50645 0.05039 10.051 \u0026lt; 2e-16 *** ## female -2.27336 0.27904 -8.147 2.76e-15 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 3.186 on 523 degrees of freedom ## Multiple R-squared: 0.2588, Adjusted R-squared: 0.256 ## F-statistic: 91.32 on 2 and 523 DF, p-value: \u0026lt; 2.2e-16  Exercise \\[Hourly\\ wage = \\beta_0 + \\beta_1Years\\ of\\ education + \\beta_2Female + œµ\\]\n      Dependent variable:           Hourly wage       Years of education   0.506***      (0.050)         Female   -2.273***      (0.279)         Constant   0.623      (0.673)            Observations   526    R2   0.259    Adjusted R2   0.256    Residual Std. Error   3.186 (df = 523)    F Statistic   91.315*** (df = 2; 523)       Note:  p\u0026lt;0.1; p\u0026lt;0.05; p\u0026lt;0.01    How would you interpret the results of our model_2?  What does the constant mean? What does the educ coefficient mean? What does the female coefficient mean?   Predicting from our models As we discussed previously, when we do not have our causal inference hats on, the main goal of linear regression is to predict an outcome value on the basis of one or multiple predictor variables.\nR has a generic function predict() that helps us arrive at the predicted values on the basis of our explanatory variables.\nThe syntax of predict() is the following:\npredict(name_of_the_model, newdata = data.frame(explanatory1 = value, explanatory2 = value))  Say that based on our model_2, we are interested in the expected average hourly wage of a woman with 15 years of education.\n predict(model_2, newdata = data.frame(educ = 15, female = 1)) ## 1 ## 5.946237  What does this result tell us? What happens when you change female to 0? What does the result mean? Can you think of a way to find the difference in the expected hourly wage between a male with 16 years of education and a female with 17?  predict(model_2, newdata = data.frame(educ = 16, female = 0)) - predict(model_2, newdata = data.frame(educ = 15, female =0)) ## 1 ## 0.5064521   Quiz Here are some questions for you. Note that there are multiple ways to reach the same answer:\nWhat is the expected hourly wage of a male with 15 years of education?\n$8.22 $9.50 5.34 $3  How much more on average does a male worker earn than a female counterpart?\u0026quot;,\n$2.27 In our data, males on average earn less than females $1.20 $4.50  How much more is a worker expected to earn for every additional year of education, keeping sex constant?\n$0.90 $1.20 $0.5   DAGs and modeling As we can remember from our slides, we were introduced to a set of key rules in understanding how to employ DAGs to guide our modeling strategy.\n A path is open or unblocked at non-colliders (confounders or mediators) A path is (naturally) blocked at colliders An open path induces statistical association between two variables Absence of an open path implies statistical independence Two variables are d-connected if there is an open path between them Two variables are d-separated if the path between them is blocked  In this portion of the tutorial we will demonstrate how different bias come to work when we model our relationships of interest.\n What happens when we control for a collider? The case for beauty, talent, and celebrity (What happens when we control for a collider?)  As it is showcased from our DAG, we assume that earning celebrity status is a function of an individuals beauty and talent.\nWe will simulate data that reflects this assumptions. In our world, someone gains celebrity status if the sum of units of beauty and celebrity are greater than 8.\n# beauty - 1000 observations with mean 5 units of beauty and sd 1.5 (arbitrary scale) beauty \u0026lt;- rnorm(1000, 5, 1.5) # talent - 1000 observations with mean 3 units of talent and sd 1 (arbitrary scale) talent \u0026lt;- rnorm(1000, 3, 1) # celebrity - binary celebrity_status \u0026lt;- ifelse(beauty + talent \u0026gt; 8, \u0026quot;Celebrity\u0026quot; , \u0026quot;Not Celebrity\u0026quot;) # celebrity if the sum of units are greater than 8 celebrity_df \u0026lt;- dplyr::tibble(beauty, talent, celebrity_status) # we make a df with our values head(celebrity_df, 10) ## # A tibble: 10 x 3 ## beauty talent celebrity_status ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 7.06 5.33 Celebrity ## 2 4.15 3.52 Not Celebrity ## 3 5.54 3.97 Celebrity ## 4 5.95 3.38 Celebrity ## 5 5.61 2.00 Not Celebrity ## 6 4.84 2.40 Not Celebrity ## 7 7.27 3.17 Celebrity ## 8 4.86 0.0715 Not Celebrity ## 9 8.03 2.15 Celebrity ## 10 4.91 3.80 Celebrity In this case, as our simulation suggest, we have a collider structure. We can see that celebrity can be a function of beauty or talent. Also, we can infer from the way we defined the variables that beauty and talent are d-separated (ie. the path between them is closed because celebrity is a collider).\nSay you are interested in researching the relationship between beauty and talent for your Master's thesis, while doing your literature review you encounter a series of papers that find a negative relationship between the two and state that more beautiful people tend to be less talented. The model that these teams of the researchers used was the following:\n\\[Y_{Talent} = \\beta_0 + \\beta_1Beauty + \\beta_2Celebrity\\]\nYour scientific hunch makes you believe that celebrity is a collider and that by controlling for it in their models, the researchers are inducing collider bias, or endogenous bias. You decide to move forward with your thesis by laying out a criticism to previous work on the field, given that you consider the formalization of their models is erroneous. You utilize the same data previous papers used, but based on your logic, you do not control for celebrity status. This is what you find:\nTrue model true_model_celebrity \u0026lt;- lm(talent ~ beauty, data = celebrity_df) summary(true_model_celebrity) ## ## Call: ## lm(formula = talent ~ beauty, data = celebrity_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.9225 -0.6588 -0.0083 0.6628 3.5877 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 2.962209 0.107595 27.531 \u0026lt;2e-16 *** ## beauty 0.006545 0.020755 0.315 0.753 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.9865 on 998 degrees of freedom ## Multiple R-squared: 9.964e-05, Adjusted R-squared: -0.0009023 ## F-statistic: 0.09945 on 1 and 998 DF, p-value: 0.7526 ggplot(celebrity_df, aes(x=beauty, y=talent)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(x = \u0026quot;Beauty\u0026quot;, y = \u0026quot;Talent\u0026quot;)  Biased model from previous literature Let's see:\nbiased_model_celibrity \u0026lt;- lm(talent ~ beauty + celebrity_status, data = celebrity_df) summary(biased_model_celibrity) ## ## Call: ## lm(formula = talent ~ beauty + celebrity_status, data = celebrity_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4244 -0.5394 0.0110 0.5064 2.9429 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 5.37834 0.13983 38.46 \u0026lt;2e-16 *** ## beauty -0.32668 0.02265 -14.43 \u0026lt;2e-16 *** ## celebrity_statusNot Celebrity -1.51375 0.06808 -22.24 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.807 on 997 degrees of freedom ## Multiple R-squared: 0.3316, Adjusted R-squared: 0.3302 ## F-statistic: 247.3 on 2 and 997 DF, p-value: \u0026lt; 2.2e-16 ggplot(celebrity_df, aes(x=beauty, y=talent, color = celebrity_status)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(x = \u0026quot;Beauty\u0026quot;, y = \u0026quot;Talent\u0026quot;, color = \u0026quot;\u0026quot;)  As we can see, by controlling for a collider, the previous literature was inducing to a non-existent association between beauty and talent, also known as collider or endogenous bias.\n   What happens when we fail to control for a confounder? Shoe size and salary (What happens when we fail to control for a confounder?)  # sex - replicate male and female 500 times each sex \u0026lt;- rep(c(\u0026quot;Male\u0026quot;, \u0026quot;Female\u0026quot;), each = 500) # shoe size - random number with mean 38 and sd 4, plus 4 if the observation is male shoesize \u0026lt;- rnorm(1000, 38, 2) + (4 * as.numeric(sex == \u0026quot;Male\u0026quot;)) # salary - a random number with mean 25 and sd 2, plus 5 if the observation is male salary \u0026lt;- rnorm(1000, 25, 2) + (5 * as.numeric(sex == \u0026quot;Male\u0026quot;)) salary_df \u0026lt;- dplyr::tibble(sex, shoesize, salary) head(salary_df, 10) ## # A tibble: 10 x 3 ## sex shoesize salary ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Male 42.5 28.6 ## 2 Male 41.4 28.4 ## 3 Male 38.6 29.2 ## 4 Male 38.0 27.7 ## 5 Male 39.4 32.2 ## 6 Male 42.7 28.2 ## 7 Male 41.7 32.6 ## 8 Male 40.5 27.1 ## 9 Male 40.4 31.7 ## 10 Male 43.1 28.8 Say now one of your peers tells you about this new study that suggests that shoe size has an effect on an individuals' salary. You are a bit skeptic and read it. The model that these researchers apply is the following:\n\\[Y_{Salary} = \\beta_0 + \\beta_1ShoeSize\\]\nYour scientific hunch makes you believe that this relationship could be confounded by the sex of the respondent. You think that by failing to control for sex in their models, the researchers are inducing omitted variable bias. You decide to open their replication files and control for sex. This is what you find:\n\\[Y_{Salary} = \\beta_0 + \\beta_1ShoeSize + \\beta_2Sex\\]\nTrue model true_model_salary \u0026lt;- lm(salary ~ shoesize + sex, data = salary_df) summary(true_model_salary) ## ## Call: ## lm(formula = salary ~ shoesize + sex, data = salary_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.2341 -1.3698 -0.0501 1.3595 6.4303 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 25.73879 1.15886 22.210 \u0026lt;2e-16 *** ## shoesize -0.02030 0.03044 -0.667 0.505 ## sexMale 5.05924 0.17616 28.720 \u0026lt;2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.981 on 997 degrees of freedom ## Multiple R-squared: 0.6129, Adjusted R-squared: 0.6121 ## F-statistic: 789.2 on 2 and 997 DF, p-value: \u0026lt; 2.2e-16 ggplot(salary_df, aes(x=shoesize, y=salary, color = sex)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(x = \u0026quot;Shoe size\u0026quot;, y = \u0026quot;Salary\u0026quot;, color = \u0026quot;\u0026quot;)  Biased model from previous literature biased_model_salary \u0026lt;- lm(salary ~ shoesize, data = salary_df) summary(biased_model_salary) ## ## Call: ## lm(formula = salary ~ shoesize, data = salary_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.8777 -1.9101 -0.0511 1.8496 7.9774 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 3.68865 1.17280 3.145 0.00171 ** ## shoesize 0.59429 0.02925 20.319 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 2.676 on 998 degrees of freedom ## Multiple R-squared: 0.2926, Adjusted R-squared: 0.2919 ## F-statistic: 412.9 on 1 and 998 DF, p-value: \u0026lt; 2.2e-16 ggplot(salary_df, aes(x=shoesize, y=salary)) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;, se = F) + theme_minimal() + labs(x = \u0026quot;Shoe size\u0026quot;, y = \u0026quot;Salary\u0026quot;)  As we can see, by failing to control for a confounder, the previous literature was creating a non-existent association between shoe size and salary, incurring in ommited variable bias.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9e2b7b722f70588957b6d443be9de254","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-4/04-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-4/04-online-tutorial/","section":"materials","summary":"Welcome Introduction! Welcome to our fourth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week's lecture you reviewed bivariate and multiple linear regressions.","tags":null,"title":"The Backdoor Criterion and Basics of Regression in R","type":"book"},{"authors":null,"categories":null,"content":"  The POF in practice Let's revisit the example from our slides once again.\nSay we are interested in assessing the premise of Allport's hypothesis about interpersonal contact being conducive to reducing intergroup prejudice.\nWe are studying a set of (\\(n=8\\)) students assigned to a dorm room with a person from their own ethnic group (contact=0) and from a different group (contact=1).\n  Student (i) Prejudice (C=0) Prejudice (C=1)    1 6 5  2 4 2  3 4 4  4 6 7  5 3 1  6 2 2  7 8 7  8 4 5    Data set Today we will work with the prejudice_df object. The data frame contains the following four variables:\n student_id: numeric student identification prej_0: prejudice level under \\(Y_{0i}\\) (Contact=0) prej_1: prejudice level under \\(Y_{1i}\\) (Contact=1) dorm_type: binary for actual treatment state  ## # A tibble: 8 x 4 ## student_id prej_0 prej_1 dorm_type ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 6 5 0 ## 2 2 4 2 1 ## 3 3 4 4 0 ## 4 4 6 7 0 ## 5 5 3 1 1 ## 6 6 2 2 1 ## 7 7 8 7 0 ## 8 8 4 5 0   Treatment Effects a) Individual Treatment Effect (ITE) We assume from the potential outcomes framework that each subject has a potential outcome under both treatment states. Let's take the first student in the list as an example.\nThe figure illustrates the potential outcomes for Student 1.\nWe see that in a reality where Student 1 is assigned to in-group dorm (contact=0) their levels of prejudice are 6. On the contrary, in a reality where Student 1 is assigned to co-ethnic dorm (contact=1) their levels of prejudice are 5.\nFrom this illustration, we can gather the individual treatment effect (ITE) for student one. The ITE is equal to the values under treatment (contact=1) minus to the values without treatment (contact=0) or \\(ITE = y_{1i} - y_{0i}\\).\n\\[ITE = 5 - 6 = -1\\]\n As it was put in Cunningham\u0026rsquo;s book:\nThe ITE is a ‚Äúcomparison of two states of the world‚Äù (Cunningham, 2021): individuals are exposed to contact, and not exposed to it.\n  Evidently, each subject can only be observed in one treatment state at any point in time in real life. This is known as the fundamental problem (Holland, 1986) of causal inference. The Individual Treatment Effect (ITE) in reality is unattainable. Still, it provides us with a conceptual foundation for causal estimation.\nExercise: Our data are coming from a world with perfect information. In that sense, we have both potential outcomes prej_0 and prej_1. Can you think of a way to calculate the ITE for the eight students with one of the dplyr verbs we learned in the previous section?\nHint Can you think of a way we can use the verb mutate()\n Answer #you can employ dplyr::mutate() to create the new variable ite prejudice_df %\u0026gt;% dplyr::mutate(ite = prej_1 - prej_0)   student_id  prej_0  prej_1  dorm_type  ite      1  6  5  0  -1    2  4  2  1  -2    3  4  4  0  0    4  6  7  0  1    5  3  1  1  -2    6  2  2  1  0    7  8  7  0  -1    8  4  5  0  1       Average Treatment Effect (ATE) Normally, we are not interested in the estimates of individual subjects, but rather a population. The Average Treatment Effect (ATE) is the difference in the average potential outcomes of the population.\n\\[ATE = E(Y_{1i}) - E(Y_{0i})\\]\nIn other words, the ATE is the average ITE of all the subjects in the population. As you can see, the ATE as defined in the formula is also not attainable. Can you think why?\nExercise: Since our data are coming from a world with perfect information. Can you think of a way to calculate the ATE for the eight students based on what we learned last week?\nHint We have already extracted the ite with mutate(). We know that the the ATE is the averge of every subject's ITE. Do you remember summarize()?\n Answer #we know that the ATE is the averge of every subject\u0026#39;s ITE. Do you remember dplyr::summarize()? #how can we use the verbs from last week to get the average treatment effect? prejudice_df %\u0026gt;% dplyr::mutate(ite = prej_1 - prej_0) %\u0026gt;% dplyr::summarize(ate=mean(ite))   ate      -0.5       The Average Treatment Effect Among the Treated and Control (ATT) and (ATC) The names for these two estimates are very self-explanatory. These two estimates are simply the average treatment effects conditional on the group subjects are assigned to.\nThe average treatment effect on the treated ATT is defined as the difference in the average potential outcomes for those subjects who were treated: \\[ATT = E(Y_{1i}-Y_{0i} | D = 1)\\]\nThe average treatment effect under control ATC is defined as the difference in the average potential outcomes for those subjects who were not treated: \\[ATC = E(Y_{1i}-Y_{0i} | D = 0)\\]\nExercise: Since our data are coming from a world with perfect information. Can you think of a way to calculate the ATT and ATC for the eight students based on what we learned last week?\nHint We have already extracted the ite with mutate(). We know that the ATT and ATC are the average of every subject's ITE grouped by their treatment status. Do you remember how the combination of group_by() and summarize() worked?\n Answer #we know that the ATT and ATC are the average of every subject\u0026#39;s ITE grouped by their treatment status. Do you remember how the combination of dplyr::group_by() and dplyr::summarize() worked? #how can we use the verbs from last week to get the average treatment effect on the treated and untreated? prejudice_df %\u0026gt;% dplyr::mutate(ite = prej_1 - prej_0) %\u0026gt;% dplyr::group_by(dorm_type) %\u0026gt;% dplyr::summarize(treatment_effects=mean(ite))   dorm_type  treatment_effects      0  0.000000    1  -1.333333       The Naive Average Treatment Effect (NATE) So far, we have worked with perfect information. Still, we know that in reality we can only observe subjects in one treatment state. This is the information we do have.\n The Naive Average Treatment Effect (NATE) is the calculation we can compute based on the observed outcomes.   \\[NATE = E(Y_{1i}|D{i}=1) - E(Y_{0i}|D{i}=0)\\] *reads in English as: \u0026quot;The expected average outcome under treatment for those treated minus the expected average outcome under control for those not treated\u0026quot;\nExercise: Can you think of a way to calculate the NATE for the eight students employing the new observed_prej variable?\nprejudice_df %\u0026gt;% dplyr::mutate(observed_prej = ifelse(dorm_type == 1, prej_1, prej_0))   student_id  prej_0  prej_1  dorm_type  observed_prej      1  6  5  0  6    2  4  2  1  2    3  4  4  0  4    4  6  7  0  6    5  3  1  1  1    6  2  2  1  2    7  8  7  0  8    8  4  5  0  4     Hint We have already extracted the average observed outcomes depending on the treatment status with mutate(). We know that the NATE is the difference in average observed outcomes grouped by their treatment status. Do you remember how the combination of group_by() and summarize() worked?\n Answer #we know that the NATE is the difference in average observed outcomes grouped by their treatment status. Do you remember how the combination of dplyr::group_by() and dplyr::summarize() worked? prejudice_df %\u0026gt;% dplyr::mutate(observed_prej = ifelse(dorm_type == 1, prej_1, prej_0)) %\u0026gt;% dplyr::group_by(dorm_type) %\u0026gt;% dplyr::summarize(mean(observed_prej)) #You can just substract the values   dorm_type  mean(observed_prej)      0  5.600000    1  1.666667     You can just substract the values\n Note. The √¨felse() function is a very handy tool to have. It allows us to generate conditional statements. The syntax is the following:\nifelse(condition_to_meet, what_to_do_if_met, what_to_do_if_not_met) In the case of observed_prej, we ask R to create a new variable, where if the subject is in a co-ethnic dorm, we print the prejudice value under treatment. If that condition is not met, we print the prejudice value under control.\n  Bias Bias During the lecture, we met two sources of bias:\n Baseline bias Baseline bias‚Äîalso known as selection bias‚Äî is difference in expected outcomes in the absence of treatment for the actual treatment and control group. In other words, these are the underlying differences that individuals in either group start off with.\n Differential treatment effect bias Differential treatment effect bias ‚Äî also known as Heterogeneous Treatment Effect (HTE) bias ‚Äî is the difference in returns to treatment (the treatment effect) between the treatment and control group, multiplied by the share of the population in control. In other words, this type of bias relates to the dissimilarities stemming for ways in which individuals in either group are affected differently by the treatment.\nWe will let you think about these for the mock assignment\nExercise: Since our data are coming from a world with perfect information. Can you think of a way to explore the existence baseline bias in our data?\nHint We know that the baseline bias is the difference in average observed outcomes under control grouped by their treatment status. Do you remember how the combination of dplyr::group_by() and dplyr::summarize() worked?\n Exercise: Since our data are coming from a world with perfect information. Can you think of a way to explore the existence differential treatment effect bias in our data?\nHint We know that the differential treatment effect bias is the difference in difference in the average of every subject's ITE grouped by their treatment status (or the difference between ATT and ATCs). Maybe you can go back an check how to get the average treatment effect on the treated and untreated.\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"78779ae7cd8fe70a383794e67e25c038","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-2/pof/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-2/pof/","section":"materials","summary":"The POF in practice Let's revisit the example from our slides once again.\nSay we are interested in assessing the premise of Allport's hypothesis about interpersonal contact being conducive to reducing intergroup prejudice.","tags":null,"title":"The Potential Outcomes Framework","type":"book"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/2022-spring-stats2/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/2022-spring-stats2/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/2022-spring-stats2/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"   Welcome Introduction! Welcome to our ninth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you were introduced to Panel Data and Fixed Effects.\nIn this lab session we will:\n Create three-way tables with janitor::tabyl() Visualize trends across time with ggplot2 Learn how to extract our pooled, unit-fixed, and unit- and time-fixed effects estimates with Least Squares Dummy Variables (LSDV) estimation (with lm() and the de-meaning approach with plm())    Measuring the effect of a carbon tax on national carbon dioxide emissions per capita After all the very valuable output you have generated in the past weeks, you have become a sensation within the policy analysis world. You are hired as an outside consultant by the Organization of Economic Non-Cooperation for Development (OENCD), they are interested in studying the effect of a carbon tax on national carbon dioxide emissions per capita. You are provided with data for the twenty-members of the organization from 2009 to 2019. The data can be called a balanced panel based on the description given in the lecture (i.e.¬†each panel member is observed every year)\nPackages # These are the libraries we will use today. Make sure to install them in your console in case you have not done so previously. set.seed(42) #for consistent results library(dplyr) # to wrangle our data library(tidyr) # to wrangle our data - pivot_longer() library(lubridate) # for working with dates-times in a more intuitive manner library(ggplot2) # to render our graphs library(readr) # for loading the .csv data library(kableExtra) # to render better formatted tables library(stargazer) # for formatting your model output library(janitor) # for data management and tabyl() library(lmtest) # to gather our clustered standard errors - coeftest() library(plm) # to gather our clustered standard errors - vcovHC() and plm()  Our data carbon_tax_df \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/seramirezruiz/stats-ii-lab/master/Session%207/data/carbon_tax_df.csv\u0026quot;) # simulated data Our dataset carbon_tax_df, contains the following information:\n country_name: Name of the country country_code: Three-letter country code year: Year tax: Dummy for whether the carbon tax was in place income_class: Categorical variable with income label (Low to High) co2_per_capita: carbon dioxide emissions per capita in metric tons (T)    Exploratory analysis Let‚Äô explore who had the tax in place at what point We can use what we have learned about the janitor::tabyl() function to check how many observations we have for each of the countries:\n One-way table:  carbon_tax_df %\u0026gt;% janitor::tabyl(country_name) %\u0026gt;% janitor::adorn_pct_formatting(digits = 1) %\u0026gt;% knitr::kable(col.names = c(\u0026quot;Country\u0026quot;, \u0026quot;N\u0026quot;, \u0026quot;Percent, %\u0026quot;)) %\u0026gt;% kableExtra::kable_styling()   Country  N  Percent, %      Adjikistan  11  8.3%    Borovia  11  8.3%    Carpania  11  8.3%    Corinthia  11  8.3%    Freedonia  11  8.3%    Glenraven  11  8.3%    Khemed  11  8.3%    Laurania  11  8.3%    Parano  11  8.3%    Ron  11  8.3%    Rumekistan  11  8.3%    Transia  11  8.3%     We can also explore how many countries had a tax in place every year.\n Two-way table:  carbon_tax_df %\u0026gt;% janitor::tabyl(tax, year) %\u0026gt;% janitor::adorn_totals(\u0026quot;row\u0026quot;) %\u0026gt;% janitor::adorn_percentages(\u0026quot;col\u0026quot;) %\u0026gt;% janitor::adorn_pct_formatting(digits = 1) %\u0026gt;% janitor::adorn_ns() %\u0026gt;% knitr::kable() %\u0026gt;% kableExtra::kable_styling() %\u0026gt;% kableExtra::add_header_above(c(\u0026quot;\u0026quot;, \u0026quot;Year\u0026quot; = 11))    Year     tax  2009  2010  2011  2012  2013  2014  2015  2016  2017  2018  2019      0  100.0% (12)  100.0% (12)  83.3% (10)  75.0% (9)  66.7% (8)  66.7% (8)  41.7% (5)  33.3% (4)  33.3% (4)  33.3% (4)  41.7% (5)    1  0.0% (0)  0.0% (0)  16.7% (2)  25.0% (3)  33.3% (4)  33.3% (4)  58.3% (7)  66.7% (8)  66.7% (8)  66.7% (8)  58.3% (7)    Total  100.0% (12)  100.0% (12)  100.0% (12)  100.0% (12)  100.0% (12)  100.0% (12)  100.0% (12)  100.0% (12)  100.0% (12)  100.0% (12)  100.0% (12)     We can further explore how many countries had a tax in place every year at the income_class cluster:\n Three-way table:  carbon_tax_df %\u0026gt;% janitor::tabyl(tax, year, income_class) %\u0026gt;% janitor::adorn_percentages(\u0026quot;col\u0026quot;) %\u0026gt;% janitor::adorn_pct_formatting(digits = 1) %\u0026gt;% janitor::adorn_ns()  ## $High ## tax 2009 2010 2011 2012 2013 2014 2015 ## 0 100.0% (3) 100.0% (3) 66.7% (2) 66.7% (2) 33.3% (1) 33.3% (1) 33.3% (1) ## 1 0.0% (0) 0.0% (0) 33.3% (1) 33.3% (1) 66.7% (2) 66.7% (2) 66.7% (2) ## 2016 2017 2018 2019 ## 33.3% (1) 33.3% (1) 33.3% (1) 33.3% (1) ## 66.7% (2) 66.7% (2) 66.7% (2) 66.7% (2) ## ## $`High-Middle` ## tax 2009 2010 2011 2012 2013 2014 2015 ## 0 100.0% (3) 100.0% (3) 66.7% (2) 66.7% (2) 66.7% (2) 66.7% (2) 0.0% (0) ## 1 0.0% (0) 0.0% (0) 33.3% (1) 33.3% (1) 33.3% (1) 33.3% (1) 100.0% (3) ## 2016 2017 2018 2019 ## 0.0% (0) 0.0% (0) 0.0% (0) 33.3% (1) ## 100.0% (3) 100.0% (3) 100.0% (3) 66.7% (2) ## ## $Low ## tax 2009 2010 2011 2012 2013 2014 ## 0 100.0% (3) 100.0% (3) 100.0% (3) 100.0% (3) 100.0% (3) 100.0% (3) ## 1 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) 0.0% (0) ## 2015 2016 2017 2018 2019 ## 100.0% (3) 66.7% (2) 66.7% (2) 66.7% (2) 66.7% (2) ## 0.0% (0) 33.3% (1) 33.3% (1) 33.3% (1) 33.3% (1) ## ## $`Low-Middle` ## tax 2009 2010 2011 2012 2013 2014 2015 ## 0 100.0% (3) 100.0% (3) 100.0% (3) 66.7% (2) 66.7% (2) 66.7% (2) 33.3% (1) ## 1 0.0% (0) 0.0% (0) 0.0% (0) 33.3% (1) 33.3% (1) 33.3% (1) 66.7% (2) ## 2016 2017 2018 2019 ## 33.3% (1) 33.3% (1) 33.3% (1) 33.3% (1) ## 66.7% (2) 66.7% (2) 66.7% (2) 66.7% (2) As we can see, the three-way table creates a list containing the different combinations which are accesible with the $ operator. We can do a little bit of code gymnastics to generate our nicely formated output by saving the janitor::tabyl() output into an object and printing each of the tables individually.\nthree_way_tab \u0026lt;- carbon_tax_df %\u0026gt;% janitor::tabyl(tax, year, income_class) %\u0026gt;% janitor::adorn_percentages(\u0026quot;col\u0026quot;) %\u0026gt;% janitor::adorn_pct_formatting(digits = 1) %\u0026gt;% janitor::adorn_ns()    High      Year     tax  2009  2010  2011  2012  2013  2014  2015  2016  2017  2018  2019      0  100.0% (3)  100.0% (3)  66.7% (2)  66.7% (2)  33.3% (1)  33.3% (1)  33.3% (1)  33.3% (1)  33.3% (1)  33.3% (1)  33.3% (1)    1  0.0% (0)  0.0% (0)  33.3% (1)  33.3% (1)  66.7% (2)  66.7% (2)  66.7% (2)  66.7% (2)  66.7% (2)  66.7% (2)  66.7% (2)       High-Middle      Year     tax  2009  2010  2011  2012  2013  2014  2015  2016  2017  2018  2019      0  100.0% (3)  100.0% (3)  66.7% (2)  66.7% (2)  66.7% (2)  66.7% (2)  0.0% (0)  0.0% (0)  0.0% (0)  0.0% (0)  33.3% (1)    1  0.0% (0)  0.0% (0)  33.3% (1)  33.3% (1)  33.3% (1)  33.3% (1)  100.0% (3)  100.0% (3)  100.0% (3)  100.0% (3)  66.7% (2)       Low-Middle      Year     tax  2009  2010  2011  2012  2013  2014  2015  2016  2017  2018  2019      0  100.0% (3)  100.0% (3)  100.0% (3)  66.7% (2)  66.7% (2)  66.7% (2)  33.3% (1)  33.3% (1)  33.3% (1)  33.3% (1)  33.3% (1)    1  0.0% (0)  0.0% (0)  0.0% (0)  33.3% (1)  33.3% (1)  33.3% (1)  66.7% (2)  66.7% (2)  66.7% (2)  66.7% (2)  66.7% (2)       Low      Year     tax  2009  2010  2011  2012  2013  2014  2015  2016  2017  2018  2019      0  100.0% (3)  100.0% (3)  100.0% (3)  100.0% (3)  100.0% (3)  100.0% (3)  100.0% (3)  66.7% (2)  66.7% (2)  66.7% (2)  66.7% (2)    1  0.0% (0)  0.0% (0)  0.0% (0)  0.0% (0)  0.0% (0)  0.0% (0)  0.0% (0)  33.3% (1)  33.3% (1)  33.3% (1)  33.3% (1)      Let‚Äôs explore visually the levels of carbon dioxide emmissions ggplot(carbon_tax_df, aes(x = factor(year), y= co2_per_capita, color = factor(tax))) + geom_point() + #create scatterplot labs(title = \u0026quot;Exploratory plot of CO2 emissions per capita\u0026quot;, x = \u0026quot;Year\u0026quot;, y = \u0026quot;CO2 emissions in metric tons (T)\u0026quot;, color = \u0026quot;Carbon tax\u0026quot;) + theme_minimal() + theme(legend.position=\u0026quot;bottom\u0026quot;) + #move legend to the bottom scale_color_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;))   What do we see here?  carbon_tax_df %\u0026gt;% dplyr::mutate(year_as_date = lubridate::ymd(year, truncated = 2L), #turning numeric to date format income_class = factor(carbon_tax_df$income_class, levels = c(\u0026quot;High\u0026quot;, \u0026quot;High-Middle\u0026quot;, \u0026quot;Low-Middle\u0026quot;, \u0026quot;Low\u0026quot;))) %\u0026gt;% #reordering the factors ggplot(., aes(x = year_as_date, y= co2_per_capita, color = factor(tax))) + geom_point() + #create scatterplot geom_path(aes(group = 1)) + #to render consecutive lines disregarding the tax (you will likely use geom_line() for the assignment) facet_wrap(country_name~income_class) + #to split plot into panels based on this dimension scale_x_date(date_labels = \u0026quot;%Y\u0026quot;) + #telling ggplot that we want the ticks to be the years in the dates labs(title = \u0026quot;Exploratory plot of CO2 emissions per capita\u0026quot;, x = \u0026quot;Year\u0026quot;, y = \u0026quot;CO2 emissions in metric tons (T)\u0026quot;, color = \u0026quot;Carbon tax\u0026quot;) + theme_bw() + scale_color_manual(name = \u0026quot; \u0026quot;, # changes to fill dimension values = c(\u0026quot;#a7a8aa\u0026quot;, \u0026quot;#cc0055\u0026quot;), labels = c(\u0026quot;Control\u0026quot;, \u0026quot;Treatment\u0026quot;))   What do we see here?  Note: The exploratory data analysis portions of our scripts will not transfer directly to this week‚Äôs assignment; however, they will be very useful for your future endevours. Summarizing, graphing, and exploring your data will be critical to discover patterns, to spot anomalies, and to check assumptions\n  Modeling and estimating We have seen during the lecture that balanced panel data can help us decompose the error term. With a balanced panel we can capture all unobserved, unit- and time-specific factors.\nIn the example at hand, we can think of unit-specific factors as characteristics of individual countries that are constant over time (e.g.¬†a country that just loves big pick-up trucks). We can also think about time-specific factors that affect all countries (e.g.¬†global economic shocks).\nWe can formulate this as:\n\\[Y_{it} = Œ≤_0 + Œ≤_1D_{it} + \\theta_{i} + \\delta_t + \\upsilon_{it}\\]\nwhere \\(\\theta_i\\) reflects the time-invariant traits of the units, \\(\\delta_t\\) reflects the time-specific factors that affect everyone and \\(\\upsilon_{it}\\) is the idiosyncratic error\nWe will move forward by creating three models:\n A naive model (also known as a pooled model), where we will regress co2_per_capita on tax. A model with unit-fixed effects, where we will capture the \\(\\theta\\) portion of our error A model with time- and unit-fixed effects, where we will capture our \\(\\delta\\) and \\(\\theta\\) portions of our error term  Naive modeling naive_carbon \u0026lt;- lm(co2_per_capita ~ tax, data = carbon_tax_df) pooled_naive_carbon \u0026lt;- plm(co2_per_capita ~ tax, data = carbon_tax_df, model = \u0026quot;pooling\u0026quot;) Naive model with lm() modelsummary::modelsummary(naive_carbon, fmt = 2, gof_omit = \u0026quot;AIC|BIC|Log.Lik.\u0026quot;, statistic = \u0026quot;conf.int\u0026quot;, stars = T)    Model 1      (Intercept)  10.63***     [9.93, 11.32]    tax  -6.26***     [-7.38, -5.14]    Num.Obs.  132    R2  0.485    R2 Adj.  0.481    F  122.394       + p \u0026lt; 0.1, * p \u0026lt; 0.05, ** p \u0026lt; 0.01, *** p \u0026lt; 0.001      Naive model with plm() modelsummary::modelsummary(pooled_naive_carbon, fmt = 2, gof_omit = \u0026quot;AIC|BIC|Log.Lik.\u0026quot;)    Model 1      (Intercept)  10.63     (0.35)    tax  -6.26     (0.57)    Num.Obs.  132    R2  0.485    R2 Adj.  0.481     What do we see here?\nThis model is telling us that on average, the \\(CO_2\\) emissions per capita are reduced by 6.2 metric tons when a carbon tax is put in place. Still, after all the work we have done throughout the semester, we understand that there may be a plethora of factors that could be skewing the results of this bivariate regression.\n  Unit-fixed effects We will learn two ways of gathering unit- and time-fixed effects in R:\nFirst, we will perform Least Squares Dummy Variables (LSDV) estimation with lm(), where we essentially get an individual estimate for each unit.\nSecond, we will run our model with plm(), which will do the same mechanics, yet it will not estimate each of the units intercepts (because it relies on the ‚Äúde-meaning‚Äù approach).\nlsdv_unit_fe \u0026lt;- lm(co2_per_capita ~ tax + country_name, data = carbon_tax_df) unit_fe \u0026lt;- plm(co2_per_capita ~ tax, data = carbon_tax_df, index = c(\u0026quot;country_name\u0026quot;), # unit effect = \u0026quot;individual\u0026quot;, model = \u0026quot;within\u0026quot;) Unit-fixed effects with lm() ‚Äî Least Squares Dummy Variables (LSDV) estimation modelsummary::modelsummary(lsdv_unit_fe, fmt = 2, gof_omit = \u0026quot;AIC|BIC|Log.Lik.\u0026quot;, statistic = \u0026quot;conf.int\u0026quot;, stars = T)    Model 1      (Intercept)  6.91***     [5.67, 8.15]    tax  -4.44***     [-5.37, -3.50]    country_nameBorovia  1.51     [-0.30, 3.31]    country_nameCarpania  5.11***     [3.30, 6.91]    country_nameCorinthia  6.43***     [4.68, 8.19]    country_nameFreedonia  6.12***     [4.33, 7.91]    country_nameGlenraven  0.37     [-1.51, 2.26]    country_nameKhemed  -0.67     [-2.56, 1.21]    country_nameLaurania  1.53     [-0.32, 3.39]    country_nameParano  3.69***     [1.94, 5.45]    country_nameRon  2.71**     [0.91, 4.52]    country_nameRumekistan  7.43***     [5.68, 9.19]    country_nameTransia  1.88+     [-0.03, 3.80]    Num.Obs.  132    R2  0.797    R2 Adj.  0.776    F  38.844       + p \u0026lt; 0.1, * p \u0026lt; 0.05, ** p \u0026lt; 0.01, *** p \u0026lt; 0.001      Unit-fixed effects with plm() modelsummary::modelsummary(unit_fe, fmt = 2, gof_omit = \u0026quot;AIC|BIC|Log.Lik.\u0026quot;, statistic = \u0026quot;conf.int\u0026quot;, stars = T)    Model 1      tax  -4.44***     [-5.36, -3.51]    Num.Obs.  132    R2  0.424    R2 Adj.  0.366       + p \u0026lt; 0.1, * p \u0026lt; 0.05, ** p \u0026lt; 0.01, *** p \u0026lt; 0.001     What do we see here?\nAdding unit-level fixed effects to the model, i.e.¬†accounting for unobserved, time-invariant characteristics of countries and only focusing on within-state variation, the imposition of a carbon tax reduces \\(CO_2\\) per capita emissions by 4.44 metric tons. Once we have captured the variation between countries, we can see that our results from the naive model were substantially biased. We can still try to capture the time-specific portion of the error.\nThe results from the Least Squares Dummy Variables (LSDV) estimation are read in reference to a baseline. In this case, the constant is representing the intercept for Adjikistan. We can utilize the individual slopes for each country to say that Freedonians emit on average 6.12 more metric tons of \\(CO_2\\) per capita than Adjikistanians.\n  Unit- and time-fixed effects We will perform our regressions with Least Squares Dummy Variables (LSDV) estimation with lm() and our simplified way with plm().\nlsdv_unit_time_fe \u0026lt;- lm(co2_per_capita ~ tax + country_name + factor(year), data = carbon_tax_df) unit_time_fe \u0026lt;- plm(co2_per_capita ~ tax, data = carbon_tax_df, index = c(\u0026quot;country_name\u0026quot;, \u0026quot;year\u0026quot;), # unit and time model = \u0026quot;within\u0026quot;, effect = \u0026quot;twoways\u0026quot;) Unit- and time-fixed effects with lm() ‚Äî Least Squares Dummy Variables (LSDV) estimation modelsummary::modelsummary(lsdv_unit_time_fe, fmt = 2, gof_omit = \u0026quot;AIC|BIC|Log.Lik.\u0026quot;, statistic = \u0026quot;conf.int\u0026quot;, stars = T)    Model 1      (Intercept)  8.62***     [7.84, 9.41]    tax  -3.91***     [-4.46, -3.35]    country_nameBorovia  1.27**     [0.44, 2.09]    country_nameCarpania  4.87***     [4.04, 5.69]    country_nameCorinthia  6.43***     [5.65, 7.22]    country_nameFreedonia  5.93***     [5.11, 6.74]    country_nameGlenraven  -0.01     [-0.90, 0.87]    country_nameKhemed  -1.06*     [-1.94, -0.17]    country_nameLaurania  1.20**     [0.33, 2.06]    country_nameParano  3.69***     [2.90, 4.48]    country_nameRon  2.47***     [1.65, 3.30]    country_nameRumekistan  7.43***     [6.64, 8.22]    country_nameTransia  1.45**     [0.54, 2.36]    factor(year)2010  -4.70***     [-5.45, -3.94]    factor(year)2011  1.89***     [1.13, 2.65]    factor(year)2012  -3.06***     [-3.83, -2.29]    factor(year)2013  0.11     [-0.67, 0.88]    factor(year)2014  -1.88***     [-2.65, -1.10]    factor(year)2015  -3.02***     [-3.84, -2.20]    factor(year)2016  -3.29***     [-4.13, -2.45]    factor(year)2017  -0.96*     [-1.80, -0.12]    factor(year)2018  -2.49***     [-3.33, -1.65]    factor(year)2019  -1.40**     [-2.22, -0.58]    Num.Obs.  132    R2  0.963    R2 Adj.  0.955    F  127.305       + p \u0026lt; 0.1, * p \u0026lt; 0.05, ** p \u0026lt; 0.01, *** p \u0026lt; 0.001      Unit-fixed effects with plm() modelsummary::modelsummary(unit_time_fe, fmt = 2, gof_omit = \u0026quot;AIC|BIC|Log.Lik.\u0026quot;, statistic = \u0026quot;conf.int\u0026quot;, stars = T)    Model 1      tax  -3.91***     [-4.46, -3.36]    Num.Obs.  132    R2  0.641    R2 Adj.  0.568       + p \u0026lt; 0.1, * p \u0026lt; 0.05, ** p \u0026lt; 0.01, *** p \u0026lt; 0.001     What do we see here?\nNow in addition to adding unit-level fixed effects to the model, we control for time-specific factors that affect the global per capita \\(CO_2\\) emissions. The results suggest that the effect of a carbon-tax leads to a decrease \\(CO_2\\) emissions of 3.91 metric tons per capita.\n Test of serial correlation for the idiosyncratic component of the errors in panel models In our models our assumption for the errors is \\(œÖ_{it} ‚àº iid(0, œÉ_œÖ^{2})\\).\nWith longer panels, serial correlation between errors is a problem:\n\\(Cor(œÖ_{it}, œÖ_i(t‚àí1))‚â†0\\).\nWe can test for serial correlation in our time and unit FE specification model, as such:\npbgtest(unit_time_fe, order = 2) ## ## Breusch-Godfrey/Wooldridge test for serial correlation in panel models ## ## data: co2_per_capita ~ tax ## chisq = 0.25101, df = 2, p-value = 0.8821 ## alternative hypothesis: serial correlation in idiosyncratic errors In here, the null hypothesis is there is serial correlation of any order up to 2 (i.e., first- and second-order). In this case, we do not find any serial correlation, so we do not need to correct our standard errors manually. If we were to find serial correlation, we could introduce robust standard errors with a similar syntax to the one used last week for clustered standard errors:\nmodel_with_robust_se \u0026lt;- coeftest(unit_time_fe, vcov=vcovHC(unit_time_fe, type = \u0026quot;sss\u0026quot;))    Drafting some brief recommedations You report back to the Organization of Economic Non-Cooperation for Development (OENCD). Based on your analysis of the data at hand, you suggest that the implementation of a carbon tax does have an effect on national carbon dioxide emissions per capita. Your results show that a carbon tax reduces \\(CO_2\\) emissions by 3.91 metric tons per capita.\nYour results are welcomed internationally and all states move forward with the measure.\n ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"abc88842603482f5532cbeb1e7220c02","permalink":"https://seramirezruiz.github.io/2022-spring-stats2/materials/session-9/09-online-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/2022-spring-stats2/materials/session-9/09-online-tutorial/","section":"materials","summary":"Welcome Introduction! Welcome to our ninth tutorial for the Statistics II: Statistical Modeling \u0026amp; Causal Inference (with R) course.\nDuring this week‚Äôs lecture you were introduced to Panel Data and Fixed Effects.","tags":null,"title":"Panel Data and Fixed Effects","type":"materials"}]